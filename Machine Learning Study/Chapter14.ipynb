{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14. 텐서플로의 구조 자세히 알아보기\n",
    "\n",
    "텐서플로 자체에 초점을 맞추고 텐서플로의 구조와 특징을 살펴볼 것.\n",
    "\n",
    "[ 주제 ]\n",
    "* 텐서플로의 주요 특징과 장점 살펴보기\n",
    "* 텐서플로의 랭크와 텐서\n",
    "* 텐서의 다차원 배열 변환하기\n",
    "* 텐서플로 그래프를 이해하고 다루기\n",
    "* 텐서플로 변수 다루기\n",
    "* tf.keras API 자세히 배우기\n",
    "* 텐서보드로 신경망 그래프 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#14.1-텐서플로의-주요-특징\" data-toc-modified-id=\"14.1-텐서플로의-주요-특징-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>14.1 텐서플로의 주요 특징</a></span></li><li><span><a href=\"#14.2-텐서플로의-랭크와-텐서\" data-toc-modified-id=\"14.2-텐서플로의-랭크와-텐서-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>14.2 텐서플로의 랭크와 텐서</a></span><ul class=\"toc-item\"><li><span><a href=\"#14.2.1-텐서의-랭크와-크기-확인하는-방법\" data-toc-modified-id=\"14.2.1-텐서의-랭크와-크기-확인하는-방법-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>14.2.1 텐서의 랭크와 크기 확인하는 방법</a></span></li></ul></li><li><span><a href=\"#14.3-텐서를-다차원-배열로-변환\" data-toc-modified-id=\"14.3-텐서를-다차원-배열로-변환-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>14.3 텐서를 다차원 배열로 변환</a></span></li><li><span><a href=\"#14.4-텐서플로의-계산-그래프-이해\" data-toc-modified-id=\"14.4-텐서플로의-계산-그래프-이해-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>14.4 텐서플로의 계산 그래프 이해</a></span></li><li><span><a href=\"#14.5-텐서플로의-변수\" data-toc-modified-id=\"14.5-텐서플로의-변수-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>14.5 텐서플로의 변수</a></span></li><li><span><a href=\"#14.6-tf.keras-API-자세히-배우기\" data-toc-modified-id=\"14.6-tf.keras-API-자세히-배우기-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>14.6 tf.keras API 자세히 배우기</a></span><ul class=\"toc-item\"><li><span><a href=\"#14.6.1-Sequential-모델\" data-toc-modified-id=\"14.6.1-Sequential-모델-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>14.6.1 Sequential 모델</a></span></li><li><span><a href=\"#14.6.2-함수형-API\" data-toc-modified-id=\"14.6.2-함수형-API-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>14.6.2 함수형 API</a></span></li><li><span><a href=\"#14.6.3-tf.keras-모델의-저장과-복원\" data-toc-modified-id=\"14.6.3-tf.keras-모델의-저장과-복원-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>14.6.3 tf.keras 모델의 저장과 복원</a></span></li></ul></li><li><span><a href=\"#14.7-계산-그래프-시각화\" data-toc-modified-id=\"14.7-계산-그래프-시각화-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>14.7 계산 그래프 시각화</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 텐서플로의 주요 특징\n",
    "\n",
    "텐서플로의 주요 특징 중 하나는 **여러 개의 GPU를 사용할 수 있는 기능**. 따라서 대규모 시스템에서 머신 러닝 모델을 매우 효율적으로 훈련 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 텐서플로의 랭크와 텐서\n",
    "\n",
    "텐서플로 라이브러리를 사용하여 텐서에 대한 연산과 함수를 계산 그래프로 정의 가능.\n",
    "\n",
    "* 텐서 : 데이터를 담고 있는 다차원 배열에 대한 일반화된 수학적 용어\n",
    "* 랭크 : 텐서 차원 (ex. 벡터는 랭크 1인 텐서, 행렬은 랭크 2인 텐서 등)\n",
    "\n",
    "### 14.2.1 텐서의 랭크와 크기 확인하는 방법\n",
    "\n",
    "tf.rank 함수를 사용해 텐서 랭크 확인 가능. tf.rank는 출력으로 텐서를 반환함. \n",
    "\n",
    "텐서 랭크 외에 텐서플로 텐서의 크기를 얻을 수도 있다. ex) X가 텐서라면 X.get_shape()을 사용해 크기 반환 가능\n",
    "\n",
    "* get_shape() 메서드 : TensorShape이라는 특별한 클래스의 객체 반환\n",
    "\n",
    "**tf.rank 함수와 텐서의 get_shape 메서드를 사용해 텐서 객체의 랭크와 크기를 추출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spyder\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크기 :  () (4,) (2, 2)\n",
      "랭크 :  0 1 2\n"
     ]
    }
   ],
   "source": [
    "# t1, t2, t3 텐서 정의\n",
    "t1 = tf.constant(np.pi)\n",
    "t2 = tf.constant([1, 2, 3, 4])\n",
    "t3 = tf.constant([[1, 2], [3, 4]])\n",
    "\n",
    "# 랭크 구하기\n",
    "r1 = tf.rank(t1)\n",
    "r2 = tf.rank(t2)\n",
    "r3 = tf.rank(t3)\n",
    "\n",
    "# 크기 구하기\n",
    "s1 = t1.get_shape()\n",
    "s2 = t2.get_shape()\n",
    "s3 = t3.get_shape()\n",
    "\n",
    "print('크기 : ', s1, s2, s3)\n",
    "print('랭크 : ', r1.numpy(), r2.numpy(), r3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과)\n",
    "\n",
    "* t1 텐서는 단순한 스칼라이므로 랭크가 0 -> 크기는 ()\n",
    "* t2 벡터는 네개의 원소를 가지므로 랭크가 1 -> 크기는 원소 하나로 이루어진 튜플 (4, )\n",
    "* t3는 2 X 2 행렬이므로 랭크가 2 -> 크기는 (2, 2) 튜플\n",
    "\n",
    "## 14.3 텐서를 다차원 배열로 변환\n",
    "\n",
    "텐서 변환에 사용되는 여러 가지 연산들 중 일부 연산들은 넘파이 배열 연산과 매우 비슷하다.\n",
    "\n",
    "랭크 2 이상인 텐서를 다룰 때 전치 같은 변환은 주의를 기울여야 함. \n",
    "\n",
    "넘파이는 arr.shape 속성으로 배열 크기를 얻을 수 있지만 텐서플로는 tf.get_shape 함수 사용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.  2.  3.  3.5]\n",
      " [4.  5.  6.  6.5]\n",
      " [7.  8.  9.  9.5]], shape=(3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1., 2., 3., 3.5],\n",
    "                [4., 5., 6., 6.5],\n",
    "                [7., 8., 9., 9.5]])\n",
    "\n",
    "T1 = tf.constant(arr)\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1의 크기 :  (3, 4)\n",
      "T1의 크기 :  (3, 4)\n"
     ]
    }
   ],
   "source": [
    "s = T1.get_shape()\n",
    "print('T1의 크기 : ', s)\n",
    "print('T1의 크기 : ', T1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float64, numpy=\n",
      "array([[-1.38467808, -1.73132382,  0.9554174 , -0.47819291],\n",
      "       [-0.29833213, -0.72336217, -1.17265655, -0.80633148],\n",
      "       [ 0.19441507, -1.17484701, -0.51716567,  0.62007094]])>\n"
     ]
    }
   ],
   "source": [
    "T2 = tf.Variable(np.random.normal(size=s))\n",
    "print(T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([ 1.11620694, -1.33072509,  1.94767401])>\n"
     ]
    }
   ],
   "source": [
    "T3 = tf.Variable(np.random.normal(size=s[0]))\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T2를 만들기 위해 s를 사용했고 T3를 만들 떄는 s의 인덱스를 참조. \n",
    "\n",
    "텐서플로 1.x에서는 차원 크기를 얻기 위해 TensorShape 객체의 인덱스를 참조하면 Dimension 클래스의 객체 반환.\n",
    "\n",
    "따라서 다른 텐서를 만들 때 바로 사용할 수 없고 s.as_list()처럼 TensorShape 객체를 파이썬 리스트로 변환한 후 인덱스를 참조해야 함.\n",
    "\n",
    "반면에 텐서플로 2.x에서는 편리하게 TensorShape 객체의 인덱스를 참조하면 스칼라 값이 반환.\n",
    "\n",
    "< 텐서 크기를 바꾸는 방법 >\n",
    "* 넘파이에서는 np.reshape이나 arr.reshape 사용\n",
    "* 텐서플로에서는 tf.reshape 함수를 사용\n",
    "\n",
    "**텐서 T1을 랭크 3인 T4와 T5로 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[1.  2.  3.  3.5 4.  5.  6.  6.5 7.  8.  9.  9.5]]], shape=(1, 1, 12), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T4 = tf.reshape(T1, shape=[1, 1, -1])\n",
    "print(T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.  2.  3.  3.5]\n",
      "  [4.  5.  6.  6.5]\n",
      "  [7.  8.  9.  9.5]]], shape=(1, 3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T5 = tf.reshape(T1, shape=[1, 3, -1])\n",
    "print(T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이에는 배열을 전치할 수 있는 방법이 세가지 존재. \n",
    "\n",
    "* arr.T\n",
    "* arr.transpose()\n",
    "* np.transpose(arr)\n",
    "\n",
    "텐서플로에서는 tf.transpose 함수를 사용.\n",
    "(일반적인 전치 연산 외에 perm[...]에 원하는 순서대로 차원을 지정해 변경 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. ]\n",
      "  [4. ]\n",
      "  [7. ]]\n",
      "\n",
      " [[2. ]\n",
      "  [5. ]\n",
      "  [8. ]]\n",
      "\n",
      " [[3. ]\n",
      "  [6. ]\n",
      "  [9. ]]\n",
      "\n",
      " [[3.5]\n",
      "  [6.5]\n",
      "  [9.5]]], shape=(4, 3, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T6 = tf.transpose(T5, perm=[2, 1, 0])\n",
    "print(T6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.  4.  7. ]\n",
      "  [2.  5.  8. ]\n",
      "  [3.  6.  9. ]\n",
      "  [3.5 6.5 9.5]]], shape=(1, 4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T7 = tf.transpose(T5, perm=[0, 2, 1])\n",
    "print(T7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.split 함수를 사용해 텐서를 작은 텐서의 리스트로 나누는 작업도 가능.\n",
    "\n",
    "-> 출력 결과는 하나의 텐서가 아닌 텐서의 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1, 3, 2), dtype=float64, numpy=\n",
      "array([[[1., 2.],\n",
      "        [4., 5.],\n",
      "        [7., 8.]]])>, <tf.Tensor: shape=(1, 3, 2), dtype=float64, numpy=\n",
      "array([[[3. , 3.5],\n",
      "        [6. , 6.5],\n",
      "        [9. , 9.5]]])>]\n"
     ]
    }
   ],
   "source": [
    "t5_splt = tf.split(T5,\n",
    "                   num_or_size_splits=2,\n",
    "                   axis=2)\n",
    "print(t5_splt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텐서 연결** : 크기와 dtype이 같은 텐서 리스트가 있다면 tf.concat 함수로 연결해 하나의 큰 텐서 생성 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.ones(shape=(5, 1), dtype=tf.float32)\n",
    "t2 = tf.ones(shape=(5, 1), dtype=tf.float32)\n",
    "\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t3 = tf.concat([t1, t2], axis=0)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t4 = tf.concat([t1, t2], axis=1)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 텐서플로의 계산 그래프 이해\n",
    "\n",
    "텐서플로 2.x 버전에서는 즉시 실행 모드가 기본으로 활성화되어 있어 계산 그래프를 만들지 않고 빠르게 개발과 테스트 가능.\n",
    "\n",
    "계산 그래프는 단순히 노드들의 네트워크.로, 각 노드는 한 개 이상의 입력 텐서를 받고 0개 이상의 출력 텐서를 반환하는 연산으로 표현 가능.\n",
    "\n",
    "ex) 랭크 0 텐서(스칼라) a, b, c가 있을 때 z=2X(a-b)+c를 평가한다고 가정했을 때, 텐서플로 2.x에서는 계산 그래프를 만들지 않고 바로 텐서 z 계산 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 * (a-b) + c =>  1\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "\n",
    "z = 2 * (a - b) + c\n",
    "\n",
    "print('2 * (a-b) + c => ', z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.x 버전은 계싼 그래프를 만든 후 세션을 통해 그래프를 실행. \n",
    "\n",
    "< 텐서플로 1.x 버전에서 계산 그래프를 만들고 실행하는 단계 >\n",
    "1. 비어 있는 새로운 계산 그래프 생성.\n",
    "2. 계산 그래프에 노드(텐서와 연산) 추가\n",
    "3. 그래프를 실행\n",
    "    - 새로운 세션 시작\n",
    "    - 그래프에 있는 변수 초기화\n",
    "    - 이 세션에서 계산 그래프 실행\n",
    "    \n",
    "위의 예시처럼 식 z=2X(a-b)+c를 평가하는 그래프를 만든다면 tf.Graph()를 호출해 그래플르 만들고 다음과 같이 노드 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c =>  1\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "# 그래프에 노드 추가\n",
    "with g.as_default():\n",
    "    a = tf.constant(1, name='a')\n",
    "    b = tf.constant(2, name='b')\n",
    "    c = tf.constant(3, name='c')\n",
    "    \n",
    "    z = 2*(a-b) + c\n",
    "\n",
    "# 그래프 실행\n",
    "with tf.compat.v1.Session(graph=g) as sess:\n",
    "    print('2*(a-b)+c => ', sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with g.as_default()를 사용해 그래프 g에 노드 추가\n",
    "* tf.Session을 호출해 세션 객체를 만들고 실행할 그래프를 매개변수로 전달\n",
    "* tf.Graph() 객체의 get_operation() 메서드를 사용해 그래프에 들어있는 연산 출력\n",
    "* as_graph_def() 메서드를 호출하면 포멧팅된 문자열로 그래프 정의 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'a' type=Const>,\n",
       " <tf.Operation 'b' type=Const>,\n",
       " <tf.Operation 'c' type=Const>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'add' type=AddV2>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"a\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"b\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"c\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"sub\"\n",
       "  op: \"Sub\"\n",
       "  input: \"a\"\n",
       "  input: \"b\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul/x\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul\"\n",
       "  op: \"Mul\"\n",
       "  input: \"mul/x\"\n",
       "  input: \"sub\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"mul\"\n",
       "  input: \"c\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 175\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 2.x 버전에서는 tf.function 데코레이터를 사용해 일반 파이썬 함수를 호출 가능한 그래프 객체로 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c =>  1\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a-b)+c\n",
    "    return z\n",
    "\n",
    "print('2*(a-b)+c => ', simple_func().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_func() 함수에서 반환되는 텐서를 바로 numpy() 메서드로 호출해 출력. \n",
    "\n",
    "simple_func() 함수는 보통 파이썬 함수처럼 호출할 수 있지만 데코레이터에 의해 객체가 변경."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.eager.def_function.Function'>\n"
     ]
    }
   ],
   "source": [
    "print(simple_func.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(a-b)+c =>  1\n"
     ]
    }
   ],
   "source": [
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a-b)+c\n",
    "    return z\n",
    "\n",
    "simple_func = tf.function(simple_func)\n",
    "\n",
    "print('2*(a-b)+c => ', simple_func().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.function으로 감싼 함수 안의 연산은 자동으로 텐서플로 그래프에 포함되어 실행 -> 자동 그래프 기능\n",
    "\n",
    "**simple_func가 만든 그래프에 있는 연산과 그래프 정의 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'Const_2' type=Const>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_func = simple_func.get_concrete_function()\n",
    "con_func.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"Const\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_1\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_2\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"sub\"\n",
       "  op: \"Sub\"\n",
       "  input: \"Const\"\n",
       "  input: \"Const_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul/x\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul\"\n",
       "  op: \"Mul\"\n",
       "  input: \"mul/x\"\n",
       "  input: \"sub\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"mul\"\n",
       "  input: \"Const_2\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"add\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 175\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 텐서플로의 변수\n",
    " \n",
    "텐서플로에서 변수는 특별한 종류의 텐서 객체로, 훈련 과정 동안 모델 파라미터를 저장하고 업데이트할 수 있다.\n",
    "\n",
    "ex) 신경망의 입력층, 은닉층, 출력층에 있는 가중치\n",
    "\n",
    "변수를 정의할 때 초기 텐서 값을 지정해야 함. \n",
    "\n",
    "텐서플로 변수를 정의하는 방법\n",
    "\n",
    "> tf.Variable(< initial-value >, name=< optional-name >)\n",
    "\n",
    "tf.Variable에는 shape이나 dtype을 설정할 수 없고, 크기와 타입은 초깃값과 동일하게 설정된다.\n",
    "\n",
    "**텐서플로 1.x 형식의 변수 (그래프 컨텍스트 블록 안에서 변수를 그래프에 추가)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\spyder\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "<tf.Variable 'w1:0' shape=(2, 4) dtype=int32>\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph()\n",
    "\n",
    "with g1.as_default():\n",
    "    w1 = tf.Variable(np.array([[1, 2, 3, 4],\n",
    "                               [5, 6, 7, 8]]), name='w1')\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 w1은 실제 값이 포함되어 있지 않으므로 g1 그래프에 포함된 연산을 출력해보면 변수 초깃값(initial_value)과 할당 연산(Assign)이 각각 존재함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'w1/Initializer/initial_value' type=Const>,\n",
       " <tf.Operation 'w1' type=VarHandleOp>,\n",
       " <tf.Operation 'w1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>,\n",
       " <tf.Operation 'w1/Assign' type=AssignVariableOp>,\n",
       " <tf.Operation 'w1/Read/ReadVariableOp' type=ReadVariableOp>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 변수는 사용하기 전에 초기화를 해 주어야 함.\n",
    "\n",
    "변수 하나씩 초기화 연산을 실행해도 되지만 변수가 많을 경우 번거롭기 때문에 global_variables_initializer() 함수를 이용해 그래프에 있는 변수 초기화 연산을 한 번에 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"init\"\n",
      "op: \"NoOp\"\n",
      "input: \"^w1/Assign\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    print(init.node_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    w1 = w1 + 1\n",
    "    print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4 5]\n",
      " [6 7 8 9]]\n",
      "[[2 3 4 5]\n",
      " [6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(graph=g1) as sess:\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.x 버전에서 변수를 다룰 때 일반적인 파이썬 프로그래밍으로 코드를 작성하면 w1은 텐서플로 변수가 아니라 덧셈 연산을 가리키는 텐서로 변경되어 w1을 여러 번 실행하더라도 변수 값이 증가하지 않고 원래 초깃값에 1을 더한 결과만 반복된다.\n",
    "\n",
    "따라서 텐서플로 1.x 버전에서는 assign() 메서드를 사용해 변수 값을 증가시키는 연산을 만든 후에 변수를 초기화하고 변수 값을 증가시키는 연산을 실행해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4 5]\n",
      " [6 7 8 9]]\n",
      "[[ 3  4  5  6]\n",
      " [ 7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "\n",
    "with g2.as_default():\n",
    "    w1 = tf.Variable(np.array([[1, 2, 3, 4],\n",
    "                               [5, 6, 7, 8]]), name='w1')\n",
    "    w1 = w1.assign(w1 + 1)\n",
    "    \n",
    "with tf.compat.v1.Session(graph=g2) as sess:\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텐서플로 2.x 버전에서의 텐서플로 변수는 파이썬 객체 자체**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w2:0' shape=(2, 4) dtype=int32, numpy=\n",
      "array([[1, 2, 3, 4],\n",
      "       [5, 6, 7, 8]])>\n"
     ]
    }
   ],
   "source": [
    "w2 = tf.Variable(np.array([[1, 2, 3, 4],\n",
    "                           [5, 6, 7, 8]]), name='w2')\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.x와 달리 2.x에서는 w2+1처럼 계산이 덧셈 연산의 출력 텐서가 아니라 덧셈이 적용된 상수 텐서를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3 4 5]\n",
      " [6 7 8 9]], shape=(2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(w2+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 텐서플로 2.x에서는 변수 값을 증가시키려면 덧셈 결과를 assign() 메서드로 반복해 전달하면 된다.\n",
    "\n",
    "호출결과는 변수에 즉각 반영 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4 5]\n",
      " [6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "w2.assign(w2+1)\n",
    "print(w2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  5  6]\n",
      " [ 7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "w2.assign(w2+1)\n",
    "print(w2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2 변수를 다시 출력해보면 numpy 속성이 바뀐 것을 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w2:0' shape=(2, 4) dtype=int32, numpy=\n",
      "array([[ 3,  4,  5,  6],\n",
      "       [ 7,  8,  9, 10]])>\n"
     ]
    }
   ],
   "source": [
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 tf.keras API 자세히 배우기\n",
    "\n",
    "### 14.6.1 Sequential 모델\n",
    "\n",
    "Sequential 모델은 층을 순서대로 쌓은 모델을 만든다. \n",
    "\n",
    "**$y = wx+b$인 선형 회귀 모델 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO2df2xd53nfvw+pE/nSDUy5ZpaYsSxny6TFMCxFnOtVw1ApWZTGta1ZydQu3dK1gxEMBWoh40rXwewMHcyMWFUM7TB4a4EO9VzGpsPZdQLFmVQMEyAnpElGUSy1TuJfV16j1KbbWNfWJfnuj3sPdXh43nPec857frz3fj+AIPLew/M+9733ft/3fd7neV5RSoEQQoi7DFRtACGEkHxQyAkhxHEo5IQQ4jgUckIIcRwKOSGEOM6WKhq97rrr1I4dO6pomhBCnGV+fv7HSqmR8OOVCPmOHTswNzdXRdOEEOIsIvJy1ON0rRBCiONQyAkhxHEo5IQQ4jgUckIIcRwKOSGEOE4lUSuEEFIkswtNTB0/jwvLLVw/3MD4wZ04tGe0arMKg0JOCOkpZheauP/JM2i1VwEAzeUW7n/yDAD0rJjTtUII6Smmjp9fF3GfVnsVU8fPV2RR8VDICSE9xYXlVqrHewEKOSGkp7h+uJHq8V6AQk4I6SnGD+5Ewxvc8FjDG8T4wZ0VWVQ83OwkhPQU/oYmo1YIIcRhDu0Z7WnhDkPXCiGEOA6FnBBCHIdCTgghjkMhJ4QQx6GQE0KI41DICSHEcSjkhBDiOIwjJ4SQCFwqhUshJ4SQEK6VwqVrhRBCQrhWCpdCTgghIVwrhUshJ4SQEK6VwqWQE0JICNdK4XKzkxBCQrhWCpdCTgghEbhUCpeuFUIIcRwKOSGEOA5dK4QQkoE6ZX5SyAkhJCV1y/yka4UQQlJSt8xPCjkhhKSkbpmfFHJCCElJ3TI/KeSEEJKSumV+UsgJISSB2YUm9k2ewE0Tz2Df5AkAwMP33ILR4QYEwOhwAw/fcwujVgghJIkqQv50ESoP33MLTk0cKLRtU3LPyEXkBhE5KSIviMhZEfkNG4YRQkgQX1Cbyy0oXBHU2YVmoe3WLUIlChsz8hUAX1BKPS8i7wUwLyLPKqW+Z+HehBACIF5Qi5yVZ41QKXP1kHtGrpR6XSn1fPfnvwHwAgA3Ks0QQpyhqpC/LBEqZa8erG52isgOAHsAPBfx3L0iMicicxcvXrTZLCGkD6gq5C9LhErZ7hhrQi4iPwVgBsB9Sqm/Dj+vlHpEKTWmlBobGRmx1SwhpE+oKuTv0J5RHN47ikERAMCgCA7vjS9xW/bqwYqQi4iHjog/qpR60sY9CSEkyKE9o5WE/M0uNDEz38SqUgCAVaUwM9+MdZOUvXrIvdkpIgLgDwC8oJT6nfwmEUJINHGHPRS1uZhlk3X84M4NIYtAsasHG1Er+wD8cwBnRGSx+9hvKaW+ZuHehBCSSJHVCNO4SYKDyfCQh61bBvBWq1141EpuIVdK/V8AYsEWQkhJ1KmWtg1shSZG9cv1ww00I0Q77CYJDyZvXmqj4Q3i2JHdhfctU/QJ6TOqSqwpEhubi7p+2b9rZNMmKwC8/e7Khj6rMnGIQk5In+FCpmJabGwu6vrl5LmLePieW7BtyNvw3HKrvWEArLK0LYWckD6jbrW0bZAUmhguehW1+ojrl0N7RjH0ns2e6OAAWGVpWwo5IX1G3Wpp6zARX5+40ERTV1JSvyQNgFWWtmX1Q0L6jLJD47KQJQpFF5pouhGa1C9Jm57+varYRKaQE9JnVCk4ptgskGXqSkrqF5MBMC7OvUgo5IT0AGnDCX3B8f/u6PQipo6fr42g2/Tjm4YPAvFCXOcBkEJOiONkTYYpMokmrk0TIUwjvknYdCVVNeNOgpudhDhO1nDCssMQ08Sv29w4rKpGS5lwRk6I42R1Q5QdhpjG723bjVHXmbQtKOSEOE5WN4RN94UJaQcO2+Lba2UJgtC1QoiDBGOsL11egTewsdyRiRui7LjnKuPXvzh7BkenF3uqLEEQCjkhjhH2Nb95qQ0IMNzwUvmAy/Ydxw0caZJ/0jK70MSjp1+BCj3uelmCIHStEOIYUb7m9qrC1Vu3YPHBT6S6V5m+Y53fG0Ch0TNTx89vEnEfl8sSBKGQE+IYUX7tuMfrRNTAsW/yhLXknyjixLpuZQmyQtcKIY7hnx1p+njdKTp6RifWAtSqLEEeKOSEOIZ/dqTp43UnzSZoFl96lG9eAHz29u2MWiGEVMOoRvh0j9cd0+iZrAdiRG3qHjuyG7996BbLr6Q66CMnxDFcqF6YBtPknzyFtJgQRAjJhe1ElDoXb8qKidDG+dJ7OdnHBAo5IQVSVGGqXp9hRqHLRL2m4ZVe/Ktu0EdOSIFUcT5m1uSaIpNybKDzpYvAiTNIi+zfvpiR9/uyi1RH2YWpbJa0HX9iCQ89dRZvtdq1+N7oXEpHpxcjr69Tsk/RJYN7XsirqLlMiE/ZhamybgjqskWXW20A9fneRLmUpo6fL7WPs2DzxKMoet61UsXSlhCfsgtTmawAopb4JrPXun5vqjz02JSiV2Y9PyMve2lLSJCyI0ySVgC6FerwkNcpvpWA6ffGd2c2l1sYFMGqUhgt6LW7EMVT9Mqs54W87KUtIWHKjDBJijHXrVC3bhlAwxvc9FwYk+9NeLDwM06LdM/UPYqn6Nj/nnetuLDsIiQPQVfJ1PHzOLx3VFuaVjejfqvV3pD9uG3Iy1TjHIgeLHzq6p6xhS4ypeiSwT0/I3dh2UVIVqJcJTPzTa1IxK1Qw7ParNFeWY+Yc52kwIoiVw09L+RA/ZddpDcpI+w1bTREmiV+1u+NbrAIPt+LFB2ZEkfPu1YIqYKsBZ7SkuUczKJPBYpyZ/r0sluzysCKvpiRE1I2aWdnWWfvWTbzi16hBt2ZZUSt1IUqAyso5IQUQJrZWZ6ktaKiIfK6hbIOFlVlYdtod/+ukU1ng5a1AqGQE1IAaWZnecuz+vewJX5VZUPbbtdUnG20O7vQxMx8c4OIC4DDe8vZn6OPnJAC2L9rxPjxvL7VQ3tGcWriAI4d2Q0AODq9mFiUKa6AU1XZ0DbbTbNHYaPdqHsoACfPXUxtexYo5IQUgO4LHPV4mqPOdKQRrqRrq9q0092/udxKXTEwjTjbeL1xtpeBFSEXkU+KyHkReVFEJmzckxBXSFO7JOpxXdLa/l0jxmVP0whX0rU2BpYsxN0/beRPmv638XrjDnguoxxwbiEXkUEAvw/g5wF8BMAvichH8t6XEBfQzW6Hh7zI66O+8FEhgYf3jmJmvmkcvphGuJKurSobOi5s0cfU5ZFGnG283vGDOyERjyuglExWG5udtwF4USn1AwAQkT8BcDeA71m4NyG1Jk3tkjhxCEd57Js8kWoDNG5zNbzppyuQ5YtcVPhgUEDzbt7pNiHDG7dK8/cmLo+0iU/BdrNG6dxXYV10G0I+CuDVwO+vAfiZ8EUici+AewFg+/btFpolpHriapccO7I7szjofKu69nTCtX/XyKaIDG9A4A0K2qtXpFKwcSPWt9N29EqaNPZ9kycSKznq+jetONuIrR91PI5ct6LY+IBSjwB4BADGxsZ0gy0hTpGmdokpswtNCCK+RNgsCkExGx7ysHXLAN5qtTE85EEp4I9Pv7LpHu21zXdWAGbmmxi78doNImg75TzNPeNm1SYhg2WX5ii6wmEcNjY7XwNwQ+D3DwK4YOG+hNSeNP5V0zMbp46fjxRx6bYXvF/QP//mpTbeXVnDZ2/fjnfaa+un+5gS9j8XEb2S5p5x5QTqeGBMGeUPdNiYkX8bwIdF5CYATQC/COCfWbgvIbXHdAmfJulEJ3YqdK1OzP7nc68gYtJtRLDtIlLO095TN6uu64ExVRXoyz0jV0qtAPh1AMcBvADgK0qps3nvS4gr+Ak5P5y8A6cmDkR+kU1nkLMLTQxIlLeyM8MLohOtrCIObBTUqNWGoDMIZT0F3lZETFUhknXFShy5UuprSqm/q5T620qp/2DjnoT0EiYJI/6s3T9RJ0iU2GURrUHNIBHVRtBVAGCD3z5rNUdb7gceGLMRZnYSEoOpXzsJk4SRuJN1omp2mMRdB/ndI7vxn/7prZF/M9zwIgXVX22MDjc2+e3T+qT9vjzaDdM7dmS3dgWTRJX+6DrColmEaLBZxGn84E4cnV7cJIZ+wsihPaOx/t2o1P4o//zb765EbnION7xN/vU0YZFZfNLhiJqfvLOyHjFjI5SRB8ZcgUJOiIa84XfhOOe4BBffNx7lVvGviSLqeLZwCJwAWG61sW/yxKbkG1PSblKG7YhKQCrr9Jx+gEJOnGJ2oYmHnjq7PuvcNuThwTtvLkQM8kRGRM3mdQwPeVrfuI+pPzyclRnl1w5eZ0raGOk4N1EQf+OU5+nmgz5y4gyzC02MP760wXXw5qU2xp9YKqQwUZ7ICFMha3iDUAqJ1+rK4kZh068dvGcan7RpGKAfBVPkcXhpsbUvUiackfcIVZ2sUiZTx89HZiW2VxW+8JUlAHYPPsiTqRcnZAMCKIX19+mopkZHkCx1rfP6taPS3k37N+kAZgCR2atVu1uqOlQjLxTyHqCMk1UAu6fQZCFOgFaVsv6Fy1NMKU7I1hTwy7dvx8lzF3F0ejHWN+6TJdElr1+7udzC+ONL+NLTZ7F8qZ3q9UcNgt6g4Or3bMFbrXZs/1SZ1FPWWau2oZD3ADZrYkR+mZ9YAhSsRhxkIWmWV8RsLmtkhC5KxSdYAyVJxIFsMeM2/NrtNbW+UZnmfTcZBJOKYlVBWWet2oZC3gPYTFeO/DKvbhaaKpbA4wd3YvzxpUj3ik9ZJ7IkcWjPKOZefiOyaJUO6bpcwi6HJHeOaVnYpBmjyeclzfueNAju3zUS2T9p9gOC2Jgdl3XWqm0o5D2AzZoYNo63Kgr/yxGMWokiGGaXl9mFJr709Nn1Welww8NDd5lFyfz2oVswM/8aWu01o7aUAl6avCOVIKUpC5uEiV8bsPe+pzkOLwlbs+M0q5g61XvpKyGviz/LNjbLZ17T8Iyr5lWxBPaFacfEM9prgl9iILtvf3ahifEnljasSJZbbYw/fmVjVfeZ8sMkTUU86jWaYHNWGPU5ikLBzmBZ9EoySz+kWcUUUVQsK30j5HXyZ9kgLCCH947i5LmLuQap2YUm3r68sunxge5aPyhJ3oDkqmuRd1DVFfH3abVX8aWnz+Kd9prRex5lz9Tx85FupfaaWg/hi/pMzb38Bqa/9WqsCyiK4caV4+FM+8emGIZF7JqGh7cvr0T2gY3vTxkryaz9kHVDt6p6L30TR17H+sVZiToncma+ifGDO2Mr8CWhE66GN4jBwVCxJX3tpUTSnPiuw6TOyJuX2sYVB6PsiRsoLiy3tJ+px55LL+LegOChu25et2f8iaUN9uhi5YusAnj11i048vdv2FR10Sfv98e08JVJXHcV1RDrVO+lb4S8Tv6svBQ1KOn64u3Lq5sEvr2qMrdnw/5wZb40hF+nzp64SoHXDze0/WUShQJgXQC2DXm4eusWHJ1exL7JE3jgq2ci+/tLT2+uDm2zCmDcBEFHnu+PiRCaDvpVVUM0KWFcBn3jWqmTPysvRQ1Kpptdpu3p3AO27PeXwFH1RRreILZuGYj094ff8zhBDp9tCXRmP5cur2hDCwcN4sJHhxs4NXEgVSq/v+FahFsN0A9oDz111vjoubQkuTFMfd95Yv57gb4R8jr5s/JS1KCk6yMTQQyLy/5dI5iZb0b6p23br/sSAzB6z3X2jHbvE4xaaXgDWAnEVodpeIM4vHc0NuzQGxBcuryCmyaeMUoGChIl/DPzTStLet2Aptv8Dh89VwRpj4brF+EO0zdC3ksjdlGDUlZBjBKXR0+/Epl+/YWvLOGXfuaGDSJvw/64L3HSex7Xn+H76pJYgCvC718fJeZD3sCGJJs0Ij7c8AqNXU67IgsfPVcEvbSSLhJRKT5IthgbG1Nzc3Olt+sqdUiZj4uiiBO3KPxZqw13gC1Mo0RumnhGezDyDyfvSLynX5UwLd6AYOozt2qzRaPaT4vORXWVNxC5AvHdQ0Wis6lfD5EQkXml1Fj48b6ZkbuKLmzy4XtuKfxLFCRuxpvWt91qr+LkuYuF2J81rNF0WZ5mhhh1T5MCWXEDnW4gsDFDzeuiKoJeWkkXCYW85tQhDThJHHXiptsgA9KLv4lARw16900v4qGnzhpnYyaR162l66tBEawplShURe/15HFRFUU/+75NoZDXnKrDJk0SqXTicnjvKB577tVIP3CaGaRpMpeuBvhyq20t+SvvDFHXV6augqpmqP0kpi5mgFPIa07Vmz0mK4I4cRm78drcM0idDV/4yhKOTi+utxc3uNlcxeQRNRtC3E+iWjauZoBTyGuO7QpxOvLGfOvExYZwJSXe+F+24SFPGxYYd5+yoRDXlzq4MrNAIa85NivE6fji7JkN4YK2Y77zCpdJWFyrvYqtWwYiE3iC9yEkjqpdmVlxVsjz+rFc8YMV/cGaXWhqY76njp+vJJEqKrlo+tuvagXaZ7nVhjcQnVYvyL+KceUzQ7JTtSszK04KeV4/lkt+sKI/WFPHz8dGlpS5uRau/Q103pvpb7+K1QQR99EVq1IAZuabGLvx2ky2u/SZIdlxNQPcyaJZeYsuuVQJsehiQHEze3+wKKMwkC+UUT7u9qpC+qrem8nzHrv0mSHZqVNFwzQ4OSPXiY9pxlxed0WZS+yiZ8RxMeBlzkJ0oYO2Cb7Had5HV32nJD0ubkY7KeRx4jO70Ex8E/K4K7IusfOIf5EfrKilpAD47O3brbaZ9PrLEkT/PU77PrrqOyX9gZOulfGDOyPPNVCA0VI3j7sirtSnrvi9SU1lk+L5RRBeSg43PAwPeXj09CvW7DB5/XGC6A2KdhMzzLYhb71Gefgvgu9xWldJVfWuCTHBSSE/tGc0V+p3Hj9YXKlPnVAliYaNE3Py4PvAjx3ZjXdX1vDmpbZVO0xEU3fiz3DDw9Snb8XUZ27d8H798u3bI4X1wTtvxqmJA3hp8g4cO7Jb+x6ndZW46jsl/YGTrhVAf2aj6VI3q7vCtNRnMIkgSTTqkoRQlB0mommyFxC2YezGaxOv19mdxVXiou80CYZU9gbOCnlVYUKmJ40DV4QqSTTqspFW9slDYdFMK5R5hNXVMDObMKSyd3DStQJUt9QNtpuEL1RJ/tUqDo5N056Nk4fq5l+mq4Qhlb2EMzNy3RKwii+e367ukAFgo1AluQ3qMjuMsiN4LFnWpXdda0r3oqskDXVZCZL8OCHkdV0CxtWWDs/u4kSjLkIXtuOahoe3L6+sJ+nk6fd+F806wpDKbNRxXyHXUW8iMgXgTgCXAXwfwL9USi0n/V3ao950R4kFj5qqonN7/Rgqk34n7tLrn98iqLrPijrq7VkA9yulVkTkywDuB/CbOe+5iaQlYBEzdpOBIc1MOs9AU9UMwKUMWJKeuqwEXaIuEWZhcgm5UuobgV9PA/h0PnOiSVoC2u5ck4EhLFLHjuzWtpVnoKnSrVRFBmzUfSg0xUGXVzrquq9gM2rlVwF8XfekiNwrInMiMnfxYrpa2klRD7Y713YCT57ogCojC4rIgE1jd9WJUoSEqUuEWZhEIReRb4rIdyP+3R245gEAKwAe1d1HKfWIUmpMKTU2MpKuLnRSqJjtzo0ryrVv8gTum15MJVJ5BpoqZwBFZMCmsZvhcaQu+CU0msut2NIPVZHoWlFKfTzueRH5HIBfAPAxlWfnNIG4JWCa8D2TpXpcUa64rE6dSOVxUdiKLMjqorCdAZvG7rouY0l/EXYTKnS0QKEzuamDuy+Xa0VEPonO5uZdSqlLdkxKj+nM0XSpHuVS8N+4OHQilcdFYSOZRve6vzh7prBCXTbsrusylvQXUStDX8SLqs+flrxRK78HYCuAZ0UEAE4rpT6f26oMmMwckzZFg7PWaxoervIGsHypbVRfJU6k8kQH2Igs0L1u3TmdNj6YNuyuS6IU6W9cWBnmjVr5O7YMKYO4NyS8fFputdHwBtejUXQx1YDZ8ipPdECav/UHo+ZyC4Mi6yfNR6E7p9PWDCNvRMShPaOYe/kNPPbcq1hVCoMiOLyXURakXFxInHK21koW4pbqSRtrOlfB7x7ZXZvlVdCFAiBWxHXUaZYxu9DEzHxz/XWsKoWZ+SajVkip1LFWUJi+EvK4NyRp+ZQngqOsQyPSHJemO6bB9iwjz2tn1AqpAy4UWHOi1oot4vy2vjsiTFDYsrgKykzoMZlNCzqvaf+uEczMNwv1P+d97S74Jkl/UPfEqb4SckD/hhS1sVZmSm/Spuxww8Pig59Y/z3pYIa85H3tLvgmCakDfSfkOmxEWUTFapc5q0w69OLtyysbDqcuepaR97UzaoUQM5wT8iJrbwSFzW/n6PSiUTs6N8I1DQ/Lrfam64uYVQYHo6iZbHtVxc6Gbfdt3hk1izoRYoZTQl6WvzlLOzo3wlXeABreYGmzyqRDL3Sz4SL61saMuu6+SULqgFNRKzajGOKiKbK0oxPI5UvtTTveh/d2NleLjGJJmxVZRISIC7v9hPQCTs3Ibfmbk2afpu0EXREDmuSb64cbm1w2Zawq0s6Gi/Llc0ZNSPE4NSO3VXsjafZp0k64fkmUiEcJZ1mx0eHZ8LYhD1u3DODo9GLkKoB1TQhxF6eE3FaGVdLsM6odAHjj7XfXBVCXfDMoEutGKDOK5dCeUZyaOIBjR3bjnfYallttbbGwqrPXykqaIqQXccq1YiuKISmawq/x8cenX9nwfKu9hvHHlwDohXdNKfxw8o7UbQ+I5DqpPg6TeO4qI0Tqerg2Ia7glJADdnyuJv7jk+eiTzFqr3VC+LKG1ulivX3XjC9icy+/gZPnLloRVdNVQFX+7Lqeg0iIKzjlWrGFSTRFnKvjwnIrsysi3PagbK564peYtXXEmW5wuabhZbqfbZiKT0g+nJuR2yJp9hmX7u5HogDZa4z719008UzkNXlKzIYTe/bvGsH0t15Fe23jXcOZnlXBVHxC8tGXM3ITxg/uhDe4ebbsDQj27xrBvskTODq9CAA4lqOUrY2jz4JEnQY0M9+MfC1+pmfVVL3RSojrUMg1HNoziqlP34ptQ1fcD8MND0duuwEz801rbg/dsXJRmIi+zt98qb0WeX0d3BdMHCIkH33rWokj7Jp48M6b10Vl3+QJqxtzUS6aPCVm0wpzXdwXTBwiJDsU8hC2sj7TEBax2YUm/nTp9XUbtg15GwaTuOJWOn/ztiEP77TXWEmQkB6ErpUQtrI+sya3+ANJsGLiOwG3SJQPPOja0fmbH7zzZrovCOlROCMPYZL1GReDnje5JSmmOun5pGgaCjchvQeFPIRJ1iegF8q8yS1JA4mJa4f+ZkL6Cwp5CJOszzihzOtDTxpIGHNNCAlDH3mIvKFweasIJsVUM+aaEBKGM/II8rgm8p6KY+rj5vFnhBAfURF1tItmbGxMzc3Nld5uWRR5righpH8RkXml1Fj4cc7ILRAl3KcmDlRtFiGkT6CQ54S1tAkhVeO0kNfBhcFa2oSQqnFWyOsyE2YtbUJI1TgbfljWIcZJ8NBiQkjVOCvkcTPhMg/yZVw3IaRqnHWt6DIch4e8Ul0ujOsmhFSNs3HkYR850JkJb90ysKFyoM/ocKNnQgLrsMlLCCmfnosj182E/ePXwjSXW9g3ecJ58avLJi8hpD44OyPXsW/yRKTLRbDxQOOGN+hkPW7d6+ulFQchJBrdjNzKZqeI/BsRUSJynY375UF3BqbuVPqiKGrDleGOhJAwuYVcRG4A8I8BvJLfnPxEVS/UrTmKEr+kU3zywHBHQkgYGzPyYwD+LTZPeivj0J5RnJo4gB9O3oFTEwcwWrL4FRnjznBHQkiYXEIuIncBaCqllgyuvVdE5kRk7uLFi3maTU3Z4lek+yNvvXRCSO+RGLUiIt8E8P6Ipx4A8FsAPmHSkFLqEQCPAJ3NzhQ25qbsWO+iT/HhUW6EkCCJQq6U+njU4yJyC4CbACyJCAB8EMDzInKbUur/WbXSAmWKX97DJQghJA2Z48iVUmcAvM//XUReAjCmlPqxBbuMqGtiDLM9CSFl4mxCUN0TY6JWAHUdeAghbmOtaJZSakeZs/G6VD80pciQREJIf9OT1Q/riGsDDyHEHZwVctcSY1wbeAgh7uCskLuWGOPawEMIcQdnhdy1xBjXBh5CiDs4G7UCuJUYw5BEQkhROC3kruHSwEMIcQdnXSuEEEI6UMgJIcRxKOSEEOI4FHJCCHEcCjkhhDgOhZwQQhyHQk4IIY5DISeEEMehkBNCiONQyAkhxHEo5IQQ4jgUckIIcRwKOSGEOA6FnBBCHIdCTgghjuNsPfLZhSYPaSCEEDgq5LMLTdz/5Jn1U+mbyy3c/+QZAKCYE0L6DiddK1PHz6+LuE+rvYqp4+crsogQQqrDSSG/sNxK9TghhPQyTgr59cONVI8TQkgv46SQjx/ciYY3uOGxhjeI8YM7K7KIEEKqw8nNTn9Dk1ErhBDiqJADHTGncBNCiKOuFUIIIVegkBNCiONQyAkhxHEo5IQQ4jgUckIIcRxRSpXfqMhFAC9n+NPrAPzYsjk2oF3pqatttCsddbULqK9teey6USk1En6wEiHPiojMKaXGqrYjDO1KT11to13pqKtdQH1tK8IuulYIIcRxKOSEEOI4rgn5I1UboIF2paeuttGudNTVLqC+tlm3yykfOSGEkM24NiMnhBASgkJOCCGOU2shF5EpETknIt8Rka+KyLDmuk+KyHkReVFEJkqw6zMiclZE1kREG0YkIi+JyBkRWRSRuRrZVWp/ddu8VkSeFZG/6P6/TXPdare/FkXkqQLtie0DEdkqItPd558TkR1F2ZLSrl8RkYuBPvpXJdn1hyLyIxH5ruZ5EZH/3LX7OyLy0ZrY9XMi8lagv/5dSXbdICInReSF7nfyNyKusddnSqna/gPwCQBbuj9/GcCXI64ZBPB9AB8C8B4ASwA+UrBdfw/ATgB/BmAs5rqXAFxXYn8l2lVFf3Xb/Y8AJro/T0S9lwXMVhEAAAPUSURBVN3nflKCLYl9AOBfA/iv3Z9/EcB0Tez6FQC/V9ZnKtDuPwLwUQDf1Tz/KQBfByAAbgfwXE3s+jkAf1pBf30AwEe7P78XwJ9HvJfW+qzWM3Kl1DeUUivdX08D+GDEZbcBeFEp9QOl1GUAfwLg7oLtekEpVbuTng3tKr2/utwN4I+6P/8RgEMltKnDpA+C9j4B4GMiIjWwqxKUUv8HwBsxl9wN4H+oDqcBDIvIB2pgVyUopV5XSj3f/flvALwAIHyAgrU+q7WQh/hVdEavMKMAXg38/ho2d1hVKADfEJF5Ebm3amO6VNVff0sp9TrQ+ZADeJ/muqtEZE5ETotIUWJv0gfr13QnE28B+OmC7EljFwAc7i7FnxCRGwq2yZQ6fw//gYgsicjXReTmshvvuuX2AHgu9JS1Pqv8hCAR+SaA90c89YBS6n91r3kAwAqAR6NuEfFY7phKE7sM2KeUuiAi7wPwrIic684gqrSrkP4C4m1LcZvt3T77EIATInJGKfV9G/YFMOmDwvopBpM2nwbwmFLqXRH5PDqrhgMF22VCFf1lwvPo1Cf5iYh8CsAsgA+X1biI/BSAGQD3KaX+Ovx0xJ9k6rPKhVwp9fG450XkcwB+AcDHVNexFOI1AMFZyQcBXCjaLsN7XOj+/yMR+So6S+dcQm7BrkL6C4i3TUT+UkQ+oJR6vbt8/JHmHn6f/UBE/gydmYxtITfpA/+a10RkC4BrUPwSPtEupdRfBX79b+jsHdWBwj5XeQiKp1LqayLyX0TkOqVU4cW0RMRDR8QfVUo9GXGJtT6rtWtFRD4J4DcB3KWUuqS57NsAPiwiN4nIe9DZmCos2sEUEblaRN7r/4zOxm3kznrJVNVfTwH4XPfnzwHYtHoQkW0isrX783UA9gH4XgG2mPRB0N5PAzihmUiUalfIh3oXOr7XOvAUgH/RjcS4HcBbviutSkTk/f7ehojcho7m/VX8X1lpVwD8AYAXlFK/o7nMXp+VvZubcuf3RXR8SIvdf34UwfUAvhba/f1zdGZuD5Rg1z9BZzR9F8BfAjgetgudyIOl7r+zdbGriv7qtvnTAP43gL/o/n9t9/ExAP+9+/PPAjjT7bMzAH6tQHs29QGAf4/OpAEArgLwePcz+C0AHyqpn5Lserj7eVoCcBLArpLsegzA6wDa3c/YrwH4PIDPd58XAL/ftfsMYqK5Srbr1wP9dRrAz5Zk1z9Ex03ynYB+faqoPmOKPiGEOE6tXSuEEEKSoZATQojjUMgJIcRxKOSEEOI4FHJCCHEcCjkhhDgOhZwQQhzn/wOWXwspH1b/PQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 랜덤한 회귀용 예제 데이터셋 생성\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "def make_random_data():\n",
    "    x = np.random.uniform(low=-2, high=2, size=200)\n",
    "    y = []\n",
    "    for t in x:\n",
    "        r = np.random.normal(loc=0.0,\n",
    "                              scale=(0.5 + t*t/3),\n",
    "                              size=None)\n",
    "        y.append(r)\n",
    "    return x, 1.1726*x - 0.84 + np.array(y)\n",
    "\n",
    "x, y = make_random_data()\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 샘플 개수는 200개로 150개는 훈련 세트로, 50개를 테스트 세트로 나눔.\n",
    "\n",
    "kf.keras는 모델을 훈련할 때 간단한 설정만으로도 검증 세트의 점수를 자동으로 계산해 주기 때문에 훈련 세트와 테스트 세트로만 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x[:150], y[:150]\n",
    "x_test, y_test = x[150:], y[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 유닛 하나를 가진 간단한 완전 연결층 하나 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=1, input_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 summary() 메서드로 네트워크 구조를 출력해본 결과 유닛이 하나이므로 모델 파라미터 개수는 가중치와 절편 두 개.\n",
    "\n",
    "모델을 컴파일하고 훈련세트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/500\n",
      "105/105 [==============================] - 1s 6ms/sample - loss: 9.7413 - val_loss: 5.7770\n",
      "Epoch 2/500\n",
      "105/105 [==============================] - 0s 402us/sample - loss: 7.8129 - val_loss: 4.8672\n",
      "Epoch 3/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 6.4745 - val_loss: 4.1495\n",
      "Epoch 4/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 5.4416 - val_loss: 3.5533\n",
      "Epoch 5/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 4.5703 - val_loss: 3.0516\n",
      "Epoch 6/500\n",
      "105/105 [==============================] - 0s 352us/sample - loss: 3.8020 - val_loss: 2.5887\n",
      "Epoch 7/500\n",
      "105/105 [==============================] - 0s 356us/sample - loss: 3.1352 - val_loss: 2.2755\n",
      "Epoch 8/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 2.6772 - val_loss: 2.0226\n",
      "Epoch 9/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 2.3074 - val_loss: 1.8154\n",
      "Epoch 10/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 1.9977 - val_loss: 1.6437\n",
      "Epoch 11/500\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 1.7465 - val_loss: 1.4940\n",
      "Epoch 12/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 1.5465 - val_loss: 1.3794\n",
      "Epoch 13/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 1.3943 - val_loss: 1.2951\n",
      "Epoch 14/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 1.2748 - val_loss: 1.2228\n",
      "Epoch 15/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 1.1725 - val_loss: 1.1662\n",
      "Epoch 16/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 1.0949 - val_loss: 1.1063\n",
      "Epoch 17/500\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 1.0119 - val_loss: 1.0680\n",
      "Epoch 18/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.9652 - val_loss: 1.0359\n",
      "Epoch 19/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.9226 - val_loss: 1.0136\n",
      "Epoch 20/500\n",
      "105/105 [==============================] - 0s 280us/sample - loss: 0.8927 - val_loss: 0.9887\n",
      "Epoch 21/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.8609 - val_loss: 0.9745\n",
      "Epoch 22/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.8427 - val_loss: 0.9690\n",
      "Epoch 23/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.8287 - val_loss: 0.9519\n",
      "Epoch 24/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.8141 - val_loss: 0.9425\n",
      "Epoch 25/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.8030 - val_loss: 0.9339\n",
      "Epoch 26/500\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.7911 - val_loss: 0.9276\n",
      "Epoch 27/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7839 - val_loss: 0.9239\n",
      "Epoch 28/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7797 - val_loss: 0.9140\n",
      "Epoch 29/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7718 - val_loss: 0.9077\n",
      "Epoch 30/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7687 - val_loss: 0.9072\n",
      "Epoch 31/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7672 - val_loss: 0.9048\n",
      "Epoch 32/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7659 - val_loss: 0.9048\n",
      "Epoch 33/500\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.7629 - val_loss: 0.8998\n",
      "Epoch 34/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7601 - val_loss: 0.9016\n",
      "Epoch 35/500\n",
      "105/105 [==============================] - 0s 249us/sample - loss: 0.7585 - val_loss: 0.9017\n",
      "Epoch 36/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7611 - val_loss: 0.9006\n",
      "Epoch 37/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7575 - val_loss: 0.8975\n",
      "Epoch 38/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7566 - val_loss: 0.8995\n",
      "Epoch 39/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7577 - val_loss: 0.8987\n",
      "Epoch 40/500\n",
      "105/105 [==============================] - 0s 484us/sample - loss: 0.7561 - val_loss: 0.9005\n",
      "Epoch 41/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7577 - val_loss: 0.9027\n",
      "Epoch 42/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7560 - val_loss: 0.9009\n",
      "Epoch 43/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7558 - val_loss: 0.8990\n",
      "Epoch 44/500\n",
      "105/105 [==============================] - 0s 307us/sample - loss: 0.7557 - val_loss: 0.8994\n",
      "Epoch 45/500\n",
      "105/105 [==============================] - 0s 269us/sample - loss: 0.7563 - val_loss: 0.8969\n",
      "Epoch 46/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7567 - val_loss: 0.8927\n",
      "Epoch 47/500\n",
      "105/105 [==============================] - 0s 286us/sample - loss: 0.7571 - val_loss: 0.8953\n",
      "Epoch 48/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7567 - val_loss: 0.8973\n",
      "Epoch 49/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7575 - val_loss: 0.8961\n",
      "Epoch 50/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7566 - val_loss: 0.8977\n",
      "Epoch 51/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7577 - val_loss: 0.8993\n",
      "Epoch 52/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7566 - val_loss: 0.8992\n",
      "Epoch 53/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7570 - val_loss: 0.8988\n",
      "Epoch 54/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7573 - val_loss: 0.8976\n",
      "Epoch 55/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7570 - val_loss: 0.9055\n",
      "Epoch 56/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7563 - val_loss: 0.9036\n",
      "Epoch 57/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7563 - val_loss: 0.9032\n",
      "Epoch 58/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7567 - val_loss: 0.9005\n",
      "Epoch 59/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7569 - val_loss: 0.9016\n",
      "Epoch 60/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7569 - val_loss: 0.9000\n",
      "Epoch 61/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.9001\n",
      "Epoch 62/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7572 - val_loss: 0.9066\n",
      "Epoch 63/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7579 - val_loss: 0.9083\n",
      "Epoch 64/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7569 - val_loss: 0.9103\n",
      "Epoch 65/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7575 - val_loss: 0.9123\n",
      "Epoch 66/500\n",
      "105/105 [==============================] - 0s 375us/sample - loss: 0.7566 - val_loss: 0.9110\n",
      "Epoch 67/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9094\n",
      "Epoch 68/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7574 - val_loss: 0.9051\n",
      "Epoch 69/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7561 - val_loss: 0.9050\n",
      "Epoch 70/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7557 - val_loss: 0.9029\n",
      "Epoch 71/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7565 - val_loss: 0.9005\n",
      "Epoch 72/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7558 - val_loss: 0.9010\n",
      "Epoch 73/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7561 - val_loss: 0.9027\n",
      "Epoch 74/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7565 - val_loss: 0.8971\n",
      "Epoch 75/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7563 - val_loss: 0.8954\n",
      "Epoch 76/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7574 - val_loss: 0.8938\n",
      "Epoch 77/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7563 - val_loss: 0.8934\n",
      "Epoch 78/500\n",
      "105/105 [==============================] - 0s 152us/sample - loss: 0.7562 - val_loss: 0.8932\n",
      "Epoch 79/500\n",
      "105/105 [==============================] - 0s 168us/sample - loss: 0.7562 - val_loss: 0.8930\n",
      "Epoch 80/500\n",
      "105/105 [==============================] - 0s 154us/sample - loss: 0.7566 - val_loss: 0.8938\n",
      "Epoch 81/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7562 - val_loss: 0.8948\n",
      "Epoch 82/500\n",
      "105/105 [==============================] - 0s 172us/sample - loss: 0.7568 - val_loss: 0.8967\n",
      "Epoch 83/500\n",
      "105/105 [==============================] - 0s 168us/sample - loss: 0.7571 - val_loss: 0.9020\n",
      "Epoch 84/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7566 - val_loss: 0.9008\n",
      "Epoch 85/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7561 - val_loss: 0.8997\n",
      "Epoch 86/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7565 - val_loss: 0.9014\n",
      "Epoch 87/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7562 - val_loss: 0.9034\n",
      "Epoch 88/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7566 - val_loss: 0.9081\n",
      "Epoch 89/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7560 - val_loss: 0.9056\n",
      "Epoch 90/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7558 - val_loss: 0.9033\n",
      "Epoch 91/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7577 - val_loss: 0.8985\n",
      "Epoch 92/500\n",
      "105/105 [==============================] - 0s 354us/sample - loss: 0.7566 - val_loss: 0.8987\n",
      "Epoch 93/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7574 - val_loss: 0.8983\n",
      "Epoch 94/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7570 - val_loss: 0.8961\n",
      "Epoch 95/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7565 - val_loss: 0.9008\n",
      "Epoch 96/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7560 - val_loss: 0.8997\n",
      "Epoch 97/500\n",
      "105/105 [==============================] - 0s 194us/sample - loss: 0.7574 - val_loss: 0.9025\n",
      "Epoch 98/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9048\n",
      "Epoch 99/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7561 - val_loss: 0.9027\n",
      "Epoch 100/500\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.7568 - val_loss: 0.9023\n",
      "Epoch 101/500\n",
      "105/105 [==============================] - 0s 333us/sample - loss: 0.7566 - val_loss: 0.9004\n",
      "Epoch 102/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7565 - val_loss: 0.9006\n",
      "Epoch 103/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7572 - val_loss: 0.8988\n",
      "Epoch 104/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7587 - val_loss: 0.9000\n",
      "Epoch 105/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7572 - val_loss: 0.9039\n",
      "Epoch 106/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7590 - val_loss: 0.9021\n",
      "Epoch 107/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7576 - val_loss: 0.9005\n",
      "Epoch 108/500\n",
      "105/105 [==============================] - 0s 484us/sample - loss: 0.7572 - val_loss: 0.9021\n",
      "Epoch 109/500\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.7573 - val_loss: 0.9028\n",
      "Epoch 110/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7566 - val_loss: 0.9038\n",
      "Epoch 111/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7569 - val_loss: 0.9046\n",
      "Epoch 112/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7568 - val_loss: 0.9011\n",
      "Epoch 113/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7574 - val_loss: 0.8994\n",
      "Epoch 114/500\n",
      "105/105 [==============================] - 0s 343us/sample - loss: 0.7583 - val_loss: 0.8995\n",
      "Epoch 115/500\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.7573 - val_loss: 0.8988\n",
      "Epoch 116/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7561 - val_loss: 0.8965\n",
      "Epoch 117/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7564 - val_loss: 0.8970\n",
      "Epoch 118/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7564 - val_loss: 0.8973\n",
      "Epoch 119/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7561 - val_loss: 0.8994\n",
      "Epoch 120/500\n",
      "105/105 [==============================] - 0s 327us/sample - loss: 0.7558 - val_loss: 0.8995\n",
      "Epoch 121/500\n",
      "105/105 [==============================] - 0s 343us/sample - loss: 0.7567 - val_loss: 0.9034\n",
      "Epoch 122/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7559 - val_loss: 0.9021\n",
      "Epoch 123/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7561 - val_loss: 0.9006\n",
      "Epoch 124/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7565 - val_loss: 0.8972\n",
      "Epoch 125/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7569 - val_loss: 0.8966\n",
      "Epoch 126/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7579 - val_loss: 0.8955\n",
      "Epoch 127/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7560 - val_loss: 0.8963\n",
      "Epoch 128/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7587 - val_loss: 0.9006\n",
      "Epoch 129/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7587 - val_loss: 0.9025\n",
      "Epoch 130/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7583 - val_loss: 0.9022\n",
      "Epoch 131/500\n",
      "105/105 [==============================] - 0s 173us/sample - loss: 0.7570 - val_loss: 0.9024\n",
      "Epoch 132/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7565 - val_loss: 0.8993\n",
      "Epoch 133/500\n",
      "105/105 [==============================] - 0s 318us/sample - loss: 0.7578 - val_loss: 0.8994\n",
      "Epoch 134/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7564 - val_loss: 0.9010\n",
      "Epoch 135/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7567 - val_loss: 0.9023\n",
      "Epoch 136/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7567 - val_loss: 0.9022\n",
      "Epoch 137/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.861 - 0s 368us/sample - loss: 0.7566 - val_loss: 0.9034\n",
      "Epoch 138/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7566 - val_loss: 0.9024\n",
      "Epoch 139/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7577 - val_loss: 0.8974\n",
      "Epoch 140/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7566 - val_loss: 0.8994\n",
      "Epoch 141/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7565 - val_loss: 0.8976\n",
      "Epoch 142/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7561 - val_loss: 0.8993\n",
      "Epoch 143/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.8976\n",
      "Epoch 144/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7563 - val_loss: 0.9025\n",
      "Epoch 145/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7567 - val_loss: 0.9012\n",
      "Epoch 146/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7572 - val_loss: 0.9035\n",
      "Epoch 147/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9055\n",
      "Epoch 148/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7570 - val_loss: 0.9100\n",
      "Epoch 149/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7567 - val_loss: 0.9078\n",
      "Epoch 150/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7571 - val_loss: 0.9051\n",
      "Epoch 151/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7558 - val_loss: 0.9012\n",
      "Epoch 152/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7561 - val_loss: 0.9013\n",
      "Epoch 153/500\n",
      "105/105 [==============================] - 0s 347us/sample - loss: 0.7568 - val_loss: 0.9014\n",
      "Epoch 154/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7558 - val_loss: 0.9018\n",
      "Epoch 155/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7567 - val_loss: 0.8989\n",
      "Epoch 156/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.7564 - val_loss: 0.9034\n",
      "Epoch 157/500\n",
      "105/105 [==============================] - 0s 646us/sample - loss: 0.7565 - val_loss: 0.9050\n",
      "Epoch 158/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7560 - val_loss: 0.9021\n",
      "Epoch 159/500\n",
      "105/105 [==============================] - 0s 355us/sample - loss: 0.7569 - val_loss: 0.9037\n",
      "Epoch 160/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7565 - val_loss: 0.9026\n",
      "Epoch 161/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7569 - val_loss: 0.9038\n",
      "Epoch 162/500\n",
      "105/105 [==============================] - 0s 589us/sample - loss: 0.7572 - val_loss: 0.9047\n",
      "Epoch 163/500\n",
      "105/105 [==============================] - 0s 358us/sample - loss: 0.7562 - val_loss: 0.9064\n",
      "Epoch 164/500\n",
      "105/105 [==============================] - 0s 389us/sample - loss: 0.7572 - val_loss: 0.9043\n",
      "Epoch 165/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7577 - val_loss: 0.9005\n",
      "Epoch 166/500\n",
      "105/105 [==============================] - 0s 507us/sample - loss: 0.7563 - val_loss: 0.8995\n",
      "Epoch 167/500\n",
      "105/105 [==============================] - 0s 769us/sample - loss: 0.7567 - val_loss: 0.8994\n",
      "Epoch 168/500\n",
      "105/105 [==============================] - 0s 912us/sample - loss: 0.7572 - val_loss: 0.9016\n",
      "Epoch 169/500\n",
      "105/105 [==============================] - 0s 703us/sample - loss: 0.7569 - val_loss: 0.9029\n",
      "Epoch 170/500\n",
      "105/105 [==============================] - 0s 500us/sample - loss: 0.7559 - val_loss: 0.9020\n",
      "Epoch 171/500\n",
      "105/105 [==============================] - 0s 347us/sample - loss: 0.7577 - val_loss: 0.9010\n",
      "Epoch 172/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7565 - val_loss: 0.9061\n",
      "Epoch 173/500\n",
      "105/105 [==============================] - 0s 448us/sample - loss: 0.7557 - val_loss: 0.9035\n",
      "Epoch 174/500\n",
      "105/105 [==============================] - 0s 480us/sample - loss: 0.7557 - val_loss: 0.9027\n",
      "Epoch 175/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7558 - val_loss: 0.9021\n",
      "Epoch 176/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7559 - val_loss: 0.9039\n",
      "Epoch 177/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7566 - val_loss: 0.9083\n",
      "Epoch 178/500\n",
      "105/105 [==============================] - 0s 284us/sample - loss: 0.7570 - val_loss: 0.9085\n",
      "Epoch 179/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7566 - val_loss: 0.9094\n",
      "Epoch 180/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7575 - val_loss: 0.9195\n",
      "Epoch 181/500\n",
      "105/105 [==============================] - 0s 551us/sample - loss: 0.7578 - val_loss: 0.9227\n",
      "Epoch 182/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7580 - val_loss: 0.9198\n",
      "Epoch 183/500\n",
      "105/105 [==============================] - 0s 589us/sample - loss: 0.7569 - val_loss: 0.9191\n",
      "Epoch 184/500\n",
      "105/105 [==============================] - 0s 522us/sample - loss: 0.7574 - val_loss: 0.9152\n",
      "Epoch 185/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7562 - val_loss: 0.9113\n",
      "Epoch 186/500\n",
      "105/105 [==============================] - 0s 389us/sample - loss: 0.7572 - val_loss: 0.9159\n",
      "Epoch 187/500\n",
      "105/105 [==============================] - 0s 379us/sample - loss: 0.7563 - val_loss: 0.9112\n",
      "Epoch 188/500\n",
      "105/105 [==============================] - 0s 619us/sample - loss: 0.7561 - val_loss: 0.9084\n",
      "Epoch 189/500\n",
      "105/105 [==============================] - 0s 579us/sample - loss: 0.7572 - val_loss: 0.9095\n",
      "Epoch 190/500\n",
      "105/105 [==============================] - 0s 750us/sample - loss: 0.7557 - val_loss: 0.9082\n",
      "Epoch 191/500\n",
      "105/105 [==============================] - 0s 408us/sample - loss: 0.7563 - val_loss: 0.9075\n",
      "Epoch 192/500\n",
      "105/105 [==============================] - 0s 367us/sample - loss: 0.7564 - val_loss: 0.9112\n",
      "Epoch 193/500\n",
      "105/105 [==============================] - 0s 685us/sample - loss: 0.7561 - val_loss: 0.9088\n",
      "Epoch 194/500\n",
      "105/105 [==============================] - 0s 522us/sample - loss: 0.7563 - val_loss: 0.9055\n",
      "Epoch 195/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7559 - val_loss: 0.9054\n",
      "Epoch 196/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.7566 - val_loss: 0.9049\n",
      "Epoch 197/500\n",
      "105/105 [==============================] - 0s 299us/sample - loss: 0.7573 - val_loss: 0.9081\n",
      "Epoch 198/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7557 - val_loss: 0.9075\n",
      "Epoch 199/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7565 - val_loss: 0.9071\n",
      "Epoch 200/500\n",
      "105/105 [==============================] - 0s 483us/sample - loss: 0.7558 - val_loss: 0.9030\n",
      "Epoch 201/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7587 - val_loss: 0.9038\n",
      "Epoch 202/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7561 - val_loss: 0.9075\n",
      "Epoch 203/500\n",
      "105/105 [==============================] - 0s 362us/sample - loss: 0.7566 - val_loss: 0.9108\n",
      "Epoch 204/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7565 - val_loss: 0.9084\n",
      "Epoch 205/500\n",
      "105/105 [==============================] - 0s 293us/sample - loss: 0.7586 - val_loss: 0.9062\n",
      "Epoch 206/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7564 - val_loss: 0.9023\n",
      "Epoch 207/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7565 - val_loss: 0.9022\n",
      "Epoch 208/500\n",
      "105/105 [==============================] - 0s 282us/sample - loss: 0.7558 - val_loss: 0.9037\n",
      "Epoch 209/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7562 - val_loss: 0.9075\n",
      "Epoch 210/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9051\n",
      "Epoch 211/500\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.7569 - val_loss: 0.9055\n",
      "Epoch 212/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7564 - val_loss: 0.9045\n",
      "Epoch 213/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7566 - val_loss: 0.9015\n",
      "Epoch 214/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7570 - val_loss: 0.9010\n",
      "Epoch 215/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7567 - val_loss: 0.9012\n",
      "Epoch 216/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7560 - val_loss: 0.8999\n",
      "Epoch 217/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7568 - val_loss: 0.8973\n",
      "Epoch 218/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7574 - val_loss: 0.8956\n",
      "Epoch 219/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7567 - val_loss: 0.9019\n",
      "Epoch 220/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7562 - val_loss: 0.9031\n",
      "Epoch 221/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7575 - val_loss: 0.9053\n",
      "Epoch 222/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7557 - val_loss: 0.9033\n",
      "Epoch 223/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7562 - val_loss: 0.9024\n",
      "Epoch 224/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7568 - val_loss: 0.9073\n",
      "Epoch 225/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7575 - val_loss: 0.9087\n",
      "Epoch 226/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7577 - val_loss: 0.9132\n",
      "Epoch 227/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7567 - val_loss: 0.9108\n",
      "Epoch 228/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7562 - val_loss: 0.9133\n",
      "Epoch 229/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7566 - val_loss: 0.9091\n",
      "Epoch 230/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7565 - val_loss: 0.9129\n",
      "Epoch 231/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7565 - val_loss: 0.9114\n",
      "Epoch 232/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7568 - val_loss: 0.9115\n",
      "Epoch 233/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7571 - val_loss: 0.9084\n",
      "Epoch 234/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7564 - val_loss: 0.9029\n",
      "Epoch 235/500\n",
      "105/105 [==============================] - 0s 333us/sample - loss: 0.7564 - val_loss: 0.9018\n",
      "Epoch 236/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7561 - val_loss: 0.8996\n",
      "Epoch 237/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7571 - val_loss: 0.9066\n",
      "Epoch 238/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7566 - val_loss: 0.9058\n",
      "Epoch 239/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7565 - val_loss: 0.9060\n",
      "Epoch 240/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7570 - val_loss: 0.9046\n",
      "Epoch 241/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7566 - val_loss: 0.9030\n",
      "Epoch 242/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7568 - val_loss: 0.9030\n",
      "Epoch 243/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7570 - val_loss: 0.9046\n",
      "Epoch 244/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7563 - val_loss: 0.9072\n",
      "Epoch 245/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7564 - val_loss: 0.9116\n",
      "Epoch 246/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7567 - val_loss: 0.9061\n",
      "Epoch 247/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7571 - val_loss: 0.9057\n",
      "Epoch 248/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7555 - val_loss: 0.9037\n",
      "Epoch 249/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7566 - val_loss: 0.9075\n",
      "Epoch 250/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7559 - val_loss: 0.9066\n",
      "Epoch 251/500\n",
      "105/105 [==============================] - 0s 300us/sample - loss: 0.7572 - val_loss: 0.9093\n",
      "Epoch 252/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7563 - val_loss: 0.9033\n",
      "Epoch 253/500\n",
      "105/105 [==============================] - 0s 350us/sample - loss: 0.7565 - val_loss: 0.9043\n",
      "Epoch 254/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7560 - val_loss: 0.9070\n",
      "Epoch 255/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7562 - val_loss: 0.9049\n",
      "Epoch 256/500\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.7558 - val_loss: 0.9065\n",
      "Epoch 257/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7561 - val_loss: 0.9107\n",
      "Epoch 258/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7563 - val_loss: 0.9102\n",
      "Epoch 259/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7563 - val_loss: 0.9114\n",
      "Epoch 260/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7565 - val_loss: 0.9135\n",
      "Epoch 261/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7568 - val_loss: 0.9109\n",
      "Epoch 262/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7571 - val_loss: 0.9133\n",
      "Epoch 263/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7588 - val_loss: 0.9157\n",
      "Epoch 264/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7577 - val_loss: 0.9167\n",
      "Epoch 265/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7567 - val_loss: 0.9157\n",
      "Epoch 266/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7563 - val_loss: 0.9157\n",
      "Epoch 267/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7564 - val_loss: 0.9160\n",
      "Epoch 268/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7567 - val_loss: 0.9132\n",
      "Epoch 269/500\n",
      "105/105 [==============================] - 0s 379us/sample - loss: 0.7562 - val_loss: 0.9128\n",
      "Epoch 270/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7564 - val_loss: 0.9125\n",
      "Epoch 271/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7564 - val_loss: 0.9074\n",
      "Epoch 272/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7563 - val_loss: 0.9034\n",
      "Epoch 273/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7565 - val_loss: 0.9016\n",
      "Epoch 274/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7573 - val_loss: 0.9026\n",
      "Epoch 275/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7578 - val_loss: 0.9016\n",
      "Epoch 276/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7573 - val_loss: 0.9038\n",
      "Epoch 277/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7562 - val_loss: 0.9033\n",
      "Epoch 278/500\n",
      "105/105 [==============================] - 0s 291us/sample - loss: 0.7574 - val_loss: 0.9077\n",
      "Epoch 279/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7566 - val_loss: 0.9087\n",
      "Epoch 280/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7573 - val_loss: 0.9074\n",
      "Epoch 281/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7572 - val_loss: 0.9025\n",
      "Epoch 282/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7563 - val_loss: 0.9054\n",
      "Epoch 283/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7563 - val_loss: 0.9052\n",
      "Epoch 284/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7563 - val_loss: 0.9043\n",
      "Epoch 285/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7565 - val_loss: 0.9081\n",
      "Epoch 286/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7583 - val_loss: 0.9053\n",
      "Epoch 287/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7565 - val_loss: 0.9068\n",
      "Epoch 288/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7567 - val_loss: 0.9112\n",
      "Epoch 289/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7567 - val_loss: 0.9130\n",
      "Epoch 290/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7564 - val_loss: 0.9125\n",
      "Epoch 291/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7563 - val_loss: 0.9092\n",
      "Epoch 292/500\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.7569 - val_loss: 0.9061\n",
      "Epoch 293/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7557 - val_loss: 0.9054\n",
      "Epoch 294/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7559 - val_loss: 0.9030\n",
      "Epoch 295/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7560 - val_loss: 0.9015\n",
      "Epoch 296/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7570 - val_loss: 0.9035\n",
      "Epoch 297/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7560 - val_loss: 0.9052\n",
      "Epoch 298/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7570 - val_loss: 0.9041\n",
      "Epoch 299/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7568 - val_loss: 0.9086\n",
      "Epoch 300/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7569 - val_loss: 0.9142\n",
      "Epoch 301/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7570 - val_loss: 0.9122\n",
      "Epoch 302/500\n",
      "105/105 [==============================] - 0s 286us/sample - loss: 0.7560 - val_loss: 0.9117\n",
      "Epoch 303/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7563 - val_loss: 0.9087\n",
      "Epoch 304/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7564 - val_loss: 0.9089\n",
      "Epoch 305/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7559 - val_loss: 0.9067\n",
      "Epoch 306/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7567 - val_loss: 0.9117\n",
      "Epoch 307/500\n",
      "105/105 [==============================] - 0s 163us/sample - loss: 0.7561 - val_loss: 0.9090\n",
      "Epoch 308/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.9083\n",
      "Epoch 309/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7559 - val_loss: 0.9085\n",
      "Epoch 310/500\n",
      "105/105 [==============================] - 0s 324us/sample - loss: 0.7567 - val_loss: 0.9098\n",
      "Epoch 311/500\n",
      "105/105 [==============================] - 0s 408us/sample - loss: 0.7586 - val_loss: 0.9127\n",
      "Epoch 312/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7573 - val_loss: 0.9144\n",
      "Epoch 313/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7565 - val_loss: 0.9097\n",
      "Epoch 314/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7561 - val_loss: 0.9104\n",
      "Epoch 315/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7564 - val_loss: 0.9093\n",
      "Epoch 316/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7582 - val_loss: 0.9116\n",
      "Epoch 317/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7570 - val_loss: 0.9085\n",
      "Epoch 318/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7559 - val_loss: 0.9057\n",
      "Epoch 319/500\n",
      "105/105 [==============================] - 0s 341us/sample - loss: 0.7570 - val_loss: 0.9067\n",
      "Epoch 320/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7564 - val_loss: 0.9082\n",
      "Epoch 321/500\n",
      "105/105 [==============================] - 0s 303us/sample - loss: 0.7576 - val_loss: 0.9050\n",
      "Epoch 322/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7573 - val_loss: 0.9055\n",
      "Epoch 323/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7578 - val_loss: 0.9067\n",
      "Epoch 324/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7555 - val_loss: 0.9065\n",
      "Epoch 325/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7564 - val_loss: 0.9082\n",
      "Epoch 326/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7569 - val_loss: 0.9107\n",
      "Epoch 327/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7570 - val_loss: 0.9073\n",
      "Epoch 328/500\n",
      "105/105 [==============================] - 0s 299us/sample - loss: 0.7575 - val_loss: 0.9135\n",
      "Epoch 329/500\n",
      "105/105 [==============================] - 0s 314us/sample - loss: 0.7579 - val_loss: 0.9187\n",
      "Epoch 330/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7573 - val_loss: 0.9160\n",
      "Epoch 331/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7575 - val_loss: 0.9141\n",
      "Epoch 332/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7586 - val_loss: 0.9111\n",
      "Epoch 333/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7569 - val_loss: 0.9156\n",
      "Epoch 334/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7578 - val_loss: 0.9110\n",
      "Epoch 335/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7564 - val_loss: 0.9100\n",
      "Epoch 336/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7563 - val_loss: 0.9099\n",
      "Epoch 337/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7572 - val_loss: 0.9003\n",
      "Epoch 338/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7579 - val_loss: 0.9009\n",
      "Epoch 339/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7558 - val_loss: 0.9008\n",
      "Epoch 340/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7565 - val_loss: 0.8990\n",
      "Epoch 341/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7569 - val_loss: 0.8996\n",
      "Epoch 342/500\n",
      "105/105 [==============================] - 0s 188us/sample - loss: 0.7575 - val_loss: 0.8981\n",
      "Epoch 343/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7586 - val_loss: 0.8964\n",
      "Epoch 344/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7569 - val_loss: 0.8991\n",
      "Epoch 345/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.8984\n",
      "Epoch 346/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7562 - val_loss: 0.8964\n",
      "Epoch 347/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7565 - val_loss: 0.8970\n",
      "Epoch 348/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7582 - val_loss: 0.8962\n",
      "Epoch 349/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7564 - val_loss: 0.8981\n",
      "Epoch 350/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7583 - val_loss: 0.8945\n",
      "Epoch 351/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7591 - val_loss: 0.8989\n",
      "Epoch 352/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7572 - val_loss: 0.8965\n",
      "Epoch 353/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.8954\n",
      "Epoch 354/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7568 - val_loss: 0.8983\n",
      "Epoch 355/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7561 - val_loss: 0.9024\n",
      "Epoch 356/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7569 - val_loss: 0.9019\n",
      "Epoch 357/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7557 - val_loss: 0.9018\n",
      "Epoch 358/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7566 - val_loss: 0.9008\n",
      "Epoch 359/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7564 - val_loss: 0.9033\n",
      "Epoch 360/500\n",
      "105/105 [==============================] - 0s 331us/sample - loss: 0.7580 - val_loss: 0.9054\n",
      "Epoch 361/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7557 - val_loss: 0.9015\n",
      "Epoch 362/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7559 - val_loss: 0.9035\n",
      "Epoch 363/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7561 - val_loss: 0.9056\n",
      "Epoch 364/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7556 - val_loss: 0.9030\n",
      "Epoch 365/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7572 - val_loss: 0.9025\n",
      "Epoch 366/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7566 - val_loss: 0.9027\n",
      "Epoch 367/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7565 - val_loss: 0.9066\n",
      "Epoch 368/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7576 - val_loss: 0.9045\n",
      "Epoch 369/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7570 - val_loss: 0.8994\n",
      "Epoch 370/500\n",
      "105/105 [==============================] - 0s 551us/sample - loss: 0.7558 - val_loss: 0.9011\n",
      "Epoch 371/500\n",
      "105/105 [==============================] - 0s 389us/sample - loss: 0.7568 - val_loss: 0.8992\n",
      "Epoch 372/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7565 - val_loss: 0.9057\n",
      "Epoch 373/500\n",
      "105/105 [==============================] - 0s 513us/sample - loss: 0.7572 - val_loss: 0.9016\n",
      "Epoch 374/500\n",
      "105/105 [==============================] - 0s 367us/sample - loss: 0.7558 - val_loss: 0.9007\n",
      "Epoch 375/500\n",
      "105/105 [==============================] - 0s 703us/sample - loss: 0.7563 - val_loss: 0.8999\n",
      "Epoch 376/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.698 - 0s 343us/sample - loss: 0.7558 - val_loss: 0.9006\n",
      "Epoch 377/500\n",
      "105/105 [==============================] - 0s 589us/sample - loss: 0.7570 - val_loss: 0.8993\n",
      "Epoch 378/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7578 - val_loss: 0.8989\n",
      "Epoch 379/500\n",
      "105/105 [==============================] - 0s 350us/sample - loss: 0.7570 - val_loss: 0.8990\n",
      "Epoch 380/500\n",
      "105/105 [==============================] - 0s 522us/sample - loss: 0.7560 - val_loss: 0.8983\n",
      "Epoch 381/500\n",
      "105/105 [==============================] - 0s 357us/sample - loss: 0.7570 - val_loss: 0.9000\n",
      "Epoch 382/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7566 - val_loss: 0.8996\n",
      "Epoch 383/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7556 - val_loss: 0.9002\n",
      "Epoch 384/500\n",
      "105/105 [==============================] - 0s 458us/sample - loss: 0.7562 - val_loss: 0.9040\n",
      "Epoch 385/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7560 - val_loss: 0.9037\n",
      "Epoch 386/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7560 - val_loss: 0.9008\n",
      "Epoch 387/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7559 - val_loss: 0.9024\n",
      "Epoch 388/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7563 - val_loss: 0.9028\n",
      "Epoch 389/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7560 - val_loss: 0.9031\n",
      "Epoch 390/500\n",
      "105/105 [==============================] - 0s 408us/sample - loss: 0.7565 - val_loss: 0.9088\n",
      "Epoch 391/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7562 - val_loss: 0.9090\n",
      "Epoch 392/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7565 - val_loss: 0.9057\n",
      "Epoch 393/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.7566 - val_loss: 0.9057\n",
      "Epoch 394/500\n",
      "105/105 [==============================] - 0s 410us/sample - loss: 0.7566 - val_loss: 0.9095\n",
      "Epoch 395/500\n",
      "105/105 [==============================] - 0s 561us/sample - loss: 0.7575 - val_loss: 0.9092\n",
      "Epoch 396/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7564 - val_loss: 0.9062\n",
      "Epoch 397/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7575 - val_loss: 0.9070\n",
      "Epoch 398/500\n",
      "105/105 [==============================] - 0s 455us/sample - loss: 0.7569 - val_loss: 0.9068\n",
      "Epoch 399/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7590 - val_loss: 0.9077\n",
      "Epoch 400/500\n",
      "105/105 [==============================] - 0s 570us/sample - loss: 0.7579 - val_loss: 0.9107\n",
      "Epoch 401/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7563 - val_loss: 0.9057\n",
      "Epoch 402/500\n",
      "105/105 [==============================] - 0s 359us/sample - loss: 0.7569 - val_loss: 0.9047\n",
      "Epoch 403/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7565 - val_loss: 0.9102\n",
      "Epoch 404/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9139\n",
      "Epoch 405/500\n",
      "105/105 [==============================] - 0s 454us/sample - loss: 0.7563 - val_loss: 0.9151\n",
      "Epoch 406/500\n",
      "105/105 [==============================] - 0s 371us/sample - loss: 0.7572 - val_loss: 0.9209\n",
      "Epoch 407/500\n",
      "105/105 [==============================] - 0s 646us/sample - loss: 0.7572 - val_loss: 0.9244\n",
      "Epoch 408/500\n",
      "105/105 [==============================] - 0s 445us/sample - loss: 0.7579 - val_loss: 0.9230\n",
      "Epoch 409/500\n",
      "105/105 [==============================] - 0s 486us/sample - loss: 0.7580 - val_loss: 0.9220\n",
      "Epoch 410/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7579 - val_loss: 0.9160\n",
      "Epoch 411/500\n",
      "105/105 [==============================] - 0s 503us/sample - loss: 0.7570 - val_loss: 0.9129\n",
      "Epoch 412/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7572 - val_loss: 0.9099\n",
      "Epoch 413/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.613 - 0s 769us/sample - loss: 0.7560 - val_loss: 0.9063\n",
      "Epoch 414/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.703 - 0s 342us/sample - loss: 0.7562 - val_loss: 0.9043\n",
      "Epoch 415/500\n",
      "105/105 [==============================] - 0s 416us/sample - loss: 0.7565 - val_loss: 0.9032\n",
      "Epoch 416/500\n",
      "105/105 [==============================] - 0s 523us/sample - loss: 0.7565 - val_loss: 0.9050\n",
      "Epoch 417/500\n",
      "105/105 [==============================] - 0s 846us/sample - loss: 0.7592 - val_loss: 0.9073\n",
      "Epoch 418/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7563 - val_loss: 0.9038\n",
      "Epoch 419/500\n",
      "105/105 [==============================] - 0s 589us/sample - loss: 0.7558 - val_loss: 0.9023\n",
      "Epoch 420/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7560 - val_loss: 0.9055\n",
      "Epoch 421/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7581 - val_loss: 0.9055\n",
      "Epoch 422/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7566 - val_loss: 0.9064\n",
      "Epoch 423/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7583 - val_loss: 0.9104\n",
      "Epoch 424/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7562 - val_loss: 0.9103\n",
      "Epoch 425/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.9062\n",
      "Epoch 426/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7563 - val_loss: 0.9071\n",
      "Epoch 427/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7564 - val_loss: 0.9058\n",
      "Epoch 428/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7557 - val_loss: 0.9033\n",
      "Epoch 429/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7569 - val_loss: 0.8991\n",
      "Epoch 430/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7579 - val_loss: 0.8983\n",
      "Epoch 431/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7564 - val_loss: 0.8970\n",
      "Epoch 432/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7570 - val_loss: 0.8968\n",
      "Epoch 433/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7565 - val_loss: 0.9027\n",
      "Epoch 434/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7567 - val_loss: 0.9039\n",
      "Epoch 435/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7565 - val_loss: 0.9039\n",
      "Epoch 436/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7562 - val_loss: 0.9040\n",
      "Epoch 437/500\n",
      "105/105 [==============================] - 0s 181us/sample - loss: 0.7560 - val_loss: 0.9036\n",
      "Epoch 438/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7570 - val_loss: 0.9065\n",
      "Epoch 439/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7567 - val_loss: 0.9067\n",
      "Epoch 440/500\n",
      "105/105 [==============================] - 0s 277us/sample - loss: 0.7569 - val_loss: 0.9016\n",
      "Epoch 441/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7571 - val_loss: 0.8999\n",
      "Epoch 442/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7560 - val_loss: 0.9007\n",
      "Epoch 443/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7572 - val_loss: 0.9002\n",
      "Epoch 444/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7562 - val_loss: 0.8998\n",
      "Epoch 445/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7560 - val_loss: 0.9003\n",
      "Epoch 446/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7565 - val_loss: 0.8998\n",
      "Epoch 447/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.955 - 0s 199us/sample - loss: 0.7560 - val_loss: 0.8993\n",
      "Epoch 448/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7572 - val_loss: 0.8970\n",
      "Epoch 449/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7570 - val_loss: 0.9025\n",
      "Epoch 450/500\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.7562 - val_loss: 0.9010\n",
      "Epoch 451/500\n",
      "105/105 [==============================] - 0s 299us/sample - loss: 0.7577 - val_loss: 0.8955\n",
      "Epoch 452/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7563 - val_loss: 0.8962\n",
      "Epoch 453/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7566 - val_loss: 0.8991\n",
      "Epoch 454/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7568 - val_loss: 0.8964\n",
      "Epoch 455/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7563 - val_loss: 0.8963\n",
      "Epoch 456/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7578 - val_loss: 0.8943\n",
      "Epoch 457/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.8960\n",
      "Epoch 458/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7565 - val_loss: 0.8993\n",
      "Epoch 459/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7567 - val_loss: 0.8983\n",
      "Epoch 460/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7568 - val_loss: 0.8978\n",
      "Epoch 461/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7561 - val_loss: 0.8978\n",
      "Epoch 462/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7562 - val_loss: 0.9006\n",
      "Epoch 463/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7558 - val_loss: 0.8996\n",
      "Epoch 464/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7565 - val_loss: 0.9002\n",
      "Epoch 465/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7563 - val_loss: 0.9048\n",
      "Epoch 466/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7571 - val_loss: 0.9050\n",
      "Epoch 467/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7565 - val_loss: 0.9044\n",
      "Epoch 468/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7568 - val_loss: 0.9004\n",
      "Epoch 469/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7569 - val_loss: 0.8995\n",
      "Epoch 470/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7575 - val_loss: 0.8991\n",
      "Epoch 471/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7561 - val_loss: 0.8979\n",
      "Epoch 472/500\n",
      "105/105 [==============================] - 0s 318us/sample - loss: 0.7565 - val_loss: 0.8991\n",
      "Epoch 473/500\n",
      "105/105 [==============================] - 0s 308us/sample - loss: 0.7583 - val_loss: 0.8990\n",
      "Epoch 474/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7569 - val_loss: 0.8982\n",
      "Epoch 475/500\n",
      "105/105 [==============================] - 0s 303us/sample - loss: 0.7565 - val_loss: 0.9003\n",
      "Epoch 476/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7560 - val_loss: 0.9008\n",
      "Epoch 477/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7559 - val_loss: 0.9006\n",
      "Epoch 478/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7568 - val_loss: 0.9006\n",
      "Epoch 479/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7566 - val_loss: 0.8998\n",
      "Epoch 480/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7558 - val_loss: 0.8998\n",
      "Epoch 481/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7573 - val_loss: 0.8984\n",
      "Epoch 482/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7560 - val_loss: 0.9014\n",
      "Epoch 483/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7567 - val_loss: 0.9063\n",
      "Epoch 484/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7576 - val_loss: 0.9016\n",
      "Epoch 485/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.9031\n",
      "Epoch 486/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7571 - val_loss: 0.9052\n",
      "Epoch 487/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7578 - val_loss: 0.9106\n",
      "Epoch 488/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7567 - val_loss: 0.9077\n",
      "Epoch 489/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7564 - val_loss: 0.9042\n",
      "Epoch 490/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7563 - val_loss: 0.9019\n",
      "Epoch 491/500\n",
      "105/105 [==============================] - 0s 286us/sample - loss: 0.7561 - val_loss: 0.9058\n",
      "Epoch 492/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7567 - val_loss: 0.9039\n",
      "Epoch 493/500\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.7564 - val_loss: 0.9034\n",
      "Epoch 494/500\n",
      "105/105 [==============================] - 0s 310us/sample - loss: 0.7566 - val_loss: 0.8997\n",
      "Epoch 495/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7562 - val_loss: 0.9017\n",
      "Epoch 496/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7564 - val_loss: 0.8998\n",
      "Epoch 497/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.8994\n",
      "Epoch 498/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7569 - val_loss: 0.8967\n",
      "Epoch 499/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7563 - val_loss: 0.8965\n",
      "Epoch 500/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7565 - val_loss: 0.8967\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='mse')\n",
    "history = model.fit(x_train, y_train, epochs=500,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile() 메서드의 loss 매개변수에 회귀 문제를 위한 평균 제곱 오차 손실 함수인 mse를 선택.\n",
    "\n",
    "회귀 문제에도 이외에도 평균 절댓값 오차 손실 함수 mae를 지정하는 것도 가능.\n",
    "\n",
    "이진 분류 문제에는 이진 크로스 엔트로피 binary_crossentropy 선택.\n",
    "\n",
    "optimizer 매개변수에는 경사 하강법 옵티마이저 지정.\n",
    "\n",
    "fit() 메서드에서 검증 세트 크기를 30%로 설정.\n",
    "\n",
    "훈련 과정을 출력해주는 verbose 매개변수의 기본값은 1로 진행바와 함께 검증 점수 출력.\n",
    "\n",
    "fit() 메서드에서 반환받은 History 객체의 history 딕셔너리에는 에포크마다 계산한 손실 함수 값이 저장되어 있어 그래프로 그리는 것이 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1, 500+1)\n",
    "plt.plot(epochs, history.history['loss'], label='Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 세트에 대한 손실 값이 부드럽게 감소해 최적 값에 수렴하는 것으로 보임.\n",
    "\n",
    "### 14.6.2 함수형 API\n",
    "\n",
    "Sequential 모델은 층을 순서대로 쌓은 네트워크 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "input = tf.keras.Input(shape=(1,))\n",
    "output = tf.keras.layers.Dense(1)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "튜플로 입력 크기를 지정해 Input 클래스의 객체를 만들고, Dense 클래스를 함수처럼 호출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(1)\n",
    "output = dense(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 다음과 같이 파이썬의 특수한 __call__() 메서드를 사용하는 것도 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(1)\n",
    "output = dense.__call__(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense층의 객체를 네트워크의 다른 부분에 재사용되지 않는다면 객체 생성과 동시에 호출하는 것이 편리.\n",
    "\n",
    "Model 클래스에 입력과 출력을 전달해 신경망 모델 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 동일하게 두 개의 모델 파라미터를 가진 Dense 층 하나로 이루어져 있다. \n",
    "\n",
    "Sequential 모델에서는 없었던 InputLayer는 입력 데이터를 위한 층으로 학습되는 파라미터를 가지고 있지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/500\n",
      "105/105 [==============================] - 1s 7ms/sample - loss: 1.8670 - val_loss: 1.3914\n",
      "Epoch 2/500\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 1.6827 - val_loss: 1.2986\n",
      "Epoch 3/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 1.5348 - val_loss: 1.2186\n",
      "Epoch 4/500\n",
      "105/105 [==============================] - 0s 440us/sample - loss: 1.4041 - val_loss: 1.1608\n",
      "Epoch 5/500\n",
      "105/105 [==============================] - 0s 371us/sample - loss: 1.3093 - val_loss: 1.1051\n",
      "Epoch 6/500\n",
      "105/105 [==============================] - 0s 656us/sample - loss: 1.2191 - val_loss: 1.0608\n",
      "Epoch 7/500\n",
      "105/105 [==============================] - 0s 664us/sample - loss: 1.1466 - val_loss: 1.0263\n",
      "Epoch 8/500\n",
      "105/105 [==============================] - 0s 911us/sample - loss: 1.0874 - val_loss: 0.9955\n",
      "Epoch 9/500\n",
      "105/105 [==============================] - 0s 636us/sample - loss: 1.0349 - val_loss: 0.9692\n",
      "Epoch 10/500\n",
      "105/105 [==============================] - 0s 484us/sample - loss: 0.9913 - val_loss: 0.9492\n",
      "Epoch 11/500\n",
      "105/105 [==============================] - 0s 401us/sample - loss: 0.9555 - val_loss: 0.9300\n",
      "Epoch 12/500\n",
      "105/105 [==============================] - 0s 407us/sample - loss: 0.9216 - val_loss: 0.9197\n",
      "Epoch 13/500\n",
      "105/105 [==============================] - 0s 503us/sample - loss: 0.9026 - val_loss: 0.9075\n",
      "Epoch 14/500\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.8792 - val_loss: 0.8991\n",
      "Epoch 15/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.8607 - val_loss: 0.8884\n",
      "Epoch 16/500\n",
      "105/105 [==============================] - 0s 636us/sample - loss: 0.8396 - val_loss: 0.8848\n",
      "Epoch 17/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.8287 - val_loss: 0.8796\n",
      "Epoch 18/500\n",
      "105/105 [==============================] - 0s 480us/sample - loss: 0.8195 - val_loss: 0.8772\n",
      "Epoch 19/500\n",
      "105/105 [==============================] - 0s 641us/sample - loss: 0.8094 - val_loss: 0.8764\n",
      "Epoch 20/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.8037 - val_loss: 0.8732\n",
      "Epoch 21/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7949 - val_loss: 0.8735\n",
      "Epoch 22/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7888 - val_loss: 0.8727\n",
      "Epoch 23/500\n",
      "105/105 [==============================] - 0s 551us/sample - loss: 0.7842 - val_loss: 0.8724\n",
      "Epoch 24/500\n",
      "105/105 [==============================] - 0s 371us/sample - loss: 0.7804 - val_loss: 0.8742\n",
      "Epoch 25/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.7784 - val_loss: 0.8766\n",
      "Epoch 26/500\n",
      "105/105 [==============================] - 0s 912us/sample - loss: 0.7753 - val_loss: 0.8818\n",
      "Epoch 27/500\n",
      "105/105 [==============================] - 0s 452us/sample - loss: 0.7713 - val_loss: 0.8837\n",
      "Epoch 28/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7692 - val_loss: 0.8851\n",
      "Epoch 29/500\n",
      "105/105 [==============================] - 0s 348us/sample - loss: 0.7667 - val_loss: 0.8855\n",
      "Epoch 30/500\n",
      "105/105 [==============================] - 0s 446us/sample - loss: 0.7654 - val_loss: 0.8845\n",
      "Epoch 31/500\n",
      "105/105 [==============================] - 0s 503us/sample - loss: 0.7672 - val_loss: 0.8889\n",
      "Epoch 32/500\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 0.7648 - val_loss: 0.8881\n",
      "Epoch 33/500\n",
      "105/105 [==============================] - 0s 827us/sample - loss: 0.7647 - val_loss: 0.8879\n",
      "Epoch 34/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7652 - val_loss: 0.8854\n",
      "Epoch 35/500\n",
      "105/105 [==============================] - 0s 410us/sample - loss: 0.7619 - val_loss: 0.8868\n",
      "Epoch 36/500\n",
      "105/105 [==============================] - 0s 503us/sample - loss: 0.7607 - val_loss: 0.8870\n",
      "Epoch 37/500\n",
      "105/105 [==============================] - 0s 452us/sample - loss: 0.7599 - val_loss: 0.8878\n",
      "Epoch 38/500\n",
      "105/105 [==============================] - 0s 409us/sample - loss: 0.7598 - val_loss: 0.8887\n",
      "Epoch 39/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7589 - val_loss: 0.8869\n",
      "Epoch 40/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7603 - val_loss: 0.8865\n",
      "Epoch 41/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7584 - val_loss: 0.8863\n",
      "Epoch 42/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7585 - val_loss: 0.8870\n",
      "Epoch 43/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7576 - val_loss: 0.8874\n",
      "Epoch 44/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7581 - val_loss: 0.8866\n",
      "Epoch 45/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7580 - val_loss: 0.8886\n",
      "Epoch 46/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7608 - val_loss: 0.8873\n",
      "Epoch 47/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7589 - val_loss: 0.8926\n",
      "Epoch 48/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7569 - val_loss: 0.8932\n",
      "Epoch 49/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7565 - val_loss: 0.8916\n",
      "Epoch 50/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7583 - val_loss: 0.8914\n",
      "Epoch 51/500\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.7573 - val_loss: 0.8936\n",
      "Epoch 52/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7576 - val_loss: 0.8950\n",
      "Epoch 53/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7562 - val_loss: 0.8965\n",
      "Epoch 54/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7571 - val_loss: 0.8974\n",
      "Epoch 55/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7568 - val_loss: 0.9001\n",
      "Epoch 56/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7560 - val_loss: 0.9001\n",
      "Epoch 57/500\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.7566 - val_loss: 0.9018\n",
      "Epoch 58/500\n",
      "105/105 [==============================] - 0s 195us/sample - loss: 0.7571 - val_loss: 0.9034\n",
      "Epoch 59/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7558 - val_loss: 0.9073\n",
      "Epoch 60/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.9036\n",
      "Epoch 61/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7560 - val_loss: 0.9008\n",
      "Epoch 62/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7560 - val_loss: 0.9028\n",
      "Epoch 63/500\n",
      "105/105 [==============================] - 0s 319us/sample - loss: 0.7558 - val_loss: 0.9020\n",
      "Epoch 64/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7571 - val_loss: 0.8992\n",
      "Epoch 65/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7560 - val_loss: 0.8998\n",
      "Epoch 66/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7572 - val_loss: 0.8998\n",
      "Epoch 67/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7569 - val_loss: 0.8999\n",
      "Epoch 68/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7572 - val_loss: 0.8992\n",
      "Epoch 69/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7576 - val_loss: 0.8965\n",
      "Epoch 70/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7587 - val_loss: 0.8958\n",
      "Epoch 71/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7577 - val_loss: 0.8969\n",
      "Epoch 72/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7577 - val_loss: 0.8966\n",
      "Epoch 73/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7569 - val_loss: 0.8987\n",
      "Epoch 74/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7567 - val_loss: 0.8979\n",
      "Epoch 75/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7583 - val_loss: 0.8974\n",
      "Epoch 76/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7573 - val_loss: 0.9023\n",
      "Epoch 77/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7562 - val_loss: 0.9033\n",
      "Epoch 78/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7569 - val_loss: 0.9050\n",
      "Epoch 79/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7566 - val_loss: 0.9032\n",
      "Epoch 80/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7569 - val_loss: 0.8985\n",
      "Epoch 81/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7587 - val_loss: 0.8985\n",
      "Epoch 82/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7586 - val_loss: 0.8990\n",
      "Epoch 83/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7570 - val_loss: 0.8992\n",
      "Epoch 84/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7573 - val_loss: 0.8988\n",
      "Epoch 85/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9019\n",
      "Epoch 86/500\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.7569 - val_loss: 0.9022\n",
      "Epoch 87/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7568 - val_loss: 0.9042\n",
      "Epoch 88/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7560 - val_loss: 0.9058\n",
      "Epoch 89/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7562 - val_loss: 0.9009\n",
      "Epoch 90/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7566 - val_loss: 0.9005\n",
      "Epoch 91/500\n",
      "105/105 [==============================] - 0s 244us/sample - loss: 0.7559 - val_loss: 0.9017\n",
      "Epoch 92/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7558 - val_loss: 0.9027\n",
      "Epoch 93/500\n",
      "105/105 [==============================] - 0s 465us/sample - loss: 0.7566 - val_loss: 0.9061\n",
      "Epoch 94/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7558 - val_loss: 0.9069\n",
      "Epoch 95/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7562 - val_loss: 0.9074\n",
      "Epoch 96/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7568 - val_loss: 0.9094\n",
      "Epoch 97/500\n",
      "105/105 [==============================] - 0s 196us/sample - loss: 0.7566 - val_loss: 0.9113\n",
      "Epoch 98/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7565 - val_loss: 0.9103\n",
      "Epoch 99/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7560 - val_loss: 0.9107\n",
      "Epoch 100/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7571 - val_loss: 0.9087\n",
      "Epoch 101/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7558 - val_loss: 0.9095\n",
      "Epoch 102/500\n",
      "105/105 [==============================] - 0s 400us/sample - loss: 0.7561 - val_loss: 0.9092\n",
      "Epoch 103/500\n",
      "105/105 [==============================] - 0s 315us/sample - loss: 0.7573 - val_loss: 0.9077\n",
      "Epoch 104/500\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.7564 - val_loss: 0.9083\n",
      "Epoch 105/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7561 - val_loss: 0.9094\n",
      "Epoch 106/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7567 - val_loss: 0.9085\n",
      "Epoch 107/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7558 - val_loss: 0.9066\n",
      "Epoch 108/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7562 - val_loss: 0.9065\n",
      "Epoch 109/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7567 - val_loss: 0.9075\n",
      "Epoch 110/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7566 - val_loss: 0.9026\n",
      "Epoch 111/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7569 - val_loss: 0.8991\n",
      "Epoch 112/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7578 - val_loss: 0.8982\n",
      "Epoch 113/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7568 - val_loss: 0.8983\n",
      "Epoch 114/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7565 - val_loss: 0.8958\n",
      "Epoch 115/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7565 - val_loss: 0.8969\n",
      "Epoch 116/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7566 - val_loss: 0.8996\n",
      "Epoch 117/500\n",
      "105/105 [==============================] - 0s 347us/sample - loss: 0.7565 - val_loss: 0.8999\n",
      "Epoch 118/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7563 - val_loss: 0.8980\n",
      "Epoch 119/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7576 - val_loss: 0.8973\n",
      "Epoch 120/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7562 - val_loss: 0.8983\n",
      "Epoch 121/500\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.7577 - val_loss: 0.8984\n",
      "Epoch 122/500\n",
      "105/105 [==============================] - 0s 203us/sample - loss: 0.7567 - val_loss: 0.9023\n",
      "Epoch 123/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7564 - val_loss: 0.9030\n",
      "Epoch 124/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7560 - val_loss: 0.9052\n",
      "Epoch 125/500\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.7571 - val_loss: 0.9039\n",
      "Epoch 126/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7562 - val_loss: 0.9048\n",
      "Epoch 127/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7576 - val_loss: 0.9015\n",
      "Epoch 128/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7567 - val_loss: 0.9013\n",
      "Epoch 129/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7573 - val_loss: 0.9012\n",
      "Epoch 130/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7572 - val_loss: 0.9018\n",
      "Epoch 131/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7561 - val_loss: 0.9018\n",
      "Epoch 132/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7564 - val_loss: 0.9006\n",
      "Epoch 133/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7561 - val_loss: 0.8964\n",
      "Epoch 134/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7562 - val_loss: 0.8983\n",
      "Epoch 135/500\n",
      "105/105 [==============================] - 0s 297us/sample - loss: 0.7564 - val_loss: 0.8967\n",
      "Epoch 136/500\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.7569 - val_loss: 0.9018\n",
      "Epoch 137/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7562 - val_loss: 0.9043\n",
      "Epoch 138/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7564 - val_loss: 0.9086\n",
      "Epoch 139/500\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.7564 - val_loss: 0.9049\n",
      "Epoch 140/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7568 - val_loss: 0.9065\n",
      "Epoch 141/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7561 - val_loss: 0.9099\n",
      "Epoch 142/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7574 - val_loss: 0.9067\n",
      "Epoch 143/500\n",
      "105/105 [==============================] - 0s 179us/sample - loss: 0.7573 - val_loss: 0.9076\n",
      "Epoch 144/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7566 - val_loss: 0.9064\n",
      "Epoch 145/500\n",
      "105/105 [==============================] - 0s 193us/sample - loss: 0.7566 - val_loss: 0.9079\n",
      "Epoch 146/500\n",
      "105/105 [==============================] - 0s 370us/sample - loss: 0.7562 - val_loss: 0.9081\n",
      "Epoch 147/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7565 - val_loss: 0.9108\n",
      "Epoch 148/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7561 - val_loss: 0.9104\n",
      "Epoch 149/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7564 - val_loss: 0.9083\n",
      "Epoch 150/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7567 - val_loss: 0.9071\n",
      "Epoch 151/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7571 - val_loss: 0.9065\n",
      "Epoch 152/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7562 - val_loss: 0.9065\n",
      "Epoch 153/500\n",
      "105/105 [==============================] - 0s 232us/sample - loss: 0.7591 - val_loss: 0.9024\n",
      "Epoch 154/500\n",
      "105/105 [==============================] - 0s 186us/sample - loss: 0.7563 - val_loss: 0.8981\n",
      "Epoch 155/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7567 - val_loss: 0.8983\n",
      "Epoch 156/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7558 - val_loss: 0.8989\n",
      "Epoch 157/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7575 - val_loss: 0.8994\n",
      "Epoch 158/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7563 - val_loss: 0.8968\n",
      "Epoch 159/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7570 - val_loss: 0.8999\n",
      "Epoch 160/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7589 - val_loss: 0.8987\n",
      "Epoch 161/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7574 - val_loss: 0.8953\n",
      "Epoch 162/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7581 - val_loss: 0.8967\n",
      "Epoch 163/500\n",
      "105/105 [==============================] - 0s 401us/sample - loss: 0.7568 - val_loss: 0.8972\n",
      "Epoch 164/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7572 - val_loss: 0.8995\n",
      "Epoch 165/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7565 - val_loss: 0.9015\n",
      "Epoch 166/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7564 - val_loss: 0.8982\n",
      "Epoch 167/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7558 - val_loss: 0.9005\n",
      "Epoch 168/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7561 - val_loss: 0.9001\n",
      "Epoch 169/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7582 - val_loss: 0.9040\n",
      "Epoch 170/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7563 - val_loss: 0.9049\n",
      "Epoch 171/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7570 - val_loss: 0.9062\n",
      "Epoch 172/500\n",
      "105/105 [==============================] - 0s 360us/sample - loss: 0.7562 - val_loss: 0.9032\n",
      "Epoch 173/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7567 - val_loss: 0.9032\n",
      "Epoch 174/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7570 - val_loss: 0.9051\n",
      "Epoch 175/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7566 - val_loss: 0.9067\n",
      "Epoch 176/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7561 - val_loss: 0.9071\n",
      "Epoch 177/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7581 - val_loss: 0.9068\n",
      "Epoch 178/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7572 - val_loss: 0.9048\n",
      "Epoch 179/500\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.7561 - val_loss: 0.9041\n",
      "Epoch 180/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.835 - 0s 589us/sample - loss: 0.7567 - val_loss: 0.9057\n",
      "Epoch 181/500\n",
      "105/105 [==============================] - 0s 192us/sample - loss: 0.7573 - val_loss: 0.9038\n",
      "Epoch 182/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7574 - val_loss: 0.9063\n",
      "Epoch 183/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7562 - val_loss: 0.9060\n",
      "Epoch 184/500\n",
      "105/105 [==============================] - 0s 198us/sample - loss: 0.7567 - val_loss: 0.9074\n",
      "Epoch 185/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7563 - val_loss: 0.9052\n",
      "Epoch 186/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7563 - val_loss: 0.9018\n",
      "Epoch 187/500\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.7559 - val_loss: 0.9035\n",
      "Epoch 188/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7559 - val_loss: 0.9008\n",
      "Epoch 189/500\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7568 - val_loss: 0.9016\n",
      "Epoch 190/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7559 - val_loss: 0.9004\n",
      "Epoch 191/500\n",
      "105/105 [==============================] - 0s 353us/sample - loss: 0.7558 - val_loss: 0.9016\n",
      "Epoch 192/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7560 - val_loss: 0.9024\n",
      "Epoch 193/500\n",
      "105/105 [==============================] - 0s 371us/sample - loss: 0.7559 - val_loss: 0.8999\n",
      "Epoch 194/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7563 - val_loss: 0.8955\n",
      "Epoch 195/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7565 - val_loss: 0.9008\n",
      "Epoch 196/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7566 - val_loss: 0.8957\n",
      "Epoch 197/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7564 - val_loss: 0.8963\n",
      "Epoch 198/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7576 - val_loss: 0.8950\n",
      "Epoch 199/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7573 - val_loss: 0.8948\n",
      "Epoch 200/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7564 - val_loss: 0.8949\n",
      "Epoch 201/500\n",
      "105/105 [==============================] - 0s 147us/sample - loss: 0.7571 - val_loss: 0.8953\n",
      "Epoch 202/500\n",
      "105/105 [==============================] - 0s 156us/sample - loss: 0.7563 - val_loss: 0.8966\n",
      "Epoch 203/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7566 - val_loss: 0.8982\n",
      "Epoch 204/500\n",
      "105/105 [==============================] - 0s 447us/sample - loss: 0.7566 - val_loss: 0.8969\n",
      "Epoch 205/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7562 - val_loss: 0.9010\n",
      "Epoch 206/500\n",
      "105/105 [==============================] - 0s 608us/sample - loss: 0.7584 - val_loss: 0.9018\n",
      "Epoch 207/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7578 - val_loss: 0.8994\n",
      "Epoch 208/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7578 - val_loss: 0.9001\n",
      "Epoch 209/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7582 - val_loss: 0.9018\n",
      "Epoch 210/500\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.7566 - val_loss: 0.9021\n",
      "Epoch 211/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7559 - val_loss: 0.9009\n",
      "Epoch 212/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7566 - val_loss: 0.9034\n",
      "Epoch 213/500\n",
      "105/105 [==============================] - 0s 500us/sample - loss: 0.7565 - val_loss: 0.9035\n",
      "Epoch 214/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.7564 - val_loss: 0.8995\n",
      "Epoch 215/500\n",
      "105/105 [==============================] - 0s 805us/sample - loss: 0.7560 - val_loss: 0.8998\n",
      "Epoch 216/500\n",
      "105/105 [==============================] - 0s 545us/sample - loss: 0.7562 - val_loss: 0.9019\n",
      "Epoch 217/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7583 - val_loss: 0.9000\n",
      "Epoch 218/500\n",
      "105/105 [==============================] - 0s 365us/sample - loss: 0.7586 - val_loss: 0.9002\n",
      "Epoch 219/500\n",
      "105/105 [==============================] - 0s 348us/sample - loss: 0.7568 - val_loss: 0.9009\n",
      "Epoch 220/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7562 - val_loss: 0.9014\n",
      "Epoch 221/500\n",
      "105/105 [==============================] - 0s 722us/sample - loss: 0.7569 - val_loss: 0.9013\n",
      "Epoch 222/500\n",
      "105/105 [==============================] - 0s 400us/sample - loss: 0.7565 - val_loss: 0.9030\n",
      "Epoch 223/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7570 - val_loss: 0.9044\n",
      "Epoch 224/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7577 - val_loss: 0.9043\n",
      "Epoch 225/500\n",
      "105/105 [==============================] - 0s 378us/sample - loss: 0.7567 - val_loss: 0.9007\n",
      "Epoch 226/500\n",
      "105/105 [==============================] - 0s 389us/sample - loss: 0.7565 - val_loss: 0.8997\n",
      "Epoch 227/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7561 - val_loss: 0.8978\n",
      "Epoch 228/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7572 - val_loss: 0.8986\n",
      "Epoch 229/500\n",
      "105/105 [==============================] - 0s 408us/sample - loss: 0.7564 - val_loss: 0.8993\n",
      "Epoch 230/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7568 - val_loss: 0.9012\n",
      "Epoch 231/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7563 - val_loss: 0.9011\n",
      "Epoch 232/500\n",
      "105/105 [==============================] - 0s 551us/sample - loss: 0.7563 - val_loss: 0.9023\n",
      "Epoch 233/500\n",
      "105/105 [==============================] - 0s 554us/sample - loss: 0.7558 - val_loss: 0.9023\n",
      "Epoch 234/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7570 - val_loss: 0.9007\n",
      "Epoch 235/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7560 - val_loss: 0.9010\n",
      "Epoch 236/500\n",
      "105/105 [==============================] - 0s 312us/sample - loss: 0.7563 - val_loss: 0.9023\n",
      "Epoch 237/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.7562 - val_loss: 0.9073\n",
      "Epoch 238/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7578 - val_loss: 0.9181\n",
      "Epoch 239/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7572 - val_loss: 0.9186\n",
      "Epoch 240/500\n",
      "105/105 [==============================] - 0s 401us/sample - loss: 0.7582 - val_loss: 0.9122\n",
      "Epoch 241/500\n",
      "105/105 [==============================] - 0s 484us/sample - loss: 0.7570 - val_loss: 0.9066\n",
      "Epoch 242/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7572 - val_loss: 0.9009\n",
      "Epoch 243/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.7573 - val_loss: 0.8988\n",
      "Epoch 244/500\n",
      "105/105 [==============================] - 0s 522us/sample - loss: 0.7568 - val_loss: 0.9038\n",
      "Epoch 245/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7561 - val_loss: 0.9060\n",
      "Epoch 246/500\n",
      "105/105 [==============================] - 0s 424us/sample - loss: 0.7559 - val_loss: 0.9080\n",
      "Epoch 247/500\n",
      "105/105 [==============================] - 0s 494us/sample - loss: 0.7567 - val_loss: 0.9060\n",
      "Epoch 248/500\n",
      "105/105 [==============================] - 0s 560us/sample - loss: 0.7557 - val_loss: 0.9065\n",
      "Epoch 249/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7562 - val_loss: 0.9120\n",
      "Epoch 250/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7566 - val_loss: 0.9145\n",
      "Epoch 251/500\n",
      "105/105 [==============================] - 0s 580us/sample - loss: 0.7566 - val_loss: 0.9139\n",
      "Epoch 252/500\n",
      "105/105 [==============================] - 0s 400us/sample - loss: 0.7582 - val_loss: 0.9164\n",
      "Epoch 253/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.7567 - val_loss: 0.9139\n",
      "Epoch 254/500\n",
      "105/105 [==============================] - 0s 589us/sample - loss: 0.7563 - val_loss: 0.9146\n",
      "Epoch 255/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7575 - val_loss: 0.9145\n",
      "Epoch 256/500\n",
      "105/105 [==============================] - 0s 268us/sample - loss: 0.7571 - val_loss: 0.9152\n",
      "Epoch 257/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9134\n",
      "Epoch 258/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7558 - val_loss: 0.9107\n",
      "Epoch 259/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9076\n",
      "Epoch 260/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7559 - val_loss: 0.9028\n",
      "Epoch 261/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7586 - val_loss: 0.9075\n",
      "Epoch 262/500\n",
      "105/105 [==============================] - 0s 291us/sample - loss: 0.7570 - val_loss: 0.9082\n",
      "Epoch 263/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7562 - val_loss: 0.9088\n",
      "Epoch 264/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7566 - val_loss: 0.9162\n",
      "Epoch 265/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7565 - val_loss: 0.9176\n",
      "Epoch 266/500\n",
      "105/105 [==============================] - 0s 293us/sample - loss: 0.7572 - val_loss: 0.9183\n",
      "Epoch 267/500\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.7575 - val_loss: 0.9221\n",
      "Epoch 268/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7590 - val_loss: 0.9188\n",
      "Epoch 269/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7574 - val_loss: 0.9155\n",
      "Epoch 270/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7571 - val_loss: 0.9125\n",
      "Epoch 271/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7561 - val_loss: 0.9128\n",
      "Epoch 272/500\n",
      "105/105 [==============================] - 0s 337us/sample - loss: 0.7566 - val_loss: 0.9125\n",
      "Epoch 273/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7583 - val_loss: 0.9126\n",
      "Epoch 274/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7572 - val_loss: 0.9071\n",
      "Epoch 275/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7571 - val_loss: 0.9060\n",
      "Epoch 276/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7556 - val_loss: 0.9061\n",
      "Epoch 277/500\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.7562 - val_loss: 0.9057\n",
      "Epoch 278/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7561 - val_loss: 0.9074\n",
      "Epoch 279/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7593 - val_loss: 0.9064\n",
      "Epoch 280/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7560 - val_loss: 0.9052\n",
      "Epoch 281/500\n",
      "105/105 [==============================] - 0s 322us/sample - loss: 0.7558 - val_loss: 0.9035\n",
      "Epoch 282/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7558 - val_loss: 0.9071\n",
      "Epoch 283/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.9081\n",
      "Epoch 284/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7559 - val_loss: 0.9034\n",
      "Epoch 285/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7563 - val_loss: 0.9053\n",
      "Epoch 286/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.746 - 0s 181us/sample - loss: 0.7565 - val_loss: 0.9102\n",
      "Epoch 287/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7564 - val_loss: 0.9069\n",
      "Epoch 288/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7566 - val_loss: 0.9017\n",
      "Epoch 289/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.611 - 0s 257us/sample - loss: 0.7558 - val_loss: 0.9013\n",
      "Epoch 290/500\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.7569 - val_loss: 0.9037\n",
      "Epoch 291/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7557 - val_loss: 0.9043\n",
      "Epoch 292/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7561 - val_loss: 0.9044\n",
      "Epoch 293/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.510 - 0s 190us/sample - loss: 0.7561 - val_loss: 0.9076\n",
      "Epoch 294/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7572 - val_loss: 0.9097\n",
      "Epoch 295/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7565 - val_loss: 0.9085\n",
      "Epoch 296/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7559 - val_loss: 0.9065\n",
      "Epoch 297/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7568 - val_loss: 0.9076\n",
      "Epoch 298/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7566 - val_loss: 0.9130\n",
      "Epoch 299/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7562 - val_loss: 0.9089\n",
      "Epoch 300/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7573 - val_loss: 0.9073\n",
      "Epoch 301/500\n",
      "105/105 [==============================] - 0s 301us/sample - loss: 0.7561 - val_loss: 0.9112\n",
      "Epoch 302/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7559 - val_loss: 0.9109\n",
      "Epoch 303/500\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.7566 - val_loss: 0.9112\n",
      "Epoch 304/500\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.7564 - val_loss: 0.9092\n",
      "Epoch 305/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7559 - val_loss: 0.9079\n",
      "Epoch 306/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7575 - val_loss: 0.9063\n",
      "Epoch 307/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7565 - val_loss: 0.9067\n",
      "Epoch 308/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7560 - val_loss: 0.9057\n",
      "Epoch 309/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7569 - val_loss: 0.9063\n",
      "Epoch 310/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7555 - val_loss: 0.9051\n",
      "Epoch 311/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7566 - val_loss: 0.9120\n",
      "Epoch 312/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7562 - val_loss: 0.9141\n",
      "Epoch 313/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7563 - val_loss: 0.9133\n",
      "Epoch 314/500\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.7582 - val_loss: 0.9178\n",
      "Epoch 315/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7573 - val_loss: 0.9177\n",
      "Epoch 316/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7568 - val_loss: 0.9151\n",
      "Epoch 317/500\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.7565 - val_loss: 0.9153\n",
      "Epoch 318/500\n",
      "105/105 [==============================] - 0s 345us/sample - loss: 0.7562 - val_loss: 0.9148\n",
      "Epoch 319/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7575 - val_loss: 0.9194\n",
      "Epoch 320/500\n",
      "105/105 [==============================] - 0s 352us/sample - loss: 0.7567 - val_loss: 0.9141\n",
      "Epoch 321/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7571 - val_loss: 0.9149\n",
      "Epoch 322/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7565 - val_loss: 0.9147\n",
      "Epoch 323/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7566 - val_loss: 0.9132\n",
      "Epoch 324/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7576 - val_loss: 0.9183\n",
      "Epoch 325/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7580 - val_loss: 0.9179\n",
      "Epoch 326/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7569 - val_loss: 0.9170\n",
      "Epoch 327/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.9161\n",
      "Epoch 328/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7572 - val_loss: 0.9177\n",
      "Epoch 329/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7578 - val_loss: 0.9150\n",
      "Epoch 330/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7581 - val_loss: 0.9130\n",
      "Epoch 331/500\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.7570 - val_loss: 0.9070\n",
      "Epoch 332/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7562 - val_loss: 0.9071\n",
      "Epoch 333/500\n",
      "105/105 [==============================] - 0s 355us/sample - loss: 0.7564 - val_loss: 0.9048\n",
      "Epoch 334/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7580 - val_loss: 0.9064\n",
      "Epoch 335/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7581 - val_loss: 0.9055\n",
      "Epoch 336/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7561 - val_loss: 0.9039\n",
      "Epoch 337/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7562 - val_loss: 0.9029\n",
      "Epoch 338/500\n",
      "105/105 [==============================] - 0s 182us/sample - loss: 0.7565 - val_loss: 0.9017\n",
      "Epoch 339/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7564 - val_loss: 0.9032\n",
      "Epoch 340/500\n",
      "105/105 [==============================] - 0s 171us/sample - loss: 0.7589 - val_loss: 0.9016\n",
      "Epoch 341/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7559 - val_loss: 0.9065\n",
      "Epoch 342/500\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.7561 - val_loss: 0.9104\n",
      "Epoch 343/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7564 - val_loss: 0.9093\n",
      "Epoch 344/500\n",
      "105/105 [==============================] - 0s 389us/sample - loss: 0.7581 - val_loss: 0.9077\n",
      "Epoch 345/500\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7568 - val_loss: 0.9068\n",
      "Epoch 346/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7558 - val_loss: 0.9074\n",
      "Epoch 347/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7564 - val_loss: 0.9052\n",
      "Epoch 348/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7565 - val_loss: 0.9055\n",
      "Epoch 349/500\n",
      "105/105 [==============================] - 0s 189us/sample - loss: 0.7560 - val_loss: 0.9096\n",
      "Epoch 350/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7564 - val_loss: 0.9097\n",
      "Epoch 351/500\n",
      "105/105 [==============================] - 0s 406us/sample - loss: 0.7565 - val_loss: 0.9058\n",
      "Epoch 352/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7565 - val_loss: 0.9136\n",
      "Epoch 353/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7571 - val_loss: 0.9110\n",
      "Epoch 354/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7560 - val_loss: 0.9122\n",
      "Epoch 355/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7560 - val_loss: 0.9077\n",
      "Epoch 356/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7583 - val_loss: 0.9082\n",
      "Epoch 357/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7568 - val_loss: 0.9099\n",
      "Epoch 358/500\n",
      "105/105 [==============================] - 0s 291us/sample - loss: 0.7560 - val_loss: 0.9093\n",
      "Epoch 359/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7564 - val_loss: 0.9083\n",
      "Epoch 360/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7564 - val_loss: 0.9095\n",
      "Epoch 361/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7583 - val_loss: 0.9084\n",
      "Epoch 362/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7558 - val_loss: 0.9071\n",
      "Epoch 363/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7563 - val_loss: 0.9074\n",
      "Epoch 364/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7559 - val_loss: 0.9082\n",
      "Epoch 365/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7576 - val_loss: 0.9071\n",
      "Epoch 366/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7563 - val_loss: 0.9078\n",
      "Epoch 367/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7559 - val_loss: 0.9072\n",
      "Epoch 368/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7557 - val_loss: 0.9088\n",
      "Epoch 369/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7561 - val_loss: 0.9093\n",
      "Epoch 370/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7563 - val_loss: 0.9119\n",
      "Epoch 371/500\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.7571 - val_loss: 0.9120\n",
      "Epoch 372/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7570 - val_loss: 0.9054\n",
      "Epoch 373/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7566 - val_loss: 0.9022\n",
      "Epoch 374/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7569 - val_loss: 0.9025\n",
      "Epoch 375/500\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.7562 - val_loss: 0.9014\n",
      "Epoch 376/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7563 - val_loss: 0.9009\n",
      "Epoch 377/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7582 - val_loss: 0.9066\n",
      "Epoch 378/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7568 - val_loss: 0.9067\n",
      "Epoch 379/500\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.7556 - val_loss: 0.9073\n",
      "Epoch 380/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7571 - val_loss: 0.9018\n",
      "Epoch 381/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7570 - val_loss: 0.9035\n",
      "Epoch 382/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7560 - val_loss: 0.9043\n",
      "Epoch 383/500\n",
      "105/105 [==============================] - 0s 191us/sample - loss: 0.7569 - val_loss: 0.9055\n",
      "Epoch 384/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7560 - val_loss: 0.9069\n",
      "Epoch 385/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7566 - val_loss: 0.9062\n",
      "Epoch 386/500\n",
      "105/105 [==============================] - 0s 175us/sample - loss: 0.7562 - val_loss: 0.9076\n",
      "Epoch 387/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7567 - val_loss: 0.9050\n",
      "Epoch 388/500\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7578 - val_loss: 0.9058\n",
      "Epoch 389/500\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.7563 - val_loss: 0.9062\n",
      "Epoch 390/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7571 - val_loss: 0.9023\n",
      "Epoch 391/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7571 - val_loss: 0.8997\n",
      "Epoch 392/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7582 - val_loss: 0.8999\n",
      "Epoch 393/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7561 - val_loss: 0.9016\n",
      "Epoch 394/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7570 - val_loss: 0.9015\n",
      "Epoch 395/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7561 - val_loss: 0.9013\n",
      "Epoch 396/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7556 - val_loss: 0.9025\n",
      "Epoch 397/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7573 - val_loss: 0.9015\n",
      "Epoch 398/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7581 - val_loss: 0.8957\n",
      "Epoch 399/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7573 - val_loss: 0.8977\n",
      "Epoch 400/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7564 - val_loss: 0.8970\n",
      "Epoch 401/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7568 - val_loss: 0.8970\n",
      "Epoch 402/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7589 - val_loss: 0.9001\n",
      "Epoch 403/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7560 - val_loss: 0.8991\n",
      "Epoch 404/500\n",
      "105/105 [==============================] - 0s 381us/sample - loss: 0.7566 - val_loss: 0.9004\n",
      "Epoch 405/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7559 - val_loss: 0.8999\n",
      "Epoch 406/500\n",
      "105/105 [==============================] - 0s 335us/sample - loss: 0.7561 - val_loss: 0.8983\n",
      "Epoch 407/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7585 - val_loss: 0.9025\n",
      "Epoch 408/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7571 - val_loss: 0.9096\n",
      "Epoch 409/500\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.7580 - val_loss: 0.9142\n",
      "Epoch 410/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7568 - val_loss: 0.9132\n",
      "Epoch 411/500\n",
      "105/105 [==============================] - 0s 322us/sample - loss: 0.7566 - val_loss: 0.9100\n",
      "Epoch 412/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7563 - val_loss: 0.9084\n",
      "Epoch 413/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7568 - val_loss: 0.9100\n",
      "Epoch 414/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7583 - val_loss: 0.9143\n",
      "Epoch 415/500\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.7564 - val_loss: 0.9129\n",
      "Epoch 416/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7578 - val_loss: 0.9126\n",
      "Epoch 417/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7572 - val_loss: 0.9123\n",
      "Epoch 418/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7567 - val_loss: 0.9075\n",
      "Epoch 419/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7567 - val_loss: 0.9031\n",
      "Epoch 420/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7571 - val_loss: 0.9051\n",
      "Epoch 421/500\n",
      "105/105 [==============================] - 0s 201us/sample - loss: 0.7578 - val_loss: 0.9057\n",
      "Epoch 422/500\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.7572 - val_loss: 0.9061\n",
      "Epoch 423/500\n",
      "105/105 [==============================] - 0s 180us/sample - loss: 0.7578 - val_loss: 0.9047\n",
      "Epoch 424/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7563 - val_loss: 0.9049\n",
      "Epoch 425/500\n",
      "105/105 [==============================] - 0s 202us/sample - loss: 0.7559 - val_loss: 0.9029\n",
      "Epoch 426/500\n",
      "105/105 [==============================] - 0s 199us/sample - loss: 0.7559 - val_loss: 0.9030\n",
      "Epoch 427/500\n",
      "105/105 [==============================] - 0s 324us/sample - loss: 0.7561 - val_loss: 0.9032\n",
      "Epoch 428/500\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.7563 - val_loss: 0.9067\n",
      "Epoch 429/500\n",
      "105/105 [==============================] - 0s 443us/sample - loss: 0.7562 - val_loss: 0.9096\n",
      "Epoch 430/500\n",
      "105/105 [==============================] - 0s 452us/sample - loss: 0.7559 - val_loss: 0.9106\n",
      "Epoch 431/500\n",
      "105/105 [==============================] - 0s 351us/sample - loss: 0.7566 - val_loss: 0.9080\n",
      "Epoch 432/500\n",
      "105/105 [==============================] - 0s 276us/sample - loss: 0.7560 - val_loss: 0.9101\n",
      "Epoch 433/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7567 - val_loss: 0.9091\n",
      "Epoch 434/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7566 - val_loss: 0.9122\n",
      "Epoch 435/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7574 - val_loss: 0.9147\n",
      "Epoch 436/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7563 - val_loss: 0.9126\n",
      "Epoch 437/500\n",
      "105/105 [==============================] - 0s 256us/sample - loss: 0.7564 - val_loss: 0.9140\n",
      "Epoch 438/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7571 - val_loss: 0.9126\n",
      "Epoch 439/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7564 - val_loss: 0.9117\n",
      "Epoch 440/500\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.7563 - val_loss: 0.9095\n",
      "Epoch 441/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7559 - val_loss: 0.9098\n",
      "Epoch 442/500\n",
      "105/105 [==============================] - 0s 532us/sample - loss: 0.7564 - val_loss: 0.9096\n",
      "Epoch 443/500\n",
      "105/105 [==============================] - 0s 463us/sample - loss: 0.7567 - val_loss: 0.9140\n",
      "Epoch 444/500\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.7562 - val_loss: 0.9113\n",
      "Epoch 445/500\n",
      "105/105 [==============================] - 0s 554us/sample - loss: 0.7560 - val_loss: 0.9069\n",
      "Epoch 446/500\n",
      "105/105 [==============================] - 0s 313us/sample - loss: 0.7560 - val_loss: 0.9042\n",
      "Epoch 447/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7567 - val_loss: 0.9031\n",
      "Epoch 448/500\n",
      "105/105 [==============================] - 0s 427us/sample - loss: 0.7557 - val_loss: 0.9015\n",
      "Epoch 449/500\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 0.7561 - val_loss: 0.8964\n",
      "Epoch 450/500\n",
      "105/105 [==============================] - 0s 494us/sample - loss: 0.7564 - val_loss: 0.8975\n",
      "Epoch 451/500\n",
      "105/105 [==============================] - 0s 334us/sample - loss: 0.7565 - val_loss: 0.8999\n",
      "Epoch 452/500\n",
      "105/105 [==============================] - 0s 608us/sample - loss: 0.7570 - val_loss: 0.8995\n",
      "Epoch 453/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7559 - val_loss: 0.8981\n",
      "Epoch 454/500\n",
      "105/105 [==============================] - 0s 475us/sample - loss: 0.7562 - val_loss: 0.8955\n",
      "Epoch 455/500\n",
      "105/105 [==============================] - 0s 598us/sample - loss: 0.7569 - val_loss: 0.8941\n",
      "Epoch 456/500\n",
      "105/105 [==============================] - 0s 723us/sample - loss: 0.7568 - val_loss: 0.8955\n",
      "Epoch 457/500\n",
      "105/105 [==============================] - 0s 405us/sample - loss: 0.7566 - val_loss: 0.8999\n",
      "Epoch 458/500\n",
      "105/105 [==============================] - 0s 366us/sample - loss: 0.7557 - val_loss: 0.8977\n",
      "Epoch 459/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7573 - val_loss: 0.8978\n",
      "Epoch 460/500\n",
      "105/105 [==============================] - 0s 466us/sample - loss: 0.7567 - val_loss: 0.8980\n",
      "Epoch 461/500\n",
      "105/105 [==============================] - 0s 485us/sample - loss: 0.7566 - val_loss: 0.8960\n",
      "Epoch 462/500\n",
      "105/105 [==============================] - 0s 399us/sample - loss: 0.7570 - val_loss: 0.8977\n",
      "Epoch 463/500\n",
      "105/105 [==============================] - 0s 447us/sample - loss: 0.7571 - val_loss: 0.8950\n",
      "Epoch 464/500\n",
      "105/105 [==============================] - 0s 361us/sample - loss: 0.7568 - val_loss: 0.8950\n",
      "Epoch 465/500\n",
      "105/105 [==============================] - 0s 304us/sample - loss: 0.7565 - val_loss: 0.8942\n",
      "Epoch 466/500\n",
      "105/105 [==============================] - 0s 347us/sample - loss: 0.7572 - val_loss: 0.8944\n",
      "Epoch 467/500\n",
      "105/105 [==============================] - 0s 327us/sample - loss: 0.7566 - val_loss: 0.8977\n",
      "Epoch 468/500\n",
      "105/105 [==============================] - 0s 846us/sample - loss: 0.7575 - val_loss: 0.8985\n",
      "Epoch 469/500\n",
      "105/105 [==============================] - 0s 371us/sample - loss: 0.7566 - val_loss: 0.8997\n",
      "Epoch 470/500\n",
      "105/105 [==============================] - 0s 658us/sample - loss: 0.7571 - val_loss: 0.9013\n",
      "Epoch 471/500\n",
      "105/105 [==============================] - 0s 815us/sample - loss: 0.7563 - val_loss: 0.9019\n",
      "Epoch 472/500\n",
      "105/105 [==============================] - 0s 456us/sample - loss: 0.7569 - val_loss: 0.9014\n",
      "Epoch 473/500\n",
      "105/105 [==============================] - 0s 570us/sample - loss: 0.7562 - val_loss: 0.8979\n",
      "Epoch 474/500\n",
      "105/105 [==============================] - 0s 418us/sample - loss: 0.7564 - val_loss: 0.8983\n",
      "Epoch 475/500\n",
      "105/105 [==============================] - 0s 855us/sample - loss: 0.7570 - val_loss: 0.9037\n",
      "Epoch 476/500\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.7561 - val_loss: 0.9055\n",
      "Epoch 477/500\n",
      "105/105 [==============================] - 0s 400us/sample - loss: 0.7560 - val_loss: 0.9051\n",
      "Epoch 478/500\n",
      "105/105 [==============================] - 0s 541us/sample - loss: 0.7563 - val_loss: 0.9038\n",
      "Epoch 479/500\n",
      "105/105 [==============================] - 0s 237us/sample - loss: 0.7569 - val_loss: 0.9069\n",
      "Epoch 480/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7563 - val_loss: 0.9078\n",
      "Epoch 481/500\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.7557 - val_loss: 0.9042\n",
      "Epoch 482/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7559 - val_loss: 0.9037\n",
      "Epoch 483/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7564 - val_loss: 0.9087\n",
      "Epoch 484/500\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.7565 - val_loss: 0.9108\n",
      "Epoch 485/500\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.7566 - val_loss: 0.9090\n",
      "Epoch 486/500\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.7562 - val_loss: 0.9121\n",
      "Epoch 487/500\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.7578 - val_loss: 0.9115\n",
      "Epoch 488/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7569 - val_loss: 0.9095\n",
      "Epoch 489/500\n",
      "105/105 [==============================] - 0s 212us/sample - loss: 0.7561 - val_loss: 0.9131\n",
      "Epoch 490/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7560 - val_loss: 0.9121\n",
      "Epoch 491/500\n",
      "105/105 [==============================] - 0s 294us/sample - loss: 0.7559 - val_loss: 0.9111\n",
      "Epoch 492/500\n",
      "105/105 [==============================] - 0s 270us/sample - loss: 0.7579 - val_loss: 0.9075\n",
      "Epoch 493/500\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.7569 - val_loss: 0.9099\n",
      "Epoch 494/500\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.7564 - val_loss: 0.9100\n",
      "Epoch 495/500\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.7559 - val_loss: 0.9071\n",
      "Epoch 496/500\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.7560 - val_loss: 0.9058\n",
      "Epoch 497/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7565 - val_loss: 0.9092\n",
      "Epoch 498/500\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.7560 - val_loss: 0.9051\n",
      "Epoch 499/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7569 - val_loss: 0.9084\n",
      "Epoch 500/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7560 - val_loss: 0.9058\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='mse')\n",
    "history = model.fit(x_train, y_train, epochs=500,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 손실과 검증 세트에 대한 손실 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, history.history['loss'], label='Training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6.3 tf.keras 모델의 저장과 복원\n",
    "\n",
    "앞서 만든 모델 가중치를 save_weights() 메서드로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('simple_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드 실행 시 현재 폴더에 simple_weights.h5 파일을 생성하고 모든 층의 가중치를 저장.\n",
    "\n",
    "save_weights 메서드는 텐서플로의 체크포인트 포맷으로 가중치를 저장.\n",
    "\n",
    "svae_format 매개변수를 h5로 지정해 HDF5 파일 포맷으로 저장 가능.\n",
    "\n",
    "저장된 가중치를 사용해 새로운 모델을 만들고 load_weights() 메서드를 사용해 가중치 로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "model.load_weights('simple_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 180us/sample - loss: 2.1160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1160143756866456"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpoint 콜백을 사용.\n",
    "\n",
    "검증 손실을 모니터링하면서 최상의 모델 가중치를 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/500\n",
      "120/120 [==============================] - 1s 7ms/sample - loss: 1.7147 - val_loss: 1.2115\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 792us/sample - loss: 1.5736 - val_loss: 1.1315\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.4562 - val_loss: 1.0676\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 970us/sample - loss: 1.3551 - val_loss: 1.0137\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 840us/sample - loss: 1.2725 - val_loss: 0.9652\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 706us/sample - loss: 1.1957 - val_loss: 0.9301\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 1.1357 - val_loss: 0.9003\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 648us/sample - loss: 1.0826 - val_loss: 0.8757\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 532us/sample - loss: 1.0359 - val_loss: 0.8562\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 598us/sample - loss: 0.9981 - val_loss: 0.8412\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 478us/sample - loss: 0.9687 - val_loss: 0.8283\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 681us/sample - loss: 0.9403 - val_loss: 0.8192\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 801us/sample - loss: 0.9183 - val_loss: 0.8111\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 740us/sample - loss: 0.8972 - val_loss: 0.8060\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 933us/sample - loss: 0.8819 - val_loss: 0.8026\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 665us/sample - loss: 0.8667 - val_loss: 0.8007\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 614us/sample - loss: 0.8535 - val_loss: 0.7994\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 640us/sample - loss: 0.8439 - val_loss: 0.7983\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 747us/sample - loss: 0.8377 - val_loss: 0.7973\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 914us/sample - loss: 0.8297 - val_loss: 0.7972\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 241us/sample - loss: 0.8238 - val_loss: 0.7988\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.8184 - val_loss: 0.7997\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 197us/sample - loss: 0.8145 - val_loss: 0.8021\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 177us/sample - loss: 0.8105 - val_loss: 0.8044\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 0.8065 - val_loss: 0.8063\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath='my_model.h5',\n",
    "                                                    monitor='val_loss', save_best_only=True),\n",
    "                 tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    "history = model.fit(x_train, y_train, epochs=500,\n",
    "                    validation_split=0.2, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1, len(history.history['loss']) + 1)\n",
    "plt.plot(epochs, history.history['loss'], label='Training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 그래프를 그려 보면 검증 손실이 감소되지 않으므로 훈련이 일찍 멈춘 것을 확인할 수 있음.\n",
    "\n",
    "**선형 모델을 그래프로 표현** (짙은 색 원은 훈련 세트, 밝은 색 원은 테스트 세트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_arr = np.arange(-2, 2, 0.1)\n",
    "y_arr = model.predict(x_arr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_train, y_train, 'bo')\n",
    "plt.plot(x_test, y_test, 'bo', alpha=0.3)\n",
    "plt.plot(x_arr, y_arr, '-r', lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7 계산 그래프 시각화\n",
    "\n",
    "텐서보드는 모델의 학습 과정뿐만 아니라 계산 그래프도 시각화할 수 있는 모듈.\n",
    "\n",
    "계산 그래프를 시각화하면 노드 사이 연결을 보고 의존성을 확인하고 필요할 때 모델을 디버깅할 수 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
