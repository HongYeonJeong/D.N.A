{"cells":[{"cell_type":"markdown","source":["# Chapter4. 구조적 API 개요\n\n구조적 API의 세 가지 분산 컬렉션 API\n* Dataset\n* DataFrame\n* SQL 테이블과 뷰\n\n배치와 스트리밍 처리에서 구조적 API를 사용할 수 있다. 구조적 API를 활용하면 배치 작업을 스트리밍 작업으로 손쉽게 변환 가능. \n\n<학습할 내용>\n* 타입형/비타입형 API의 개념과 차이점\n* 핵심 용어\n* 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b50bd343-1e4d-4833-b7cf-cfb94e5cb82d"}}},{"cell_type":"markdown","source":["## 4.1 DataFrame과 Dataset\n\n스파크가 가지는 두 가지 구조화된 컬렉션 개념 : DataFrame & Dataset\n\n* DataFrame & Dataset : 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션\n* 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 하고, 값이 없는 경우 null로 표시\n* 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가져야 함 \n* DataFrame & Dataset은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며 불변서응ㄹ 가짐\n* DataFrame에 액션을 호출하면 스파크는 트랜스포메이션을 실제로 실행하고 결과 반환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dab01b30-dbfa-48d8-ab15-f3f7cb1575ca"}}},{"cell_type":"markdown","source":["## 4.2 스키마\n\n스키마 : DataFrame의 컬럼명과 데이터 타입 정의\n\n* 스키마는 데이터소스에서 얻거나 직접 정의 가능\n* 여러 데이터 타입으로 구성되므로 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법 필요"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01f4342b-47bb-4d78-b0da-82cf7cb724f1"}}},{"cell_type":"markdown","source":["## 4.3 스파크의 구조적 데이터 타입 개요\n\n스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진 사용.\n\n카탈리스트 엔진은 다양한 실행 최적화 기능 제공. \n\n스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되고 각 언어에 대한 매핑 테이블을 가짐.\n\n예제) 스파크의 덧셈 연산 수행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccb6434b-ec01-41d1-a35c-266fb7335a96"}}},{"cell_type":"code","source":["df = spark.range(500).toDF(\"number\")\ndf.select(df[\"number\"] + 10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f7bff0a-b745-41b7-978f-e4b5f5f689fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: DataFrame[(number + 10): bigint]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: DataFrame[(number + 10): bigint]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 4.3.1 DataFrame과 Dataset 비교\n\n구조적 API에는 **비타입형 DataFrame** & **타입형 Dataset** 존재\n\n< 스파크의 DataFrame >\n* 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서야 확인 가능\n* Row 타입으로 구성된 Dataset (Row 타입 : 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적인 표현 방식)\n* 파이썬이나 R에서는 스파크의 Dataset을 사용할 수 없지만 최적화된 포맷인 DataFrame으로 처리 가능\n\n< 스파크의 Dataset >\n* 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인\n* JVM 기반의 언어인 스칼라와 자바에서만 지원\n* 데이터 타입을 정의하려면 스칼라의 케이스 클래스(case class)나 자바 빈(JavaBean)을 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"573256cc-7f17-4db2-94ed-840e7e9a8e5d"}}},{"cell_type":"markdown","source":["### 4.3.2 컬럼\n\n컬럼의 표현\n* 정수형이나 문자열 같은 단순 데이터 타입\n* 배열이나 맵 같은 복합 데이터 타입\n* null 값\n\n스파크는 데이터 타입의 모든 정보를 추적하며 다양한 컬럼 변환 방법을 제공"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7e158b7-0e4a-44c3-ae9e-45522824b56a"}}},{"cell_type":"markdown","source":["### 4.3.3 로우\n\n로우는 데이터의 레코드로, DataFrame의 레코드는 Row 타입으로 구성.\n\n로우는 SQL, RDD, 데이터소스에서 얻거나 직접 생성 가능. \n\n예제) range 메서드를 사용해 DataFrame 생성 (Row 객체로 이루어진 배열 반환)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07a41333-ba4c-4953-9d10-a6e46eaa3b1a"}}},{"cell_type":"code","source":["spark.range(2).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac45f7c5-3aa3-4bdd-a41d-b55b425734f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: [Row(id=0), Row(id=1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: [Row(id=0), Row(id=1)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 4.3.4 스파크 데이터 타입"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85b7267a-d437-458c-9077-a2680864a6e6"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nb = ByteType()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2375afb-dfa5-4058-a3aa-b73d7f67003f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4.4 구조적 API의 실행 과정\n\n구조적 API 쿼리가 사용자 코드에서 실제 실행 코드로 변환되는 과정\n1. DataFrame/Dataset/SQL을 이용해 코드 작성\n2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환\n3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인\n4. 스파크는 클러스터에서 물리적 실행 계획(RDD  처리)을 실행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a38a2b17-971e-4e26-9e7c-8d0cefcc62cb"}}},{"cell_type":"markdown","source":["### 4.4.1 논리적 실행 계획\n\n논리적 실행 계획 단계에서는 추상적 트랜스포메이션만 표현. \n* 드라이버나 익스큐터의 정보를 고려하지 않음\n* 사용자의 다양한 표현식을 최적화된 버전으로 변환\n* 사용자 코드는 검증 전 논리적 실행 계획으로 변환 (코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단하는 과정이므로 아직 실행 계획을 검증하지 않은 상태)\n* 스파크 분석기는 컬럼과 테이블을 검증하기 위해 카탈로그, 모든 테이블의 저장소 그리고 DataFrame 정보 활용\n* 필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 만들어지지 않음\n* 테이블과 컬럼에 대한 검증 결과는 카탈리스트 옵티마이저로 전달"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abddfe86-cfaa-4c0e-bfbb-95c9e30218de"}}},{"cell_type":"markdown","source":["### 4.4.2 물리적 실행 계회\n\n물리적 실행 계획은 논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의.\n* 다양한 물리적 실행 전량을 생성하고 비용 모델을 이용해 비교한 후 최적의 전략 선택\n* 일련의 RDD와 트랜스포메이션으로 변환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a3fc14c-3a9f-4241-a387-4760f702231f"}}},{"cell_type":"markdown","source":["### 4.4.3 실행\n\n1. 스파크는 물리적 실행 계획을 선정한 다음 저수준 프로그래밍 인터페이스인 RDD룰 대상으로 모든 코드 실행.\n\n2. 스파크는 런타임에 전체 태스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화 수행.\n\n3. 스파크는 처리 결과를 사용자에게 반환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"860b8d39-4a79-4d0e-b36b-d8a646e76efa"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter4","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":446413481131074}},"nbformat":4,"nbformat_minor":0}
