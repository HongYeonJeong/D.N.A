{"cells":[{"cell_type":"markdown","source":["# Chapter3. 스파크 기능 둘러보기\n\n스파크는 기본 요소인 저수준 API와 구조적 API 그리고 추가 기능을 제공하는 일련의 표준 라이브러리로 구성되어 있다.\n\n스파크의 라이브러리는 그래프 분석, 머신러닝 그리고 스트리밍 등 다양한 작업을 지원하며, 컴퓨팅 및 스토리지 시스템과의 통합을 돕는 역할을 수행한다. \n\n<학습할 내용>\n* spark-submit 명령문으로 운영용 애플리케이션 실행\n* Dataset : 타입 안전성을 제공하는 구조적 API\n* 구조적 스트리밍\n* 머신러닝과 고급 분석\n* RDD : 스파크의 저수준 API\n* SparkR\n* 서드파티 패키지 에코시스템"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a02decd-405c-419f-9037-3f2d0be96fcb"}}},{"cell_type":"markdown","source":["## 3.1 운영용 애플리케이션 실행하기\n\nspark-submit 명령을 사용해 대화형 셀에서 개발한 프로그램을 운영용 애플리케이션으로 쉽게 전환 가능. \n\n* spark-submit 명령 : 애플리케이션 코드를 클러스터에 전송해 실행시키는 역할. (애플리케이션 실행에 필요한 자원과 실행 방식 그리고 다양한 옵션을 지정 가능)\n\n클러스터에 제출된 애플리케이션은 작업이 종료되거나 에러가 발생할 때까지 실행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99ef782a-9c8c-419b-84aa-13c957d8312a"}}},{"cell_type":"markdown","source":["## 3.2 Dataset : 타입 안전성을 제공하는 구조적 API\n\nDataset은 자바와 스칼라의 정적 데이터 타입에 맞는 코드 즉 정적 타입 코드를 지원하기 위함. 따라서 동적 타입 언어인 파이썬과 R에서는 사용 불가능.\n\nDataFrame은 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입의 객체로 구성된 분산 컬렉션. \n\n< Dataset API >\n* Dataset API는 DataFrame의 레코드를 사용자가 자바나 스칼라로 정의한 클래스에 할당하고 자바의 ArrayList 또는 스칼라의 Seq 객체 등의 고정 타입형 컬렉션으로 다룰 수 있는 기능 제공\n* Dataset API는 타입 안정성을 지원해 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없으므로 다수의 소프트웨어 엔지니어가 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션을 개발하는데 유용\n* Dataset 클래스는 내부 객체의 데이터 타입을 매개변수로 사용 \n* 필요한 경우에 선택적으로 사용 가능\n* 데이터 타입을 정의하고 map과 filter 함수 사용 가능\n* collect 메서드나 take 메서드를 호출하면 DataFrame을 구성하는 Row 타입의 객체가 아닌 Dataset에 매개변수로 지정한 타입의 객체를 반환하므로 코드 변경 없이 타입 안정성을 보장할 수 있고 로컬이나 분산 클러스터 환경에서 데이터를 안전하게 다룰 수 있다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf2f529-277e-45ee-b760-4619acf277fb"}}},{"cell_type":"markdown","source":["## 3.3 구조적 스트리밍\n\n구조적 스트리밍 : 스파크 2.2 버전에서 안정화된 스트림 처리용 고수준 API\n\n구조적 스트리밍을 사용하면\n* 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행 가능\n* 지연 시간을 줄이고 증분 처리 가능\n* 배치 처리용 코드를 일부 수정해 스트리밍 처리를 수행하고 값을 빠르게 얻을 수 있음\n* 프로토타입을 배치 잡으로 개발한 다음 스트리밍 잡으로 변환할 수 있어 개념 잡기가 수월\n\n예제) 소매(retail) 데이터셋 중 하루치 데이터를 나타내는 by-day 디렉터리 파일 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d07cb55b-3081-496f-8126-5ea824fa00ed"}}},{"cell_type":"code","source":["staticDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferShema\", \"true\")\\\n  .load('/FileStore/tables/data/retail-data/by-day/*.csv')\n\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = staticDataFrame.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c36ec7b-cbf8-4d83-a492-433386780e95"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["시계열 데이터를 다루기 때문에 데이터를 그룹화하고 집계하는 방법\n\n예시) 특정 고객(CustomerId로 구분)이 대량으로 구매하는 영업 시간을 살펴보기 위해 총 구매비용 컬럼을 추가하고 고객이 가장 많이 소비한 날 찾아보기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c37f6d34-ec8b-474f-95a2-2ef0d9d3fa92"}}},{"cell_type":"code","source":["from pyspark.sql.functions import window, col\n\nstaticDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\\\n  .show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e754258-406e-4e4c-ab9f-400a209edc7e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+--------------------+------------------+\n|CustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n|   15274.0|{2011-12-05 00:00...| 665.1600000000002|\n|   14719.0|{2011-12-08 00:00...| 812.8399999999998|\n|   16794.0|{2011-12-08 00:00...|201.32000000000002|\n|   12464.0|{2011-11-29 00:00...|             563.8|\n|   15269.0|{2011-11-16 00:00...|             817.6|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   15274.0|{2011-12-05 00:00...| 665.1600000000002|\n   14719.0|{2011-12-08 00:00...| 812.8399999999998|\n   16794.0|{2011-12-08 00:00...|201.32000000000002|\n   12464.0|{2011-11-29 00:00...|             563.8|\n   15269.0|{2011-11-16 00:00...|             817.6|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["null 값을 가진 결과의 로우는 일부 트랜잭션에서 customerId 값이 없음을 의미.\n\n스트리밍 코드는 read 메서드 대신 readStream 메서드를 사용. 그리고 maxFilesPerTrigger 옵션을 추가로 지정해 한 번에 읽을 파일 수 설정 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa294b5c-78e1-4d2c-bdb3-9b1f8bd36362"}}},{"cell_type":"code","source":["streamingDataFrame = spark.readStream\\\n  .schema(staticSchema)\\\n  .option(\"maxFilesPerTrigger\", 1)\\\n  .format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .load(\"/FileStore/table/data/retail-data/by-day/*.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63800cb1-9864-4bba-b116-c1d124e4b10b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# DataFrame이 스트리밍 유형인지 확인 - true/false 반환\nstreamingDataFrame.isStreaming  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59dcdc42-9887-4e5e-a32c-e3379c12bac5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: True</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["기존 DataFrame 처리와 동일한 비지니스 로직인지 적용\n\n예시) 총 판매 금액 계산"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"855656db-aefc-4ca1-a3e5-568514bd2655"}}},{"cell_type":"code","source":["purchaseByCustomerPerHour = streamingDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38eabb55-5953-404c-932e-d989539a865d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["지연 연산이므로 데이터 플로를 실행하기 위해 스트리밍 액션 호출 필수.\n\n스트리밍 액션은 어딘가에 데이터를 채워 넣어야 하므로 count 메서드와 같은 일반적인 정적 액션과는 조금 다른 특성을 가진다. 여기서 사용하는 스트리밍 액션은 트리거가 실행된 다음 데이터를 갱신하게 될 인메모리 테이블에 데이터를 저장. \n\n(스파크는 이전 집계값보다 더 큰 값이 발생한 경우에만 인메모리 테이블을 갱신하므로 언제나 가장 큰 값을 얻을 수 있다.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04af8eed-6cda-4e1d-b48f-4fb8e8120afd"}}},{"cell_type":"code","source":["purchaseByCustomerPerHour.writeStream\\\n  .format(\"memory\")\\\n  .queryName(\"customer_purchases\")\\\n  .outputMode(\"complete\")\\\n  .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a0480cd-8e9a-48d0-bf06-afd47b719c8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[16]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fa36e530d60&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fa36e530d60&gt;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["스트림이 시작되면 쿼리 실행 결과가 어떠한 형태로 인메모리 테이블에 기록되는지 확인 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68415bab-f36f-441c-8830-d57ee7b54736"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  SELECT *\n  FROM customer_purchases\n  ORDER BY 'sum(total_cost)' DESC\n  \"\"\")\\\n  .show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbfe169-559e-4529-9515-746e327a5aba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+------+---------------+\n|CustomerId|window|sum(total_cost)|\n+----------+------+---------------+\n+----------+------+---------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+---------------+\nCustomerId|window|sum(total_cost)|\n+----------+------+---------------+\n+----------+------+---------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3.4 머신러닝과 고급 분석\n\n* 스파크에서는 내장된 머신러닝 라이브러리인 MLlib을 사용해 대규모 머신러닝 수행 가능. \n* MLlib을 사용하면 대용량 데이터를 대상으로 전처리, 멍잉(데이터 랭클링, 원본 데이터를 다른 형태로 변환 혹은 매핑하는 과정 의미), 모델 학습 및 예측이 가능.\n* 구조적 스트리밍에서 예측하고자 할 때도 MLlib에서 학습시킨 다양한 예측 모델 사용 가능.\n* 스파크는 분류(classification), 회귀(regression), 군집화(clustering), 딥러닝(deep learning)까지 머신러닝과 관련된 정교한 API 제공\n* 스파크는 데이터 전처리에 사용하는 다양한 메서드 제공\n\nk-평균이라는 표준 알고리즘을 이용한 기본적인 군집화 수행\n\n예제) 원본 데이터를 올바른 포맷으로 만드는 트랜스포메이션을 정의하고 실제로 모델을 학습해 예측 수행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26f9ac80-11a6-4dd5-9388-0332e64b179c"}}},{"cell_type":"code","source":["staticDataFrame.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a030a02a-7608-4280-8c8e-9887e807c783"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: string (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: string (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- InvoiceNo: string (nullable = true)\n-- StockCode: string (nullable = true)\n-- Description: string (nullable = true)\n-- Quantity: string (nullable = true)\n-- InvoiceDate: string (nullable = true)\n-- UnitPrice: string (nullable = true)\n-- CustomerID: string (nullable = true)\n-- Country: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["MLlib의 머신러닝 알고리즘을 사용하기 위해서는 수치형 데이터가 필요하므로 예제의 데이터에서 타임스탬프, 정수, 문자열 타입의 데이터를 수치형 데이터로 변환 과정 필요."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e26e7606-2373-4a63-a6f9-2095cfb7713b"}}},{"cell_type":"code","source":["# DataFrame 트랜스포메이션을 사용해 날짜 데이터를 다루는 예제\n\nfrom pyspark.sql.functions import date_format, col\n\npreppedDataFrame = staticDataFrame\\\n  .na.fill(0)\\\n  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n  .coalesce(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc3117c5-6c12-4d0d-948f-d1d1bfe073f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["데이터를 학습 데이터셋과 테스트 데이터셋으로 분리\n\n* 예제에서는 특정 구매가 이루어진 날짜를 기준으로 직접 분리\n* MLlib의 트랜스포메이션 API(TrainValidationSplit 혹은 CrossValidator)를 사용해 학습 데이터셋과 테스트 데이터셋을 생성 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82b4e262-8e35-4345-8cee-7fe6fcbb950a"}}},{"cell_type":"code","source":["trainDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate < '2011-07-01'\")\n\ntestDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate >= '2011-07-01'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4b34002-82f7-493b-99d1-7762aebd49fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["액션을 호출해 데이터 분리\n\n예제의 데이터는 시계열 데이터셋으로 임의 날짜를 기준으로 데이터를 분리한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d5b5bff-5c50-42ec-a5ec-86c4c088a4b3"}}},{"cell_type":"code","source":["trainDataFrame.count()\ntestDataFrame.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f02a118d-c911-4769-a548-264c5988839d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: 592012</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: 592012</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["스파크 MLlib은 일반적인 트랜스포메이션을 자동화하는 다양한 트랜스포메이션 제공하는데 그 중 하나가 StringIndexer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0020481-642b-46d9-a83e-50878e484b8d"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer()\\\n  .setInputCol(\"day_of_week\")\\\n  .setOutputCol(\"day_of_index\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf844011-da83-404b-acac-9e79528ffa5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["앞에서는 요일을 수치형(토요일은 6, 월요일은 1, ...)으로 반환해 암묵적으로 토요일이 월요일보다 크다는 것을 의미하게 된다.\n\n따라서 OneHotEncoder를 사용해 각 값을 자체 컬럼으로 인코딩해야 한다. 이렇게 하면 특정 요일이 해당 요일인지 아닌지 불리언 타입으로 나타낼 수 있다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5952e19-6294-4cbe-9b1f-a49fa6de852d"}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder()\\\n  .setInputCol(\"day_of_week_index\")\\\n  .setOutputCol(\"day_of_woeek_encoded\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c21f745a-b6d7-40ee-9a56-278dc4a6399f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["위의 결과는 벡터 타입을 구성할 컬럼 중 하나로 사용."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0ae2147-140b-4c3e-a408-6fbe1a9b4c2b"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler()\\\n  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n  .setOutputCol(\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5224bd4-0737-41af-8835-a8777f6c1e6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["위의 예제는 가격, 수량, 특정 날짜의 요일을 가지고 있다. 다음은 나중에 입력 값으로 들어올 데이터가 같은 프로세스를 거쳐 변환되도록 파이프라인을 설정"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f46b02a8-34ad-4f1d-9312-a68060afc201"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\ntransformationPipeline = Pipeline()\\\n  .setStages([indexer, encoder, vectorAssembler])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cf44552-516d-4659-bcb7-57bf94630d59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["모델 학습 준비 과정\n\n* 변환자를 데이터셋에 적합\n* StringIndexer는 인덱싱할 고윳값의 수를 알아야 함. 알 수 없다면 컬럼에 있는 모든 고윳값을 조사하고 인덱싱"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0237c94-f33a-4be9-9561-f4e22f090bd8"}}},{"cell_type":"code","source":["fittedPipeline = transformationPipeline.fit(trainDataFrame)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9693f4d-eb19-4ec7-8ab4-9cd0a93e12f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2861523922385080&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>fittedPipeline <span class=\"ansi-blue-fg\">=</span> transformationPipeline<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>trainDataFrame<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/pipeline.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>                     dataset <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>  <span class=\"ansi-red-fg\"># must be an Estimator</span>\n<span class=\"ansi-green-fg\">--&gt; 114</span><span class=\"ansi-red-fg\">                     </span>model <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                     transformers<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>                     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&lt;</span> indexOfLastEstimator<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: day_of_week_index does not exist. Available: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, day_of_week, day_of_index</div>","errorSummary":"<span class=\"ansi-red-fg\">IllegalArgumentException</span>: day_of_week_index does not exist. Available: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, day_of_week, day_of_index","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2861523922385080&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>fittedPipeline <span class=\"ansi-blue-fg\">=</span> transformationPipeline<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>trainDataFrame<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/pipeline.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>                     dataset <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>  <span class=\"ansi-red-fg\"># must be an Estimator</span>\n<span class=\"ansi-green-fg\">--&gt; 114</span><span class=\"ansi-red-fg\">                     </span>model <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                     transformers<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>                     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&lt;</span> indexOfLastEstimator<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: day_of_week_index does not exist. Available: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, day_of_week, day_of_index</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["학습 데이터셋에 변환자를 적합시키면 학습을 위한 파이프라인 준비. 이를 이용해 일관되고 반복적인 방식으로 모든 데이터 변환 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fa4b9d5-694c-440a-bd2b-782c54bc0c3e"}}},{"cell_type":"code","source":["transformedTraining = fittedPipeline.transform(trainDataFrame)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7eac464-c441-4caa-a24a-a4ac9f3d1635"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-446413481131067&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>transformedTraining <span class=\"ansi-blue-fg\">=</span> fittedPipeline<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>trainDataFrame<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;fittedPipeline&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;fittedPipeline&#39; is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-446413481131067&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>transformedTraining <span class=\"ansi-blue-fg\">=</span> fittedPipeline<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>trainDataFrame<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;fittedPipeline&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["모델을 학습하기 위해서 관련 클래스 import 및 인스턴스 생성"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24dc796d-647e-4a01-95cc-7774fc70ab23"}}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\n\nkmeans = KMeans()\\\n  .setK(20)\\\n  .setSeed(1L)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c2ac751-8005-4cca-8dc7-5b6f9a4febab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["kmModel = kmeans.fit(transformedTraining)\n\n# 성과 지표에 따라 학습 데이터셋에 대한 비용 계산\ntransformedTest = fittedPipeline.transform(testDataFrame)\nkmModel.computeCost(transformedTest)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c64e082a-468b-4d13-80a0-5c967781ce3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3.5 저수준 API\n\n스파크는 RDD를 통해 자바와 파이썬 객체를 다루는 데 필요한 기본 기능인 저수준 API를 제공하고, 거의 모든 기능은 RDD를 기반으로 만들어졌다. \n\nDataFrame 연산도 RDD를 기반으로 만들어졌으며 편리하고 효율적인 분산 처리를 위해 저수준 명령으로 컴파일. \n\n예제) 간단한 숫자를 이용해 병렬화해 RDD 생성 후 다른 DataFrame과 함께 사용할 수 있도록 DataFrame으로 변환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"850c9e63-1fa7-4afb-bcde-bf6e0d50e69b"}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nspark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21a6302e-5994-490f-aed7-c08410ed0723"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[39]: DataFrame[_1: bigint]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: DataFrame[_1: bigint]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3.6 SparkR\n\nSparkR은 스파크를 R 언어로 사용하기 위한 기능으로 스파크가 지원하는 모든 언어에 적용된 원칙을 동일하게 따른다. \n\nSparkR을 사용하기 위해서는 SparkR을 설치하고 코드를 실행하면 된다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f16d861-efbb-49b1-a279-8fb150d12eb2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":368035351193226}},"nbformat":4,"nbformat_minor":0}
