{"cells":[{"cell_type":"markdown","source":["# Chapter6. 다양한 데이터 타입 다루기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c163cc6-cd6d-4293-8fbe-25096182337a"}}},{"cell_type":"markdown","source":["## 6.1 API는 어디서 찾을까"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daacfbb4-f972-4a51-9519-b0c31e19dab4"}}},{"cell_type":"markdown","source":["**분석에 사용할 DataFrame 생성**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6925612a-9f67-48d3-b3f3-0401dbc3312d"}}},{"cell_type":"code","source":["file_location = \"/FileStore/tables/data/retail-data/by-day/2010_12_01.csv\"\nfile_type = \"csv\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", \"true\") \\\n  .option(\"header\", \"true\") \\\n  .load(file_location)\n\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4f11d29-afa3-4c55-babf-46b0c824e47b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n|   536367|    22745|POPPY&#39;S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22748|POPPY&#39;S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- InvoiceNo: string (nullable = true)\n-- StockCode: string (nullable = true)\n-- Description: string (nullable = true)\n-- Quantity: integer (nullable = true)\n-- InvoiceDate: string (nullable = true)\n-- UnitPrice: double (nullable = true)\n-- CustomerID: double (nullable = true)\n-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nInvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n   536367|    22745|POPPY&#39;S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n   536367|    22748|POPPY&#39;S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 6.2 스파크 데이터 타입으로 변환하기\n\n데이터 타입 변환은 **lit 함수**를 사용한다. lit 함수는 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ad09663-7bf7-4b47-90ef-861b0eb498f6"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n\ndf.select(lit(5), lit(\"five\"), lit(5.0))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"212db4f5-f497-4d67-80ee-29254d40a1c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.3 불리언 데이터 타입 다루기\n\n불리언은 모든 필터링 작업의 기반이므로 데이터 분석에 필수적. \n\n불리언 구문은 and, or, true, false로 구성된다. \n\n불리언 구문을 사용해 true 또는 false로 평가되는 논리 문법을 만든다. 논리 문법은 데이터 로우를 필터링할 때 필요조건의 일치와 불일치를 판별하는데 사용."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b837f5-fc0e-474b-b4c4-19a4ea5f3acd"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf.where(col(\"InvoiceNo\") != 536365)\\\n  .select(\"InvoiceNo\", \"Description\")\\\n  .show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4708e68-9710-4f3f-acbb-5f0c003bf8df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**문자열 표현식에 조건절 명시 - \"일치하지 않음\" 표현**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ffc9e1e-7ea9-4334-90af-09a8258b809c"}}},{"cell_type":"code","source":["df.where(\"InvoiceNo = 536365\")\\\n  .show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22f8353e-9ae9-4a1e-8319-3176e9762691"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.where(\"InvoiceNo <> 536365\")\\\n  .show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f627d6de-3a70-47ce-a42b-29fb0af035d9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**and 메서드나 or 메서드 사용**\n\n불리언 표현식을 사용하는 경우 항상 모든 표현식을 and 메서드로 묶어 차례대로 필터를 적용해야 한다. \n\n차례대로 필터를 적용해야 하는 이유 : 불리언 문을 차례대로 표현해도 스파크는 내부적으로 and 구문을 필터 사이에 추가해 모든 필터를 하나의 문장으로 변환하고, 동시에 모든 필터를 처리한다. 원한다면 and 구문으로 조건문을 만들 수도 있지만 차례로 조건을 나열하면 이해하기 쉽고 읽기가 편해진다. 반면 or 구문을 사용할 때는 반드시 동일한 구문에 조건을 정의해야 한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5284782c-03d4-486f-89fe-1cc3aa6baac8"}}},{"cell_type":"code","source":["from pyspark.sql.functions import instr\n\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(df.Description, \"POSTAGE\") >=1 \ndf.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aba422ce-5561-4e04-9473-0ec7f2984766"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**불리언 컬럼을 사용해 DataFrame 필터링**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f7a6705-abc3-40fa-bd4f-1f4fea5324b9"}}},{"cell_type":"code","source":["from pyspark.sql.functions import instr\n\nDOTCodeFilter = col(\"StockCode\") == \"DOT\"\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n\ndf.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n  .where(\"isExpensive\")\\\n  .select(\"unitPrice\", \"isExpensive\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c8a4e61-e0b3-4e3f-93d9-81c85428c443"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["필터를 반드시 표현식으로 정의할 필요는 없다. 별도의 작업 없이 컬럼명을 사용해 필터를 정의하는 것도 가능하다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"809e48a2-dee4-4779-9f1d-26c6386307d7"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n  .where(\"isExpensive\")\\\n  .select(\"Description\", \"UnitPrice\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2139ab9a-e34b-479f-a733-94cbb2711b61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.4 수치형 데이터 타입 다루기 \n\n카운트(count)는 빅데이터 처리에서 필터링 다음으로 많이 수행하는 작업이다. 대부분은 수치형 데이터 타입을 사용해 연산 방식을 정의하면 된다. \n\n* pow 함수 : 표시된 지수만큼 컬럼명 값을 거듭제곱한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5629d70-32eb-400c-87ae-74d755e2fbd7"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr, pow\n\nfabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"894db48e-092a-4e8c-81e2-cf1e253c3327"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["소수점 자리를 없애기 위해 Integer 데이터 타입으로 형변환하기도 하지만 반올림을 주로 사용함. \n* round 함수 : 소수점 값이 정확히 중간값 이상이라면 반올림\n* bround 함수 : 내림"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"524b76a7-1eea-4552-bf29-da0e8fd5179e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, round, bround\n\ndf.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6285f07-57d4-4623-8190-4201c8d4e604"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["두 컬럼 사이의 상관관계를 계산하는 것도 수치형 연산 작업 중 하나. \n\nDataFrame의 통계용 함수나 메서드를 사용해 피어슨 상관관계 계산 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f8231cd-1155-4d6d-8011-7af2b74aa5d0"}}},{"cell_type":"code","source":["from pyspark.sql.functions import corr\n\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799350ee-01d5-4bd3-8056-24b0d315b98c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* describe 메서드 : 관련 컬럼에 대한 집계(count), 평균(mean), 표준편차(stddev), 최솟값(min), 최댓값(max)를 계산\n\n단, 통계 스키마는 변경될 수 있으므로 describe 메서드는 콘솔 확인용으로만 사용."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75eca7c7-c49e-4193-ab18-3601ac8294a5"}}},{"cell_type":"code","source":["df.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"422bf268-0451-4e8e-8354-9005a88c820b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["정확한 수치가 필요하면 함수를 임포트하고 해당 컬럼에 적용하는 방식으로 직접 집계 수행 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c28be7c7-8d26-4cb9-acad-6c50d51b6a4f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import count, mean, stddev_pop, min, max"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d83308f6-1609-4419-bb98-f13be638ecf0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**StatFunctions 패키지** : 다양한 통계함수 제공 (stat 속성을 사용해 접근)\n\n* approxQuantile 메서드 : 데이터의 백분위수를 정확하게 계산하거나 근사치를 계산할 있음"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"678943dd-132e-41e7-8300-486b610f4a2f"}}},{"cell_type":"code","source":["olName = \"UnitPrice\"\nquantileProbs = [0.5]\nrelError = 0.05\n\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d062e0e-5f3d-458c-a202-479dc995ea52"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* freqItems 메서드 : 교차표나 자주 사용하는 항목 쌍을 확인하는 용도의 메서드\n\n단, 연산 결과가 너무 크면 화면에 모두 보이지 않을 수 있음."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b64bf431-d3d0-440e-bc18-9f1bda5df890"}}},{"cell_type":"code","source":["df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c309c15-edbe-4854-8a6a-68d7634a902c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* monotonically_increasing_id 함수 : 모든 로우에 고유 ID 값을 추가. (0부터 시작)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10c27c7d-75b8-4766-816b-b851d0bb71b4"}}},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id\n\ndf.select(monotonically_increasing_id()).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"911ddde5-9584-4165-997d-3cf322044828"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.5 문자열 데이터 타입 다루기\n\n문자열을 다루는 작업은 거의 모든 데이터 처리 과정에서 발생. 로그 파일에 정규 표현식을 사용해 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등의 작업 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82456d3c-b92a-4747-b996-a0a359ccdb31"}}},{"cell_type":"markdown","source":["* initcap 함수 : 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b32ca3e1-56db-4d28-9a7a-425d72889d0f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import initcap\n\ndf.select(initcap(col(\"Description\"))).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56aef82f-7704-45fa-aa77-0895201706ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* lower 함수 : 문자열 전체를 소문자로 변경\n* upper 함수 : 문자열 전체를 대문자로 변경"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e828ada2-ea68-4097-80d6-0508ed7e8f92"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper\n\ndf.select(col(\"Description\"),\n   lower(col(\"Description\")),\n   upper(lower(col(\"Description\")))).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91f767d2-f5c9-409c-be31-b4bd57cc2e1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* lpad, ltrim, rpad, rtrim, trim 함수 : 문자열 주변의 공백을 제거하거나 추가하는 작업"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0892e08-f20c-43be-8e27-60befe38bdb0"}}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, ltrim, rtrim, lpad, rpad, trim\n\ndf.select(\n  ltrim(lit(\"     HELLO     \")).alias(\"ltrim\"),\n  rtrim(lit(\"     HELLO     \")).alias(\"rtrim\"),\n  trim(lit(\"     HELLO     \")).alias(\"trim\"),\n  lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n  rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3d731ae-8414-4eda-9adf-a05d332588bc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.5.1 정규 표현식\n\n**정규 표현식** 이란?\n\n문자열의 존재 여부를 확인하거나 일치하는 모든 문자열을 치환할 때 사용. 정규 표현식을 사용해 문자열에서 값을 추출하거나 다른 값으로 치환하는데 필요한 규칙 모음 정의 가능.\n\n스파크에서는 regexp_extract 함수와 regexp_replace 함수를 이용해 값을 추출하고 치환하는 역할을 수행한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a83d0eb-b2c5-4e5b-b208-c9d049aff7e1"}}},{"cell_type":"code","source":["# regexp_replace 함수를 사용해 description 컬럼의 값을 'COLOR'로 치환\n\nfrom pyspark.sql.functions import regexp_replace\n\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n\ndf.select(\n  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n  col(\"Description\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a45d1a33-7844-44ea-b03c-71e56efad603"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* translate 함수 : 교체 문자열에서 색인된 문자에 해당하는 모든 문자를 치환.\n\n아래의 예제에서는 L=1, E=3, T=7로 치환."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52d895be-9ec1-4288-abbd-06a457ea3cb5"}}},{"cell_type":"code","source":["from pyspark.sql.functions import translate\n\ndf.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\"))\\\n  .show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02952b6c-a489-4691-a85b-16f0238e7786"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 처음 나타난 색상 이름을 추출하는 것과 같은 작업 수행 가능\n\nfrom pyspark.sql.functions import regexp_extract\n\nextract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n\ndf.select(\n  regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n  col(\"Description\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f840393a-98eb-4ab4-9764-11ffe0cd0c16"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* contains 메서드 : 값 추출 없이 단순히 값의 존재 여부를 확인하고자 할 때, 인수로 입력된 값이 컬럼의 문자열에 존재하는지 불리언 타입으로 반환\n\n단, 파이썬과 SQL에서는 **instr 함수**를 사용해 값의 존재 여부 확인"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63b1093e-9015-4458-bbc9-59b51c2e5a2a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import instr\n\ncontainsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\ncontainsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n\ndf.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n  .where(\"hasSimpleColor\")\\\n  .select(\"Description\").show(3, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f37071e-9ba7-46a5-9a29-22f099fe7736"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["동적으로 인수의 개수가 변하는 상황?\n\n(Scalar)\n\n* varargs : 값 목록을 인수로 변환해 함수에 전달할 때 사용하는 스칼라 고유 기능. \n\nvarargs 기능을 사용하면 임의 길이의 배열을 효율적으로 다룰 수 있다. \n\nex) select 메서드와 varargs를 함께 사용하면 원하는 만큼 동적으로 컬럼을 생성할 수 있다.\n\n\n(Python) \n\n\n* locate 함수 : 문자열의 위치(1부터 시작)를 정수로 반환\n\n그 다음 위치 정보를 불리언 타입으로 변환. \n\nex) locate 함수를 확장해 입력 값의 최소공배수를 구하거나 소수 여부 판별 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eec9ddb-cff3-4fc9-82e2-22a5a10e98b8"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr, locate\n\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n\ndef color_locator(column, color_string):\n  return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\"))\n\ndf.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n  .select(\"Description\").show(3, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d39d12e-86cf-4eb3-bea5-127be0684a95"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.6 날짜와 타임스탬프 데이터 다루기 \n\n* 스파크는 두 가지 종류의 시간 관련 정보만 관리하는데 하나는 **달력 형태의 날짜(date)**, 다른 하나는 날짜와 시간 정보를 모두 가지는 **타임스탬프(timestamp)**\n\n* 스파크의 inferSchema 옵션이 활성화된 경우 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 시도함. \n* 스파크는 특정 날짜 포맷을 명시하지 않아도 자체적으로 식별해 데이터를 읽을 수 있음.\n\n*TimestampType 클래스는 초 단위의 정밀도까지만 지원하므로 밀리세컨드나 마이크로세컨드 단위를 다루기 위해서는 Long 데이터 타입으로 데이터를 변환해 처리. 그 이상의 정밀도는 TimestampType으로 변환될 때 제거된다.*\n\n(스파크는 자바의 날짜와 타임스탬프를 사용해 표준체계를 따른다.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7d0e9a1-8ea2-4a94-9fe0-7fb8277ce944"}}},{"cell_type":"code","source":["# 오늘 날짜와 현재 타임스탬프 값 구하기\n\nfrom pyspark.sql.functions import current_date, current_timestamp\n\ndateDF = spark.range(10)\\\n  .withColumn(\"today\", current_date())\\\n  .withColumn(\"now\", current_timestamp())\n\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"534c54ef-7642-4f81-bb4d-fa41a4438b74"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 오늘을 기준으로 5일 전후의 날짜 구하기\n# date_add 함수와 date_sub 함수는 컬럼과 더하거나 뺄 날짜의 수를 인수로 전달해야 함.\n\nfrom pyspark.sql.functions import date_add, date_sub\n\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e26d72cf-0d14-4b42-9518-ac3eef6c4cbc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* datediff 함수 : 두 날짜 사이의 일 수를 반환\n* months_between 함수 : 두 날짜 사이의 개월 수를 반환\n* to_date 함수 : 문자열을 날짜로 변환할 수 있으며, 필요에 따라 날짜 포맷도 함께 지정 가능. 단, 날짜 포맷은 반드시 자바의 SimpleDateFormat 클래스가 지원하는 포맷 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2b36578-c3bf-4e3f-83bf-ceef1707d886"}}},{"cell_type":"code","source":["from pyspark.sql.functions import datediff, months_between, to_date\n\ndateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n\ndateDF.select(\n  to_date(lit(\"2016-01-01\")).alias(\"start\"),\n  to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61af785e-9c19-45e5-9302-536fe616e182"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, lit\n\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n  .select(to_date(col(\"date\"))).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f59605bc-e894-4846-b3f3-ec5b2b01faa2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["스파크는 날짜를 파싱할 수 없다면 에러 대신 null 값을 반환함. 따라서 다단계 처리 파이프라인에서는 조금 까다로울 수 있다. 데이터 포맷이 지정된 데이터에서 또 다른 포맷의 데이터가 나타날 수 있기 때문. \n\nex) 년-월-일 형태가 아닌 년-일-월 형태의 날짜 포맷 사용시 날짜를 파싱할 수 없으므로 null 값을 반환함."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f0fc372-af48-4fc8-86a2-b202720053f6"}}},{"cell_type":"code","source":["dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1222f50c-0074-42fa-9063-7bcd0b65c9d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["자바의 SimpleDateFormat 표준에 맞춰 날짜 포맷 지정\n\n* to_date 함수 : 필요에 따라 날짜 포맷 지정\n* to_timestamp 함수 : 반드시 날짜 포맷 지정"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb9ecb50-2744-4d96-b2bd-fb2755abce9d"}}},{"cell_type":"code","source":["# to_date 예제\n\nfrom pyspark.sql.functions import to_date\n\ndateFormat = \"yyyy-dd-MM\"\n\ncleanDateDF = spark.range(1).select(\n  to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n  to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ab4ec32-cff4-4600-88d7-dfc91ed2aeef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# to_timestamp 예제\n\nfrom pyspark.sql.functions import to_timestamp\n\ncleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07dc310c-7357-41aa-bfd1-25cc84f159ed"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["날짜를 비교할 때는 날짜나 타임스탬프 타입을 사용하거나 yyyy-MM-dd 포맷에 맞는 문자열 지정"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cbe7662-2482-41d4-84b2-b5df46f01c6e"}}},{"cell_type":"code","source":["cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aafed83c-0758-4e26-bd56-c298abb6ca29"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.7 null 값 다루기\n\nDataFrame에서 빠져 있거나 비어 있는 데이터를 표현할 때는 null 값을 사용하는 것이 빈 문자열이나 대체 값을 사용하는 것보다 좋다. - 최적화를 수행할 수 있기 때문.\n\nDataFrame의 하위 패키지인 .na를 사용하는 것이 DataFrame에서 null 값을 다루는 기본 방식. \n\n<null 값을 다루는 방법>\n* null 값을 제거\n* 전역 또는 컬럼 단위로 null 값을 특정 값으로 채워 넣는 방법"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7d67079-0649-45c6-b756-89165cf7db51"}}},{"cell_type":"markdown","source":["### 6.7.1 coalesce\n\n* coalesce 함수 : 인수로 지정한 여러 컬럼 중 null이 아닌 첫 번째 값 반환 (모든 컬럼이 null이 아닌 값을 가진 경우 첫 번째 컬럼의 값 반환)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b256ebec-0016-40a9-936b-8af63d2a7f6d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import coalesce\n\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16cfb0cb-cb96-4e8c-b622-24d0cdb0f1cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.7.2 ifnull, nullif, nvl, nvl2\n\ncoalesce 함수와 유사한 결과를 얻을 수 있는 SQL 함수\n* ifnull 함수 : 첫 번째 값이 null이면 두 번째 값 반환 (첫 번째 값이 null이 아니면 첫 번째 값 반환)\n* nullif 함수 : 두 값이 같으면 null 반환 (두 값이 다르면 첫 번째 값 반환)\n* nvl 함수 : 첫 번째 값이 null이면 두 번째 값 반환 (첫 번째 값이 null 이 아니면 첫 번째 값 반환)\n* nlv2 함수 : 첫 번째 값이 null이 아니면 두 번째 값 반환 (첫 번째 값이 null이면 세 번째 인수로 지정된 값 반환)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f15a0d3c-7a4b-4143-a855-1babdcee7cda"}}},{"cell_type":"markdown","source":["### 6.7.3 drop\n\ndrop 메소드 : null 값을 가진 로우 제거. (null 값을 가진 모든 로우 제거)\n\n\n* 단, SQL을 사용한다면 컬럼별로 수행\n\n-- SQL\nSELECT * \nFROM dfTable \nWHERE Description IS NOT NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1b773fc-67a6-4405-b87a-b690ba231fe3"}}},{"cell_type":"code","source":["df.na.drop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"476afe25-4055-49a4-b402-5dc213b245b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.na.drop(\"any\") # 인수로 any를 지정한 경우 로우의 컬럼 값 중 하나라도 null 값을 가지면 해당 로우 제거 "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4080da8f-e93b-41dd-8255-5d6523312730"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.na.drop(\"all\") # 인수를 all로 지정한 경우 모든 컬럼의 값이 null이거나 NaN인 경우에만 해당 로우 제거"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"215959be-e793-4997-999f-6cf86945777f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop 메서드에 배열 형태의 컬럼을 인수로 적용 가능\ndf.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90bf1bcf-13c0-4c16-8ea5-96c649ccfee8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.7.4 fill\n\nfill 함수 : 하나 이상의 컬럼을 특정 값으로 채우는 함수 - 채워 넣을 값과 컬럼 집합으로 구성된 맵을 인수로 사용\n\nex) String 데이터 타입의 컬럼에 존재하는 null 값을 다른 값으로 채우기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8d04dd5-f733-4441-83ef-5ac9c2340f2c"}}},{"cell_type":"code","source":["df.na.fill(\"All Null values become this string\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8c34cf5-e4b7-4117-89a1-d31b0e68be31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["* df.na.fill(5:Integer) : Integer 데이터 타입의 컬럼에 존재하는 null 값을 다른 값으로 채워넣기\n* df.na.fill(5:Double)  : Double 데이터 타입의 컬럼에 적용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"660ba840-9f24-477e-9428-cd855f0b624a"}}},{"cell_type":"code","source":["df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]) # 다수의 컬럼에 적용할 경우 적용하고자 하는 컬럼명을 배열로 만들어 인수로 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0698af10-2d5f-48a6-94aa-adad8a637632"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["스칼라 Map 타입을 사용해 다수의 컬럼에 fill 메서드 적용 가능\n\n* 키(key) : 컬럼명\n* 값(value) : null 값을 채우는데 사용할 값"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b53a37e-afda-4ff5-b1ba-b2b1178e865f"}}},{"cell_type":"code","source":["fill_cols_vals = {\"StockCode\" : 5, \"Description\" : \"No Value\"}\ndf.na.fill(fill_cols_vals)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ea97cb8-628c-4e54-b2bc-ede95fad65aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.7.5 replace\n\nreplace 메소드 : 조건에 따라 다른 값으로 대체하는 것으로, 변경하고자 하는 값과 원래 값의 데이터 타입이 같아야함."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05cca7ec-7cc4-468d-a9a1-09d057653ca6"}}},{"cell_type":"code","source":["df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"283e8749-3cdc-44b7-820f-a949e9fd516e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 6.8 정렬하기\n\nDataFrame을 정렬할 때 다음과 같은 함수로 null 값이 표시되는 기준 지정 가능\n\n* acc_nulls_first\n* desc_nulls_fisrt\n* acc_nulls_last\n* desc_nulls_last"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2c08216-995e-44d2-afda-341f5dcf0a96"}}},{"cell_type":"markdown","source":["## 6.9 복합 데이터 타입 다루기\n\n복합 데이터 타입을 사용하면 해결하려는 문제에 더욱 적합한 방식으로 데이터 구성 및 구조화 가능.\n\n< 복합 데이터 타입의 종류 >\n* 구조체(struct)\n* 배열(array)\n* 맵(map)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7a19e46-5c08-4322-997b-e71a4ba49525"}}},{"cell_type":"markdown","source":["### 6.9.1 구조체\n\n구조체 : DataFrame 내부의 DataFrame - 쿼리문에서 다수의 컬럼을 괄호로 묶어 구조체 생성 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20811805-71a0-49e1-8f8e-7619c0976258"}}},{"cell_type":"code","source":["from pyspark.sql.functions import struct\n\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d39e344-c1b3-4fdf-9992-c8781f3fb09c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["복합 데이터타입을 가진 DataFrame도 다른 DataFrame을 조회하는 것과 마찬가지로 사용이 가능하지만 점(.)을 사용하거나 getField 메소드를 사용해야 한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92d96fb6-3047-47df-ae0d-253a45b3b530"}}},{"cell_type":"code","source":["complexDF.select(\"complex.Description\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"294baa99-198e-47e0-8101-c0002506ccd2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["complexDF.select(col(\"complex\").getField(\"Description\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8fd0cf1-b158-4856-9f5f-5aa30bc0115e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["별표(*) 문자를 사용하면 모든 값을 조회하는 것도 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ee53a93-6d53-4e3e-a462-bac0768d0b58"}}},{"cell_type":"code","source":["complexDF.select(\"complex.*\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cebbe2d-22b0-467c-ae55-43dcfb0588f3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.9.2 배열\n\n배열을 정의하기 위해 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환과정 필요. \n\n**split**\n\nsplit 함수에 구분자를 인수로 전달해 인수에 따라 배열로 변환."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70131700-b8cb-437a-ba66-7cea6a305f36"}}},{"cell_type":"code","source":["from pyspark.sql.functions import split\n\ndf.select(split(col(\"Description\"), \" \")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b532458-8999-441d-af18-b9197e5a822f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["split 함수는 스파크에서 복합 데이터 타입을 다른 컬럼처럼 다룰 수 있는 기능을 가지기 때문에 파이썬과 유사한 문법으로 배열 값 조회 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df525628-7967-467e-ad32-07e9d0749a2c"}}},{"cell_type":"code","source":["df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n  .selectExpr(\"array_col[0]\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"892dc275-6e3a-49bf-8329-df7b8a160485"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**배열의 길이** - 배열의 크기 조회"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36936911-b7b3-4cfd-a632-60f5b25d4e2f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import size\n\ndf.select(size(split(col(\"Description\"), \" \"))).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb0fec98-c072-4d95-966b-d766493da6ad"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**array_contains**\n\narray_contains 함수를 사용해 배열에 특정 값이 존재하는지 여부 확인 가능 (true / false 반환)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"620d452e-de5f-4556-910b-d128ad02aa0e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import array_contains\n\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0259463c-deb1-45cd-b90e-379d0e0881b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**explode**\n\nexplode 함수는 배열 타입의 컬럼을 입력 받아 입력된 컬럼의 배열값에 포함된 모든 값을 로우로 변환. 나머지 컬럼 값은 중복되어 표시.\n\n< 텍스트로 이루어진 explode 함수 처리 과정 >\n\nex) \"Hello World\"와 \"other col\"의 컬럼 값이 존재할 때 \"Hello World\"에 적용\n\n* split 함수 적용 -> [\"Hello\", \"World\"], \"other col\"\n* explode 함수 적용 \n  -> \"Hello\", \"other col\"\n     \"World\", \"other col\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fe7b74e-c407-4746-aa47-96152d4b9de3"}}},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode\n\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffe91aff-699e-4500-8be2-d7958535b818"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 6.9.3 맵\n\n맵 : map 함수와 컬럼의 키-값 쌍을 이용해 생성하고, 배열과 동일한 방법으로 값 선택 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdf938e9-3983-4a8b-b46f-529a0e50e9aa"}}},{"cell_type":"code","source":["from pyspark.sql.functions import create_map, col\n\ndf.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"497f8413-ad1b-469b-a63e-a2ee72a1137d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+\n|         complex_map|\n+--------------------+\n|{WHITE HANGING HE...|\n|{WHITE METAL LANT...|\n+--------------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n         complex_map|\n+--------------------+\n{WHITE HANGING HE...|\n{WHITE METAL LANT...|\n+--------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["적합한 키를 이용해 데이터 조회 가능 (해당 키가 존재하지 않으면 null 값 반환)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63091c90-eaf4-40ea-a86d-9504e8cc0d12"}}},{"cell_type":"code","source":["df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"555c5289-2371-4291-aaaf-3d6e67cb2a6a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------------------+\n|complex_map[WHITE METAL LANTERN]|\n+--------------------------------+\n|                            null|\n|                          536365|\n+--------------------------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+\ncomplex_map[WHITE METAL LANTERN]|\n+--------------------------------+\n                            null|\n                          536365|\n+--------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["맵은 분해해 컬럼으로 변환 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e4edb4a-a47e-4339-b2c4-0780ad880789"}}},{"cell_type":"code","source":["df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .selectExpr(\"explode(complex_map)\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92a575da-89c1-459d-8b63-cde768b190d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+------+\n|                 key| value|\n+--------------------+------+\n|WHITE HANGING HEA...|536365|\n| WHITE METAL LANTERN|536365|\n+--------------------+------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+\n                 key| value|\n+--------------------+------+\nWHITE HANGING HEA...|536365|\n WHITE METAL LANTERN|536365|\n+--------------------+------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 6.10 JSON 다루기\n\n스파크는 JSON 데이터를 다루기 위한 기능 지원\n* 문자열 형태의 JSON을 직접 조회 가능\n* JSON 파싱 혹은 JSON 객체 생성 가능\n\nex) JSON 컬럼 생성"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9315ed6f-610f-4d21-8409-8de55ba71346"}}},{"cell_type":"code","source":["jsonDF = spark.range(1).selectExpr(\"\"\"\n  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3387b68b-c916-4dfb-bc12-24a1b77d39cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["* get_json_object 함수 : JSON 객체(딕셔너리 혹은 배열)를 인라인 쿼리로 조회 가능\n* json_tuple : 중첩이 없는 단일 수준의 JSON 객체의 경우"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b74ccfc-018f-4fd5-95f4-b9c9f508950b"}}},{"cell_type":"code","source":["from pyspark.sql.functions import get_json_object, json_tuple\n\njsonDF.select(\n  get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n  json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b7947d6-6486-4314-b69a-54bb320b16ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+--------------------+\n|column|                  c0|\n+------+--------------------+\n|     2|{&#34;myJSONValue&#34;:[1...|\n+------+--------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+--------------------+\ncolumn|                  c0|\n+------+--------------------+\n     2|{&#34;myJSONValue&#34;:[1...|\n+------+--------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["* to_json 함수 : StructType을 JSON 문자열로 변경"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"841bbb13-1679-4988-8239-f1933405cf1f"}}},{"cell_type":"code","source":["from pyspark.sql.functions import to_json\n\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n  .select(to_json(col(\"myStruct\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e7615d2-3859-4769-81fb-0cb9d972b26d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: DataFrame[to_json(myStruct): string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: DataFrame[to_json(myStruct): string]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["* to_json 함수 : JSON 데이터소스와 동일한 형태의 딕셔너리(맵)를 파라미터로 사용 가능. \n* from_json 함수 : JSON 문자열을 다시 객체로 변환. 단, 파라미터로 반드시 스키마 지정. (맵 데이터 타입의 옵션을 인수로 지정 가능)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3c5a90d-e31e-45c4-b0b5-367d897a620a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import from_json\nfrom pyspark.sql.types import *\n\nparseSchema = StructType((\n  StructField(\"InvoiceNo\", StringType(), True),\n  StructField(\"Description\", StringType(), True)))\n\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n  .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n  .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"840b19d2-9323-4706-a05e-bfa7d39927b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+--------------------+\n|  from_json(newJSON)|             newJSON|\n+--------------------+--------------------+\n|{536365, WHITE HA...|{&#34;InvoiceNo&#34;:&#34;536...|\n|{536365, WHITE ME...|{&#34;InvoiceNo&#34;:&#34;536...|\n+--------------------+--------------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+\n  from_json(newJSON)|             newJSON|\n+--------------------+--------------------+\n{536365, WHITE HA...|{&#34;InvoiceNo&#34;:&#34;536...|\n{536365, WHITE ME...|{&#34;InvoiceNo&#34;:&#34;536...|\n+--------------------+--------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 6.11 사용자 정의 함수\n\nUDF\n* 파이썬이나 스칼라 그리고 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게 함. \n* 하나 이상의 컬럼을 입력으로 받고, 반환 가능.\n* 레코드별로 데이터를 처리하는 함수이기 때문에 독특한 포맷이나 도메인에 특화된 언어를 사용하지 않음.\n* 특정 SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록.\n\nex) 숫자를 입력받아 세제곱 연산을 하는 power3 함수 생성 - 사용할 UDF를 만들기 위해 함수 필요"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bcf82b2-ec0a-4a89-86ba-5395cd90b93c"}}},{"cell_type":"code","source":["udfExampleDF = spark.range(5).toDF(\"num\")\n\ndef power3(double_value):\n  return double_value ** 3\n\npower3(2.0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"888eac97-c8f7-4daa-8d53-181b2b0b9542"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: 8.0</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: 8.0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["모든 워커 노드에서 생성된 함수를 사용할 수 있도록 스파크에 등록하는 과정 필요.\n\n< 파이썬 UDF 처리 과정 >\n1. 함수 직렬화 후 워커에 전달\n2. 스파크에서 파이썬 프로세스 실행 후 데이터 전송\n3. 파이썬에서 처리 결과 반환\n\nex) DataFrame에서 사용할 수 있도록 함수 등록"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d442d3ed-aa93-43ff-b058-9e986084b2a2"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\npower3udf = udf(power3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d151a27d-5b80-4e75-aacc-e05715e0ae3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["사용자 정의 함수 등록 후 DataFrame에서 사용 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3168f7e-9dcd-46e7-8bd4-1ce52359398e"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nudfExampleDF.select(power3udf(col(\"num\"))).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adc7cd39-8af3-451a-aedc-e5630b88a47b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+\n|power3(num)|\n+-----------+\n|          0|\n|          1|\n+-----------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\npower3(num)|\n+-----------+\n          0|\n          1|\n+-----------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["사용자 정의 함수를 DataFrame에서만 사용이 가능하고 문자열 표현식에서는 사용 불가능.\n\n하지만 사용자 정의 함수를 스파크 SQL 함수로 등록하면 모든 프로그래밍 언어와 SQL에서 사용자 정의 함수 사용 가능.\n\n스파크는 파이썬의 데이터 타입과 다른 자체 데이터 타입을 사용하므로 함수를 정의할 때 반환 타입을 지정하는 것이 좋음. \n(만약 함수에서 반환될 실제 데이터 타입과 일치하지 않는 데이터 타입 지정시 오류가 아닌 null 값 반환)\n\nex) 함수의 반환 데이터 타입을 DoubleType으로 변경하면?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a5d25c2-285c-4a88-b508-d07ee1e0afca"}}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType, DoubleType\n\nspark.udf.register(\"power3py\", power3, DoubleType())\n\nudfExampleDF.selectExpr(\"power3py(num)\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84505266-3f12-42c5-a09f-4660d2c29afd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------------+\n|power3py(num)|\n+-------------+\n|         null|\n|         null|\n+-------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+\npower3py(num)|\n+-------------+\n         null|\n         null|\n+-------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["null 값을 반환하는 이유 : range 메서드가 Integer 데이터 타입의 데이터를 만들기 때문. (파이썬에서 Integer 데이터 타입을 사용해 연산했다면 Float 데이터 타입(스파크의 Double 데이터 타입)으로 변환할 수 없음)\n\n따라서 파이썬 함수가 Integer 데이터 타입 대신 Float 데이터 타입을 반환하도록 수정하면 null 값을 반환하지 않게 된다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2cdeb68-f3a1-4523-a6e3-86dd4c91871d"}}},{"cell_type":"markdown","source":["## 6.12 Hive UDF\n\n하이브 문법을 이용해 만든 UDF/UDAF도 사용이 가능. \n\n단, SparkSession을 생성할 때 SparkSession.builder().enableHiveSupport()를 명시해 하이브 지원 기능 활성화.\n\n하이브 지원 기능이 활성화되면 SQL로 UPF 등록 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43ae5ac6-b880-4da5-9129-ffee29e35eb6"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter6","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":8832519310326}},"nbformat":4,"nbformat_minor":0}
