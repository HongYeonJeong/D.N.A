{"cells":[{"cell_type":"markdown","source":["# Chapter 9. 데이터소스"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd40a67c-3f36-4477-a25f-abc1e2d7ad9e"}}},{"cell_type":"markdown","source":["## 9.1 데이터소스 API의 구조"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99c878a3-7b79-4d43-b9ea-cc382a1d2a61"}}},{"cell_type":"markdown","source":["### 9.1.1 읽기 API의 구조\n\n[ 데이터 읽기의 핵심 구조 ]\n\nDataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n\n데이터소스를 읽을 때는 위와 같은 형식을 사용. \n\n* format 메서드는 선택적으로 사용할 수 있으며 기본값은 파케이 포맷\n* option 메서드를 사용해 데이터를 읽는 방법에 대한 파라미터를 키-값 쌍으로 설정 가능\n* schema 메서드는 데이터 소스에서 스키마를 제공하거나 스키마 추론 기능을 사용하려는 경웨 선택적으로 사용 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e43d5c2-d823-417d-beb2-db0c0d9bfb3a"}}},{"cell_type":"markdown","source":["### 9.1.2 데이터 읽기의 기초\n\n스파크에서 데이터를 읽을 때는 기본적으로 DataFrameReader 사용. (DataFrameReader는 SparkSession의 read 속성으로 접근)\n\nspark.read\n\n[ DataFrameReader를 얻은 다음에는 다음과 같은 값 지정 ]\n* 포맷\n* 스키마\n* 읽기 모드\n* 옵션\n\n포맷, 스키마, 옵션은 트랜스포메이션을 추가로 정의할 수 있는 DataFrameReader를 반환. \n\n읽기 모드를 제외한 세 가지 항목은 필요한 경우에만 선택적으로 지정 가능.\n\n데이터소스마다 데이터를 읽은 방식을 결정할 수 있는 옵션 제공.\n\nDataFrameReader에 반드시 데이터를 읽을 경로 지정.\n\n[ 전반적인 코드 구성 ]\n\nspark.read.format(\"csv\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"path\", \"path/to.file(s)\")\\\n  .schema(someSchema)\\\n  .load()\n  \n  \n**읽기 모드**\n\n외부 데이터소스에서 데이터를 읽다 보면 자연스럽게 형식에 맞지 않는 데이터를 만나게 되는데, 특히 반정형 데이터소스를 다룰 때 많이 발생. \n\n읽기 모드는 스파크가 형식에 맞지 않는 데이터를 만났을 때 동작 방식을 지정하는 옵션\n\n[ 스파크의 읽기 모드 ]\n* permissive : 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 _corrupt_record라는 문자열 컬럼에 기록\n* dropMalformed : 형식에 맞지 않는 레코드가 포함된 로우 제거\n* failFast : 형식에 맞지 않는 레코드를 만나면 즉시 종료\n\n단, 읽기 모드의 기본값은 permissive"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"293a9bad-3c07-472b-83ba-281943f2fc27"}}},{"cell_type":"markdown","source":["### 9.1.3 쓰기 API 구조\n\n[ 데이터 쓰기의 핵심 구조 ]\nDataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n\n* format 메서드는 선택적으로 사용할 수 있으며 기본값은 파케이 포맷\n* option 메서드를 사용해 데이터의 쓰기 방법 설정 가능\n* partitionBy, bucketBy, sortBy 메서드는 파일 기반의 데이터소스에서만 동작하며 이 기능으로 최종 파일 배치 형태를 제어 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57f1a67d-66d4-45bd-91c5-3143b8fda3e4"}}},{"cell_type":"markdown","source":["### 9.1.4 데이터 쓰기의 기초\n\n데이터 쓰기는 데이터 읽기와 매우 유사하며 DataFrameReader 대신 DataFrrameWriter를 사용. \n\n데이터소스에 항상 데이터를 기록해야 하기 때문에 DataFrame의 write 속성을 이용해 DataFrame 별로 DataFrameWriter에 접근 해야 함.\n\n**저장 모드**\n\n저장 모드는 스파크가 지정된 위치에서 동일한 파일이 발견했을 때의 동작 방식을 지정하는 옵션.\n\n[ 스파크의 저장 모드 ]\n\n* append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일 추가\n* overwrite : 이미 존재하는 모든 데이터를 완전히 덮어씀\n* errorIfExists : 해당 경로에 데이터나 파일이 존재하는 경우 올유를 발생시키면서 쓰기 작업이 실패\n* ignore : 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않음\n\n단, 기본값은 errorIfExists 즉 스파크가 파일을 저장할 경로에 데이터나 파일이 이미 존재하면 쓰기 작업은 즉시 실패."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5d31e87-a393-425f-adb9-9c7802a44935"}}},{"cell_type":"markdown","source":["## 9.2 CSV 파일\n\nCSV 파일은 콤마(,)로 구분된 값을 의미함. \n\nCSV는 각 줄이 단일 레코드가 되며 레코드의 각 필드를 콤마로 구분하는 일반적인 텍스트 파일 포맷\n\nCSV 파일은 구조적으로 보이지만 매우 까다로운 파일 포맷 중 하나 - 운영 환경에서는 어떤 내용이 들어 있는지, 어떠한 구조로 되어 있는지 등 다양한 전제를 만들어낼 수 없기 때문.\n\n때문에 CSV reader는 많은 수의 옵션 제공. \n\n(ex. CSV 파일 내 컬럼 내용에 콤마가 들어 있거나 비표준적인 방식으로 null 값이 기록된 경우 특정 문자를 이스케이프 처리하는 옵션을 사용해 문제 해결 가능)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b1a0a6e-aa5d-470b-b1fd-d0f8e4dd9d4e"}}},{"cell_type":"markdown","source":["### 9.2.1 CSV 옵션\n\n* sep : 각 필드와 값을 구분하는데 사용되는 단일 문자 (기본값 : ,)\n* header : 첫 번째 줄이 컬럼명인지 나타내는 불리언 값 (기본값 : false)\n* escape : 스파크가 파일에서 이스케이프 처리할 문자 (기본값 : \\)\n* inferSchema : 스파크가 파일을 읽을 때 컬럼의 데이터 타입을 추론할지 정의 (기본값 : false)\n* ignoreLeadingWhiteSpace : 값을 읽을 때 값의 선행 공백을 무시할지 정의 (기본값 : false)\n* ignoreTrailingWhiteSpace : 값을 읽을 때 값의 후행 공백을 무시할지 정의 (기본값 : false)\n* nullValue : 파일에서 null 값을 나타내는 문자 (기본값 : \"\")\n* nanValue : CSV 파일에서 NaN이나 값 없음을 나타내는 문자 선언 (기본값 : NaN)\n* positiveInf : 양의 무한 값을 나타내는 문자(열)를 선언 (기본값 : Inf)\n* negativeInf : 음의 무한 값을 나타내는 문자(열)를 선언 (기본값 : -Inf)\n* compression 또는 codec : 스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱 정의 (기본값 : none)\n* dataFormat : 날짜 데이터 타입인 모든 필드에서 사용할 날짜 형식 (기본값 : yyyy-MM-dd)\n* timestampFormat : 타임스탬프 데이터 타입인 모든 필드에서 사용할 날짜 형식 (기본값 : yyyy-MM-dd'T'HH:mm:ss.SSSZZ)\n* maxColumns : 파일을 구성하는 최대 컬럼 수를 선언 (기본값 : 20480)\n* maxCharsPerColumn : 컬럼의 문자 최대 길이를 선언 (기본값 : 1000000)\n* escapeQuotes : 스파크가 파일의 라인에 포함된 인용부호를 이스케이프할지 선언 (기본값 : true)\n* maxMalformedLogPerPartition : 스파크가 각 파티션별로 비정상적인 레코드를 발견했을 때 기록할 최대 수. 이 숫자를 초과하는 비정상적인 레코드는 무시된 (기본값 : 10)\n* quoteAll : 인용부호 문자가 있는 값을 이스케이프 처리하지 않고, 전체 값을 인용부호로 묶을지 여부 (기본값 : false)\n* multiLline : 하나의 논리적 레코드가 여러 줄로 이루어진 CSV 파일 읽기를 허용할지 여부 (기본값 : false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6085110d-01a1-47d2-b71a-4c6eb67d3c37"}}},{"cell_type":"markdown","source":["### 9.2.2 CSV 파일 읽기\n\ncsv 파일을 읽으려면 먼저 CSV용 DataFrameReader를 생성하고 스키마와 읽기 모드 지정.\n\n스파크는 지연 연산 특성이 있으므로 DataFrame 정의 시점이 아닌 잡 실행 시점에만 오류가 발생."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd1666ac-e067-4b8b-8cb5-811210373797"}}},{"cell_type":"markdown","source":["### 9.2.3 CSV 파일 쓰기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f220bb8-1966-4acf-b6ec-3aa1c1fbe635"}}},{"cell_type":"code","source":["csvFile = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/2010_summary-1.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7f19853-1081-458d-81a1-feae4fc19b25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["CSV 파일을 읽어 TSV 파일로 내보내는 처리"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9141b939-ab2d-4757-a904-a382c779f915"}}},{"cell_type":"code","source":["csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n  .save(\"/tmp/my-tsv-file.tsv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cde6729f-ee9a-4762-b951-51f63b98c42e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 9.3 JSON 파일 \n\n스파크에서는 JSON 파일을 사용할 때 줄로 구분된 JSON을 기본적으로 사용. \n\nmultiLine 옵션을 사용해 줄로 구분된 방식과 여러 줄로 구성된 방식을 선택적으로 사용 가능하다. 이 옵션을 true로 설정하면 전체 파일을 하나의 JSON 객체로 읽을 수 있다. \n\n스파크는 JSON 파일을 파싱한 다음에 DataFrame을 생성. \n\n줄로 구분된 JSON은 전체 파일을 읽어 들인 다음 저장하는 방식이 아니므로 새로운 레코드 추가 가능. \n\n줄로 구분된 JSON이 인기 있는 이유는 구조화되어 있고, 최소한의 기본 데이터 타입이 존재하기 때문."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40dab58a-3236-4e27-98be-b66592c5ef39"}}},{"cell_type":"markdown","source":["### 9.3.1 JSON 옵션\n\n* compression 또는 codec : 스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱 정의 (기본값 : none)\n* dateFormat : 날짜 데이터 타입인 모든 필드에서 사용할 날짜 형식을 정의 (기본값 : yyyy-MM-dd)\n* timestampFormat : 타임스탬프 데이터 타입인 모든 필드에서 사용할 날짜 형식을 정의 (기본값 : yyyy-MM-dd'T'HH:mm:ss.SSSZZ)\n* priomitiveAsString : 모든 프리미티브값을 문자열로 추정할지 정의 (기본값 : false)\n* allowComments : JSON 레코드에서 자바나 C++ 스타일로 된 코멘트를 무시할지 정의 (기본값 : false)\n* allowUnquotedFieldNames : 인용부호로 감싸여 있지 않은 JSON 필드명을 허용할지 정의 (기본값 : false)\n* allowSingleQuotes : 인용부호로 큰따옴표 대신 작은따옴표를 허용할지 정의 (기본값 : true)\n* allowNumericLeadingZeros : 숫자 앞에 0을 허용할지 정의 (기본값 : false)\n* allowBackslashEscapingAnyCharacter : 백슬래시 인용부호 매커니즘을 사용한 인용부호를 허용할지 정의 (기본값 : false)\n* columnNameOfCorruptRecord : permissive 모드에서 생성된 비정상 문자열을 가진 새로운 필드명을 변경 가능 (기본값 : spark.sql.columnNameOfCorruptRecord 속성의 설정값)\n* multiLine : 줄로 구분되지 않은 JSON 파일의 읽기를 허용할지 정의 (기본값 : false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3c2e39b-2211-4b66-9673-0be977944602"}}},{"cell_type":"markdown","source":["### 9.3.2 JSON 파일 읽기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bad99a7b-a9da-430f-ae57-b954265db2dc"}}},{"cell_type":"code","source":["spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/2010_summary-1.json\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866325a6-3a63-423a-b2ce-8f68b696a05f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|    1|\n    United States|            Ireland|  264|\n    United States|              India|   69|\n            Egypt|      United States|   24|\nEquatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 9.3.3 JSON 파일 쓰기\n\n파티션당 하나의 파일을 만들며 전체 DataFrame을 단일 폴더에 저장.\n\nJSON 객체는 한 줄에 하나씩 기록."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"177df3c9-fc4f-4d21-ac7b-42128f05f1d7"}}},{"cell_type":"code","source":["csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a46c289f-c8c6-4863-8b0d-31d0bdc68d2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 9.4 파케이 파일\n\n파케이는 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반의 데이터 저장 방식. \n\n저장 공간을 절약할 수 있고 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있으며 컬럼 기반의 압축 기능 제공."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"713365af-9d72-4dcc-b792-bd01372ae206"}}},{"cell_type":"markdown","source":["### 9.4.1 파케이 파일 읽기\n\n파케이는 옵션이 거의 없다. 데이터를 저장할 때 자체 스키마를 사용해 데이터를 저장하기 때문. \n\n**파케이 옵션**\n\n* compression 또는 codec : 스파크가 파일을 읽고 쓸 때 사용하는 압축 코덱 정의 (기본값 : none)\n* mergeSchema : 동일한 테이블이나 폴더에 신규 추가된 파케이 파일에 컬럼을 점진적으로 추가할 수 있음. 이러한 기능을 활성화하거나 비활성화하기 위해 옵션 사용 (기본값 : spark.sql.parquet.mergeSchema 속성의 설정값)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3027f13d-1fd2-4b52-bf24-5ec6cc4c7fa2"}}},{"cell_type":"markdown","source":["### 9.4.2 파케이 파일 쓰기\n\n파케이 파일 쓰기도 파일의 경로만 명시하면 가능.\n\n분할 규칙은 달든 포맷과 동일하게 적용."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"831a30b6-5ab8-4f54-900e-023c7cb25c07"}}},{"cell_type":"markdown","source":["## 9.5 ORC 파일\n\nORC는 하둡 워크로드를 위해 설계된 자기 기술적이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷. \n\n대규모 스트리밍 처리에 최적화되어 있을 뿐만 아니라 필요한 로울르 신속하게 찾아낼 수 있는 기능이 통합되어 있음. \n\n스파크는 ORC 파일 포맷을 효율적으로 사용할 수 있으므로 별도의 옵션 지정 없이 데이터를 읽을 수 있음."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6dc22f5-9679-4434-9d46-d5e867a17130"}}},{"cell_type":"markdown","source":["### 9.5.1 ORC 파일 읽기\n\nspark.read.foramt(\"orc\").load(\"/data/flight-data.orc/2010-summary.orc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45e3d4ea-ec74-4419-ab32-8a7bb4832613"}}},{"cell_type":"markdown","source":["### 9.5.2 ORC 파일 쓰기\n\ncsvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-orc-file.orc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ffa162f-9b85-4388-954a-1168e50ea089"}}},{"cell_type":"markdown","source":["## 9.6 SQL 데이터베이스\n\n사용자는 SQL을 지원하는 다양한 시스템에 SQL 데이터소스를 연결 가능. \n\n데이터베이스의 데이터를 읽고 쓰기 위해서는 스파크 클래스패스에 데이터베이스의 JDBC 드라이버를 추가하고 적절한 JDBC 드라이버 jar 파일을 제공해야 함. \n\nex. PostgreSQL 데이터베이스에 데이터를 읽거나 쓰기 위한 실행식\n\n./bin/spark-shell\\\n--driver-class-path postgresql-9.4.1207.jar\\\n--jars postgresql-9.4.1207.jar\n\n[ JDBC 데이터소스 옵션 ]\n* url : 접속을 위한 JDBC URL. 소스 시스템에 특화된 설정은 URL에 지정 가능. \n* dtable : 읽을 JDBC 테이블 설정\n* driver : 지정한 URL에 접속할 때 사용할 JDBC 드라이버 클래스명 지정\n* partitionColumn, lowerBound, upperBound : 세가지 옵션은 항상 같이 지정. numPartitions도 반드시 지정. partitionColumn은 반드시 해당 테이블의 수치형 컬럼. lowerBound와 upperBound는 테이블의 로우를 필터링하는 데 사용되는 것이 아니라 각 파티션의 범위를 결정하는데 사용. 따라서 테이블의 로우는 분할되어 반환 (읽기에만 적용)\n* numPartitions : 테이블의 데이터를 병렬로 읽거나 쓰기 작업에 사용할 수 있는 최대 파티션 수를 결정. 이 속성은 최대 동시 JDBC 연결 수를 결정. \n* fetchsize : 한 번에 얼마나 많은 로우를 가져올지 결정하는 JDBC의 패치 크기 설정. 이 옵션은 기본적으로 패치 크기가 작게 설정된 JDBC 드라이버의 성능을 올리는데 도움. (읽기에만 적용)\n* betchsize : 한 번에 얼마나 많은 로우를 저장할지 결정하는 JDBC의 배치 크기 설정. JDBC 드라이버의 성능을 향상시킬 수 있음. 쓰기에만 적용되며 기본값은 1000\n* isolationLevel : 현재 연결에 적용되는 트랜잭션 격리 수준 정의. \n* truncate : JDBC writer 관련 옵션\n* createTableOptions : JDBC writer 관련 옵션 - 테이블 생성 시 특정 테이블의 데이터베이스와 파티션 옵션 설정 가능\n* createTableColumnTypes : 테이블을 생성할 때 기본값 대신 사용할 데이터베이스 컬럼 데이터 타입을 정의"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2feb7bb-9233-4750-8332-06ab6f5a91f9"}}},{"cell_type":"markdown","source":["### 9.6.1 SQL 데이터베이스 읽기\n\n접속 관련 속성 정의."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"042b295e-c529-4c18-b22b-04d927162110"}}},{"cell_type":"code","source":["driver = \"org.sqlite.JDBC\"\npath = \"dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/my_sqlite-2.db\"\nurl = \"jdbc:sqlite:\" + path\ntablename = \"flight_info\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e73fd09-d295-46aa-a7a8-2d134fb78cd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["접속 테스트\n\n\nimport java.sql.DriverManager\n\nconnection = DriverManager.getConnection(url)\n\nconnection.isClosed()\n\nconnection.Close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b6970c-726a-46c6-be86-43d7b5cbfed0"}}},{"cell_type":"markdown","source":["접속 관련 속성 정의 후 정상적으로 데이터베이스에 접속되는지 테스트해 해당 연결이 유효한지 확인 가능. \n\n코드) SQL 테이블을 읽어 DataFrame 생성"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2221410a-c9c3-468a-99e3-e14e5eb3f390"}}},{"cell_type":"code","source":["dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n  .option(\"dbtable\", tablename).option(\"driver\", driver).load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99a28e56-c606-4ade-80b7-85ebf6b2578c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1174887900894666&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbDataFrame <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jdbc&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;url&#34;</span><span class=\"ansi-blue-fg\">,</span> url<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dbtable&#34;</span><span class=\"ansi-blue-fg\">,</span> tablename<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;driver&#34;</span><span class=\"ansi-blue-fg\">,</span> driver<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    208</span>             <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    209</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 210</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    211</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    212</span>     def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o518.load.\n: java.sql.SQLException: path to &#39;dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/my_sqlite-2.db&#39;: &#39;/databricks/driver/dbfs:&#39; does not exist\n\tat org.sqlite.core.CoreConnection.open(CoreConnection.java:192)\n\tat org.sqlite.core.CoreConnection.&lt;init&gt;(CoreConnection.java:76)\n\tat org.sqlite.jdbc3.JDBC3Connection.&lt;init&gt;(JDBC3Connection.java:24)\n\tat org.sqlite.jdbc4.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:23)\n\tat org.sqlite.SQLiteConnection.&lt;init&gt;(SQLiteConnection.java:45)\n\tat org.sqlite.JDBC.createConnection(JDBC.java:114)\n\tat org.sqlite.JDBC.connect(JDBC.java:88)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"java.sql.SQLException: path to &#39;dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/my_sqlite-2.db&#39;: &#39;/databricks/driver/dbfs:&#39; does not exist","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1174887900894666&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbDataFrame <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jdbc&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;url&#34;</span><span class=\"ansi-blue-fg\">,</span> url<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dbtable&#34;</span><span class=\"ansi-blue-fg\">,</span> tablename<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;driver&#34;</span><span class=\"ansi-blue-fg\">,</span> driver<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    208</span>             <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    209</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 210</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    211</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    212</span>     def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o518.load.\n: java.sql.SQLException: path to &#39;dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/my_sqlite-2.db&#39;: &#39;/databricks/driver/dbfs:&#39; does not exist\n\tat org.sqlite.core.CoreConnection.open(CoreConnection.java:192)\n\tat org.sqlite.core.CoreConnection.&lt;init&gt;(CoreConnection.java:76)\n\tat org.sqlite.jdbc3.JDBC3Connection.&lt;init&gt;(JDBC3Connection.java:24)\n\tat org.sqlite.jdbc4.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:23)\n\tat org.sqlite.SQLiteConnection.&lt;init&gt;(SQLiteConnection.java:45)\n\tat org.sqlite.JDBC.createConnection(JDBC.java:114)\n\tat org.sqlite.JDBC.connect(JDBC.java:88)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:399)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["코드) PostgreSQL을 이용해 동일한 데이터 읽기 작업 수행"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5371e2ba-882e-446b-b731-48371edf4dea"}}},{"cell_type":"markdown","source":["pgDF = spark.read.format(\"jdbc\")\\\n\n  .option(\"driver\", \"org.postgresql.Driver\")\\\n\n  .option(\"url\", \"jdbc:postgresql://database_server\")\\\n\n  .option(\"dbtable\", \"schema.tablename\")\\\n  \n  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25a8a7b1-924a-4a6e-910f-f21391b7deda"}}},{"cell_type":"markdown","source":["### 9.6.2 쿼리 푸시다운\n\n스파크는 DataFrame을 만들기 전에 데이터베이스 자체에서 데이터를 필터링하도록 만들 수 있음.\n\ndbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().explain"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00150281-89a4-47c1-bf7a-58f7740fb7bb"}}},{"cell_type":"markdown","source":["**데이터베이스 병렬로 읽기**\n\nnumPartitions 옵션을 사용해 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 수 설정 가능.\n\ndbDataFrame = spark.format(\"jdbc\")\\\n\n  .option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver)\\\n  \n  .option(\"numPartitions\", 10).load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbe74ef8-ac8e-4ff9-a496-3d4b24a39dcc"}}},{"cell_type":"markdown","source":["데이터소스 생성 시 조건절 목록을 정의해 스파크 자체 파티션에 결과 데이터를 저장 가능.\n\nprops = {\"driver\" : \"org.sqlite.JDBC\"}\n\npredicates = [\n\n  \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n  \n  \n  \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n\nspark.read.jdbc(url, tablename, predicates-predicates, properties=props).show()\n\nspark.read.jdbc(url, tablename, predicates=predicates, properties=pops)\\\n\n  .rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ad20946-2ba4-4776-8ce3-c3197e4b96b4"}}},{"cell_type":"markdown","source":["연관성 없는 조건절을 중복 로우가 많이 발생할 수 있음. 중복 로우를 발생시키는 조건절 예제\n\npops = {\"driver\" : \"org.sqlite.JDBC\"}\n\npredicates = [\n  \n  \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n  \n  \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\"]\n  \nspark.read.jdbc(url, tablename, predicates=predicates, properties=pops).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"042b2721-0e46-478a-88f2-f270d0e45f8a"}}},{"cell_type":"markdown","source":["**슬라이딩 윈도우 기반의 파티셔닝**\n\n조건절을 기반으로 분할.\n\n처음과 마지막 파티션 사이의 최솟값과 최댓값을 사용해 이 범위 밖의 모든 값은 첫 번째 또는 마지막 파티션에 속함. \n\n그 다음 전체 파티션 수를 설정. \n\n이 값은 병렬 처리 수준 의미. \n\n스파크는 데이터베이스에 병렬로 쿼리를 요청하며 numPartitions에 설정된 값만큼 파티션 반환. \n\n그리고 파티션에 값을 할당하기 위해 상한값과 하한값을 수정."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74558e72-b766-40b7-86f0-734c3d9f0db5"}}},{"cell_type":"markdown","source":["### 9.6.3 SQL 데이터베이스 쓰기\n\nSQL 데이터베이스에 데이터를 쓰는 것은 URI를 지정하고 지정한 쓰기 모드에 따라 데이터를 쓰면 된다. \n\nnewPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n\ncsvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)\n\nspark.read.jdbc(newPath, tablename, properties=props).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20618586-d9a0-479c-9feb-9648ebbb504e"}}},{"cell_type":"markdown","source":["## 9.7 텍스트 파일\n\n스파크는 일반 텍스트 파일을 읽을 수 있음.\n\n파일의 각 줄은 DataFrame의 레코드."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed93b917-a3b3-4699-bff6-b6073998ad00"}}},{"cell_type":"markdown","source":["### 9.7.1 텍스트 파일 읽기\n\ntextFile 메서드에 텍스트 파일을 지정하면 된다. \n\n단, textFile 메서드는 파티션 수행 결과로 만들어진 디렉터리명을 무시하기 때문에 파티션된 텍스트 파일을 읽거나 쓰려면 읽기 및 쓰기 시 파티션 수행 결과로 만들어진 디렉터리를 인식할 수 있도록 text 메서드 사용"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5d024e0-6315-411a-a879-89a3953295bb"}}},{"cell_type":"markdown","source":["spark.read.format(\"csv\").load(\"dbfs:/FileStore/shared_uploads/hyjeong0815@gmail.com/2010_summary-3.csv\")\\\n\n  .selectExpr(\"split(value, ',') as rows\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2e7380b-9097-4894-a54a-1aed771adbee"}}},{"cell_type":"markdown","source":["### 9.7.2 텍스트 파일 쓰기\n\n텍스트 파일을 쓸 때는 문자열 컬럼이 하나만 존재해야 함. 그렇지 않으면 실패.\n\ncsvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")\n\n텍스트 파일에 데이터를 저장할 때 파티셔닝 작업을 수행하면 더 많은 컬럼 지정 가능. \n\n하지만 모든 파일에 컬럼을 추가하는 것이 아니라 텍스트 파일이 저장되는 디렉터리에 폴더별로 컬럼 저장. \n\ncsvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\\\n\n  .write.partitionBy(\"count\").text(\".tmp/five-csv-file2py.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ec6c2c-7dd8-43a1-a07c-d2e9902bed62"}}},{"cell_type":"markdown","source":["## 9.8 고급 I/O 개념"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2bc45f9-75b0-43c7-949f-021d4e6f3b6d"}}},{"cell_type":"markdown","source":["### 9.8.1 분할 가능한 파일 타입과 압축 방식\n\n특정 파일 포맷은 기본적으로 분할을 지원. \n\n스파크에서 전체 파일이 아닌 쿼리에 필요한 부분ㄴ만 읽을 수 있으므로 성능 향상에 도움. \n\n하둡 분산 파일 시스템 같은 시스템을 사용하면 분할된 파일을 여러 블록으로 나누어 분산 저장하기 때문에 훨씬 더 최적화 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a211915e-9d4e-49d2-a641-af93d926edf0"}}},{"cell_type":"markdown","source":["### 9.8.2 병렬로 데이터 읽기\n\n여러 익스큐터가 같은 파일을 동시에 읽을 수는 없지만 여러 파일을 동시에 읽을 수는 있음. \n\n다수의 파일이 존재하는 폴더를 읽을 때 폴더의 개별 파일은 DataFrame의 파티션이 된다.\n\n따라서 사용 가능한 익스큐터를 이용해 병렬로 데이터를 읽음."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b39c705-a7b3-419a-960b-4164fe8ac4a4"}}},{"cell_type":"markdown","source":["### 9.8.3 병렬로 데이터 쓰기\n\n파일이나 데이터 수는 데이터를 쓰는 시점에 DataFrame이 가진 파티션 수에 따라 달라질 수 있음. \n\n기본적으로 데이터 파티션당 하나의 파일 작성. \n\n따라서 옵션에 지정된 파일명은 실제로는 다수의 파일을 가진 디렉터리. 그리고 그 디렉터리 안에 파티션당 하나의 파일로 데이터 저장.\n\n**파티셔닝**\n\n파티셔닝은 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능. \n\n파티셔닝된 디렉터리 또는 테이블에 파일을 쓸 때 디렉터리별로 컬럼 데이터를 인코딩해 저장. \n\n그러므로 데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽을 수 있음. \n\n파티셔닝은 필터링을 자주 사용하는 테이블을 가진 경우에 사용할 수 있는 가장 손쉬운 최적화 방식. \n\n**버케팅**\n\n버케팅은 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법.\n\n이 기법을 사용하면 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여있기 때문에 데이터를 읽을 때 셔플을 피할 수 있음.\n\n즉, 데이터가 이후의 사용 방식에 맞춰 사전에 파티셔닝되므로 조인이나 집계 시 발생하는 고비용의 셔플을 피하라 수 있음.\n\n특정 컬럼을 파티셔닝하면 수억 개의 디렉터리를 만들어낼 수 있음."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fcaa26c-5a3e-4cc0-a165-6c568a12c76a"}}},{"cell_type":"markdown","source":["### 9.8.4 복합 데이터 유형 쓰기\n\n스파크는 다양한 자체 데이터 타입을 제공. \n\n단 모든 데이터 파일 포맷에 적합한 것은 아니다. \n\nex. csv 파일은 복합 데이터 타입을 지원하지 않지만 파케이나 ORC는 복합 데이터 타입 지원."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ef55846-5eb3-4943-8b0c-e223b3d4ce31"}}},{"cell_type":"markdown","source":["### 9.8.5 파일 크기 관리\n\n작은 파일을 많이 생성하면 메타데이터에 엄청난 관리 부하 발생. \n\nHDFS 같은 많은 파일 시스템은 작은 크기의 파일을 잘 다루지 못함. \n\nmaxRecordsPerFile 옵션에 파일당 레코드 수를 지정하면 각 파일에 기록될 레코드 수를 조절할 수 있으므로 파일 크기를 더 효과적으로 제어 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a74bcd69-0c3c-4441-b895-e438b968fa50"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter9","dashboards":[{"elements":[{"elementNUID":"f5d31e87-a393-425f-adb9-9c7802a44935","guid":"12b2f63b-7925-4f6a-a25c-0374df0e5a7f","options":null,"position":{"x":0,"y":43,"height":14,"width":12,"z":null},"elementType":"command"},{"elementNUID":"bd40a67c-3f36-4477-a25f-abc1e2d7ad9e","guid":"2cfc3a09-7dd2-49d5-bd04-c919c0e9a726","options":null,"position":{"x":0,"y":0,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"57f1a67d-66d4-45bd-91c5-3143b8fda3e4","guid":"55218337-ff7a-43d1-b5f3-78e39eac696f","options":null,"position":{"x":0,"y":37,"height":6,"width":12,"z":null},"elementType":"command"},{"elementNUID":"99c878a3-7b79-4d43-b9ea-cc382a1d2a61","guid":"85c6b1f6-8a91-4d17-89e0-38bd039af801","options":null,"position":{"x":0,"y":2,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"293a9bad-3c07-472b-83ba-281943f2fc27","guid":"92d60f29-c1cd-44e9-80f0-25016ba6f071","options":null,"position":{"x":0,"y":12,"height":25,"width":12,"z":null},"elementType":"command"},{"elementNUID":"2e43d5c2-d823-417d-beb2-db0c0d9bfb3a","guid":"dcffc2a1-2153-4788-bf2d-7be56b3f38d1","options":null,"position":{"x":0,"y":4,"height":8,"width":12,"z":null},"elementType":"command"},{"elementNUID":"6085110d-01a1-47d2-b71a-4c6eb67d3c37","guid":"eec043be-0c27-40cc-b775-b958d3705838","options":null,"position":{"x":0,"y":67,"height":2,"width":12,"z":null},"elementType":"command"},{"elementNUID":"9b1a0a6e-aa5d-470b-b1fd-d0f8e4dd9d4e","guid":"fada09f1-3c8e-4c6d-a7d8-29aa3d2968f2","options":null,"position":{"x":0,"y":57,"height":10,"width":12,"z":null},"elementType":"command"}],"guid":"408ba34e-8a76-4743-9358-2a251b40bc52","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"6316741f-6bf3-4fd4-b09a-558a6d031704","origId":2034962795926930,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2971641225470486}},"nbformat":4,"nbformat_minor":0}
