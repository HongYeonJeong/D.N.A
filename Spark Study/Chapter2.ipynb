{"cells":[{"cell_type":"markdown","source":["# Chapter2. 스파크 간단히 살펴보기"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"936ef682-5571-4a12-8ab3-29155ac6b4ab"}}},{"cell_type":"markdown","source":["## 2.1 스파크의 기본 아키텍처\n\n* 컴퓨터 클러스터 : 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있도록 만드는 것\n* 클러스터의 작업을 조율할 수 있는 프레임워크 필요\n* 스파크는 클러스터의 데이터 처리 작업을 관리하고 조율\n\n* 스파크가 연산에 사용할 클러스터는 스파크 스탠드얼론 클러스터 매니저, 하둡 YQRN, , 메소스 같은 클러스터 매니저에서 관리\n* 사용자 : 클러스터 매니저에 애플리케이션 제출\n* 클러스터 매니저 : 사용자에게서 제출받은 애플리케이션에서 실행에 필요한 자원을 할당하고, 할당받은 자원으로 작업을 처리"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a39807a-a73c-4d27-a7d9-a906cf64c8dd"}}},{"cell_type":"markdown","source":["## 2.2 스파크 애플리케이션\n\n스파크 애플리케이션은 **드라이버 프로세스**와 다수의 **익스큐터 프로세스**로 구성\n\n드라이버 프로세스 : 클러스터 노드 중 하나에서 실행되며 main() 함수를 실행 - 필수적\n\n* 스파크 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의 작업과 고나련된 분석, 배포 그리고 스케줄링 역할 수행\n* 애플리케이션의 수명 주기 동안 관련 정보 모두 유지\n\n익스큐터 프로세스 : 드라이버 프로세스가 할당한 작업 수행\n\n* 드라이버가 할당한 코드 실행\n* 진행 상황을 다시 드라이버 노드에 보고\n\n클러스터 매니저는 스파크 스탠드얼론 클러스터 매니저, 하둡 YARN, 메소스 중 하나를 선택할 수 있으며 하나의 클러스터에서 여러 개의 스파크 애플리케이션 실행.\n\n* 스파크는 사용 가능한 자원을 파악하기 위해 클러스터 매니저 사용\n* 드라이버 프로세스는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임 존재\n\n즉, 익스큐터는 스파크 코드를 실행하는 역할을 한다. 하지만 드라이버는 스파크의 언어 API를 통해 다양한 언어로 실행 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ed8460a-7a3e-4c49-8073-bf3a2e1d6356"}}},{"cell_type":"markdown","source":["## 2.3 스파크의 다양한 언어 API\n\n* 스칼라 : 스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 기본 언어.\n* 자바 : 스파크가 스칼라로 개발되어 있지만 자바를 이용해 스파크 코드 작성 가능.\n* 파이썬 : 파이썬은 스칼라가 지원하는 거의 모든 구조 지원.\n* SQL : 분석가나 비프로그래머도 SQL을 이용해 스파크의 빅데이터 처리 능력을 쉽게 활용 가능\n* R : 스파크 코어에 포함된 SparkR 라이브러리와 R 커뮤니티 기반의 sparklyr 라이브러리 제공\n\n사용자는 스파크 코드를 실행하기 위해 SparkSession 객체를 진입점으로 사용 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53e65e8e-fcdb-49f7-9998-2f075c0766d2"}}},{"cell_type":"markdown","source":["## 2.3 스파크 API\n\n다양한 언어로 스파크를 사용할 수 있는 이유는 스파크가 기본적으로 두 가지 API를 제공하기 때문.\n\n* 저수준의 비구조적 API\n* 고수준의 구조적 API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d12d4db-3602-445b-878b-08db9903f5ef"}}},{"cell_type":"markdown","source":["## 2.4 스파크 시작하기\n\n실제 스파크 애플리케이션을 개발하려면 사용자 명령과 데이터를 스파크 애플리케이션에 전송. \n\n대화형 모드로 스파크를 시작하면 스파크 애플리케이션을 관리하는 SparkSession이 자동으로 생성.\n\n스탠드얼론 애플리케이션을 스파크로 시작하려면 사용자 애플리케이션 코드에서 SparkSession 객체를 직접 생성."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15a287f4-9763-49eb-8875-ffa8c7de5db8"}}},{"cell_type":"markdown","source":["## 2.5 SparkSession\n\n* SparkSession 인스턴스 : 사용자가 정의한 처리 명령을 클러스터에서 실행\n* 하나의 SparkSession은 하나의 스파크 애플리케이션에 대응\n* 스칼라나 파이썬 콘솔을 시작하면 spark 변수로 SparkSession 사용 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae20418f-8b9f-47a3-8ebf-2553562a0948"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2fe313d-bada-414e-bd67-d13bf454d081"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4267719355602687#setting/sparkui/0629-061916-radio782/driver-5531418027679586084\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4267719355602687#setting/sparkui/0629-061916-radio782/driver-5531418027679586084\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["일정 범위의 숫자 생성"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bcaefa8-dbd3-4085-874e-e7b86c88d82f"}}},{"cell_type":"code","source":["myRange = spark.range(1000).toDF(\"number\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53e3b591-0281-40a5-af7a-a4987b1f9da6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["생성된 DataFrame은 한 개의 컬럼과 1,000개의 로우로 구성되어 있고, 각 로우는 0부터 999까지의 값이 할당되어 있다. \n\n클러스터 모드에서 코드 예제를 실행하면 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당되는데, 이것이 스파크의 DataFrame이다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"755b8392-0e3a-4a58-9830-b883c0942308"}}},{"cell_type":"markdown","source":["## 2.6 DataFrame\n\nDataFrame은 가장 대표적인 구조적 API로, 테이블의 데이터를 로우와 컬럼으로 단순하게 표현한다. \n* 스키마 : 컬럼과 컬럼의 타입을 정의한 목록\n* 스프레드시트는 한 대의 컴퓨터에 존재\n* 스파크의 DataFrame은 수천 대의 컴퓨터에 분산되어 존재"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6e6d59b-8ee2-4d3e-8d23-5c58b1d65762"}}},{"cell_type":"markdown","source":["### 2.6.1 파티션\n\n* 파티션 : 클러스터의 물리적 머신에 존재하는 로우의 집합\n* DataFrame의 파티션 : 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방법\n\n* 파티션이 하나라면 스파크에 수천 개의 익스큐터가 있더라도 병렬성은 1\n* 수백 개의 파티션이 있더라도 익스큐터가 하나밖에 없다면 병렬성은 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"206af422-d85a-4002-9530-0429bf52aa6c"}}},{"cell_type":"markdown","source":["## 2.7 트랜스포메이션\n\n스파크의 핵심 데이터 구조는 불편성을 가진다. 즉, 한 번 생성하면 변경할 수 없다.\n\n만약 DataFrame을 변경하려면 원하는 변경 방법을 스파크에 알려줘야 한다. 그리고 이때 사용하는 명령을 트랜스포메이션이라고 부른다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a72a3701-d972-43dc-a220-cfca2f24bea1"}}},{"cell_type":"markdown","source":["DataFrame에서 짝수를 찾는 간단한 트랜스포메이션 예제"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf133ffe-c751-4d9f-83f2-707e5a970c60"}}},{"cell_type":"code","source":["divisBy2 = myRange.where(\"number % 2 = 0\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36ce2b73-04c6-47f7-ad41-3e199d449425"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["추상적인 트랜스포메이션만 지정한 상태이기 때문에 코드를 실행해도 결과가 출력되지 않는다. 결과를 출력하기 위해서는 액션을 호출해야 한다.\n\n트랜스포메이션의 두 가지 유형.\n\n* 좁은 의존성 : 각 입력 파티션이 하나의 출력 파티션에만 영향 / 좁은 트랜스포메이션을 사용하면 스파킁서 파이프라이닝 자동 수행. 즉 DataFrame에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어남 \n* 넓은 의존성 : 하나의 입력 파티션이 여러 출력 파티션에 영향"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06b48f7e-30ff-4579-a55d-5918a424257a"}}},{"cell_type":"markdown","source":["### 2.7.1 지연 연산\n\n지연 연산 : 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식 의미.\n\n* 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획 생성\n* 스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d9027f7-0c8f-44bd-8fbd-a89f25c3ee9a"}}},{"cell_type":"markdown","source":["## 2.8 액션\n\n사용자는 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있지만 실제 연산을 수행하기 위해서는 액션 명령을 내려야 한다.\n\n액션? 일련의 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령어\n\n* count 메서드 : DataFrame의 전체 레코드 수 반환"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94e127b0-5c63-4fe7-88a4-8affbbd2fa88"}}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fffcdf3-080e-4bd2-b1d8-8ffdb3c3947b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: 500</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: 500</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["스파크의 세 가지 유형의 액션\n\n* 콘솔에서 데이터를 보는 액션\n* 각 언어로 된 네이티브 객체에 데이터를 모으는 액션\n* 출력 데이터소스에 저장하는 액션\n\n액션을 지정하면 잡이 시작된다. \n\n스파크의 잡? 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션) 한다. 그리고 각 언어에 적합한 네이티브 객체에 결과를 모은다. 이때 스파크가 제공하는 스파크 UI로 클러스터에서 실행 중인 스파크의 잡을 모니터링 가능."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51403b84-5f8e-4f33-a526-8d3b97da53de"}}},{"cell_type":"markdown","source":["## 2.9 스파크 UI\n\n스파크의 UI : 스파크 잡의 진행 상황을 모니터링할 때 사용. \n* 스파크 잡의 상태, 환경 설정, 클러스터 상태 등의 정보 확인\n* 스파크 잡을 튜팅하고 디버깅할 때 유용\n\n요약하면, 스파크의 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있으며 스파크 UI로 잡을 모니터링할 수 있다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6490f839-8780-4663-97fb-4f1fd87ba82e"}}},{"cell_type":"markdown","source":["## 2.10 종합 예제 \n\n미국 교통통계국의 항공운항 데이터 중 일부를 스파크로 분석\n\n스파크에서 데이터는 SparkSession의 DataFrameReader 클래스를 사용해 읽는다. 그리고 이때 특정 파일 포맷과 몇 가지 옵션을 함께 설정하는데 여기서는 스파크 DataFrame의 스키마 정보를 알아내는 스키마 추론 기능을 사용하고, 파일의 첫 로우를 헤더로 지정하는 옵션 설정."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7802666d-7985-4009-bc86-48da98f0b218"}}},{"cell_type":"code","source":["flightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/FileStore/tables/data/2015_summary.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e390bf7-a328-4cf1-9969-f79653e1837e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["* DataFramme의 take 액션 : head 명령과 같은 결과 확인 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23a49f32-abd6-4464-a918-8bd605aec56f"}}},{"cell_type":"code","source":["flightData2015.take(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b17d1ff0-dfdc-455f-9256-6d3d6f343202"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Croatia&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Ireland&#39;, count=344)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Croatia&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Ireland&#39;, count=344)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["정수 데이터 타입인 count 컬럼을 기준으로 데이터 정렬\n\n* sort 메서드 : 단지 트랜스포메이션이므로 호출 시 데이터에 아무런 변화가 일어나지 않음\n* 특정 DataFrame 객체에 explain 메서드 : DataFrame의 계보나 스파크의 쿼리 실행 계획 확인 가능"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b90b6d47-fd5e-4e91-970b-6e9b04ea319b"}}},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5ed31b8-d102-45c0-842e-49247c10855e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#390 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#390 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#321]\n      +- FileScan csv [DEST_COUNTRY_NAME#388,ORIGIN_COUNTRY_NAME#389,count#390] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#390 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#390 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#321]\n      +- FileScan csv [DEST_COUNTRY_NAME#388,ORIGIN_COUNTRY_NAME#389,count#390] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["트랜스포메이션의 논리젃 실행 계획은 DataFrame의 계보 정의. 스파크는 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있다. 그리고 이 기능은 스파크의 프로그래밍 모델인 함수형 프로그래밍의 핵심으로, 함수형 프로그래밍은 데이터의 변환 규칙이 일정한 경우 같은 입력에 대해 항상 같은 출력을 생성.\n\n사용자는 물리적 데이터를 직접 다루지 않고 앞서 설정한 셔플 파티션 파라미터와 같은 속성으로 물리적 실행 특성을 제어한다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7df49876-6702-4428-8178-0e1d3e6245d7"}}},{"cell_type":"markdown","source":["### 2.10.1 DataFrame과 SQL\n\n스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL 쿼리를 사용할 수 있다.\n\n* createOrReplaceTempView 메소드 : 모든 DataFrame을 테이블이나 뷰로 만들 수 있다."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"786f56ac-2cf5-4f3e-85e5-22689dceae8a"}}},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04d7106f-8d86-4c25-a821-5377f262831b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["새로운 DataFrame을 반환하는 spark.sql 메서드로 SQL 쿼리 실행.\n\n* spark : SparkSession의 변수\n* DataFrame에 쿼리를 수행하면 새로운 DataFrame 반환\n\n두 가지 실행 계획 비교"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ef7e687-2155-476c-9018-b9c2f416ff85"}}},{"cell_type":"code","source":["sqlWay = spark.sql(\"\"\"\n  SELECT DEST_COUNTRY_NAME, count(1)\n  FROM flight_data_2015\n  GROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\nsqlWay.explain()\ndataFrameWay.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"368a64f4-b158-4518-8b9d-1c1910c2120e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_count(merge count#762L) AS count(1)#750L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#474]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_count(1) AS count#762L])\n         +- FileScan csv [DEST_COUNTRY_NAME#388] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_count(merge count#767L) AS count(1)#757L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#520]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_count(1) AS count#767L])\n         +- FileScan csv [DEST_COUNTRY_NAME#388] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_count(merge count#762L) AS count(1)#750L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#474]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_count(1) AS count#762L])\n         +- FileScan csv [DEST_COUNTRY_NAME#388] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_count(merge count#767L) AS count(1)#757L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#520]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_count(1) AS count#767L])\n         +- FileScan csv [DEST_COUNTRY_NAME#388] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["두 가지 실행 계획은 동일한 기본 실행 계획으로 컴파일 된다.\n\n스파크의 DataFrame과 SQL은 다양한 데이터 처리 기능 % 빅데이터 문제를 빠르게 해결하는 데 필요한 수백 개의 함수 제공\n\n* max 함수 : DataFrame의 특정 컬럼 값을 스캔하면서 이전 최댓값보다 더 큰 값을 찾는다. 즉, 필터링을 수행해 단일 로우를 결롸로 반환하는 트랜스포메이션"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dffe3393-3ea1-4910-a97e-fe8d89a0346a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b86f391-c4b8-43b0-a940-c473394994bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: [Row(max(count)=370002)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [Row(max(count)=370002)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["예제) 상위 5개의 도착 국가 찾기\n\n* 직관적인 SQL 집계 쿼리"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63ca4734-6b9e-4a48-ba04-c65c3c7efd02"}}},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\n  SELECT DEST_COUNTRY_NAME, sum(count) as definition_total\n  FROM flight_data_2015\n  GROUP BY DEST_COUNTRY_NAME\n  ORDER BY sum(count) DESC\n  LIMIT 5\n\"\"\")\n\nmaxSql.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af3437e9-24b9-4d4c-9864-bc3e988afa09"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+----------------+\n|DEST_COUNTRY_NAME|definition_total|\n+-----------------+----------------+\n|    United States|          411352|\n|           Canada|            8399|\n|           Mexico|            7140|\n|   United Kingdom|            2025|\n|            Japan|            1548|\n+-----------------+----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+----------------+\nDEST_COUNTRY_NAME|definition_total|\n+-----------------+----------------+\n    United States|          411352|\n           Canada|            8399|\n           Mexico|            7140|\n   United Kingdom|            2025|\n            Japan|            1548|\n+-----------------+----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["* 의미와 구현은 같지만 정렬 방식이 조금 다른 DataFrame 구문\n\n(기본 실행 계획은 SQL로 정의한 처리와 같음)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"000ce640-26ee-40e7-b05c-2983ccbc9057"}}},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3519df8-f2dd-40ea-93f3-480ec9a1a009"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["DataFrame의 7가지 단계\n\n실행 계획은 트랜스포메이션의 지향성 비순환 그래프로 액션이 호출되면 결과를 만든다. 그리고 지향성 비순환 그래프의 각 단계는 불변성을 가진 신규 DataFrame을 생성한다. \n\n1. 데이터 읽기 - 스파크는 해당 DataFrame이나 자신의 원본 DataFrame에 액션이 호출되기 전까지 데이터를 읽지 않는다.\n2. 데이터를 그룹화 - groupBy 메서드가 호출되면 최종적으로 그룹화된 DataFrame을 지칭하는 이름을 가진 RelationalGroupedDataset을 반환 (기본적으로 키 혹은 키셋을 기준으로 그룹을 만들고 각 키에 대한 집계 수행)\n3. 집계 유형을 지정하기 위해 컬럼 표현식이나 컬럼명을 인수로 사용하는 sum 메서드 사용 - sum 메서드(트랜스포메이션)는 새로운 스키마 정보를 가지는 별도의 DataFrame을 생성.\n4. 컬럼명 변경 - 이름을 변경하기 위해 withColumnRenamed 메서드에 원본 컬럼명과 신규 컬럼명을 인수로 지정 (연산이 일어나지 않으므로 트랜스포메이션)\n5. 데이터 정렬 - 역순으로 정렬하기 위해 desc 함수 import (desc 함수는 문자열이 아닌 Column 객체 반환)\n6. limit 메서드로 반환 결과의 수 제한\n7. 액션 수행 - 이 단계에서 DataFrame의 결과를 모으는 프로세스 시작해 처리가 끝나면 코드를 작성한 프로그래밍 언어에 맞는 리스트나 배열을 반환\n\n(7가지 단계는 explain 메서드를 호출해 실행 계획을 확인할 수 있다)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f642b0ba-a9ce-4e65-8177-1e59174e46ac"}}},{"cell_type":"code","source":["flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7da1903-9f7b-487c-98be-84b935e0bce9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#853L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#388,destination_total#853L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_sum(merge sum#857L) AS sum(cast(count#390 as bigint))#849L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#792]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_sum(cast(count#390 as bigint)) AS sum#857L])\n            +- FileScan csv [DEST_COUNTRY_NAME#388,count#390] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#853L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#388,destination_total#853L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[finalmerge_sum(merge sum#857L) AS sum(cast(count#390 as bigint))#849L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#388, 200), ENSURE_REQUIREMENTS, [id=#792]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#388], functions=[partial_sum(cast(count#390 as bigint)) AS sum#857L])\n            +- FileScan csv [DEST_COUNTRY_NAME#388,count#390] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2830111118859171}},"nbformat":4,"nbformat_minor":0}
